#!/usr/bin/env python3
"""
AIãƒ‡ã‚¤ãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
X API Basic ($200/æœˆ) ã«åã¾ã‚‹ã‚ˆã†è¨­è¨ˆ
"""

import os
import json
import yaml
import requests
import feedparser
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from collections import defaultdict
import time
from content_classifier import ContentClassifier, ClassificationResult
from content_validator import ContentValidator
from post_prompt import get_system_prompt, create_user_prompt_from_tweet, create_user_prompt_from_article, create_user_prompt_from_thread
from article_fetcher import fetch_article_content_safe, fetch_rss_feed_safe
from state_manager import StateManager
from ai_lint_checker import AILintChecker
from x_api_client import XAPIClient


@dataclass
class Item:
    """åé›†ã—ãŸæƒ…å ±ã‚¢ã‚¤ãƒ†ãƒ """
    source: str  # "x_account", "x_search", "rss", "github"
    title: str
    url: str
    published_at: str
    score: int = 0
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class DataCollector:
    """ãƒ‡ãƒ¼ã‚¿åé›†ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼"""

    # å„ªå…ˆåº¦ã®é«˜ã„RSSãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆé‡è¦ãƒ™ãƒ³ãƒ€ãƒ¼ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¼ã‚‰ã•ãªã„ãŸã‚ï¼‰
    PRIORITY_FEEDS = {
        # å…¬å¼ãƒ–ãƒ­ã‚°
        "https://www.anthropic.com/news/rss.xml": 1000,  # Anthropicæœ€å„ªå…ˆ
        "https://openai.com/blog/rss.xml": 1000,          # OpenAIæœ€å„ªå…ˆ
        "https://github.blog/feed/": 800,                 # GitHub Blog
        "https://code.visualstudio.com/updates/feed.xml": 800,  # VSCode Updates

        # GitHub Releases Atom Feedï¼ˆå¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã¯must_include_feedsã§ç®¡ç†ï¼‰
        "https://github.com/anthropics/claude-code/releases.atom": 800,  # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã«ç§»è¡Œ
        "https://github.blog/changelog/label/copilot/feed/": 800,        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã«ç§»è¡Œ
        "https://github.com/langchain-ai/langchain/releases.atom": 800,   # LangChain
        "https://github.com/openai/openai-python/releases.atom": 800,     # OpenAI Python SDK
        "https://github.com/run-llama/llama_index/releases.atom": 600,    # LlamaIndex
        "https://github.com/huggingface/transformers/releases.atom": 600, # Transformers
    }

    def __init__(self, config: Dict, state: StateManager, x_client: XAPIClient):
        self.config = config
        self.state = state
        self.x_client = x_client
        self.items: List[Item] = []
        self.stats = {
            "x_accounts_fetched": 0,
            "x_search_fetched": 0,
            "x_total_fetched": 0,
            "x_limit_reached": False,
            "x_followers_filtered": 0,
            "rss_fetched": 0,
            "github_fetched": 0,
            "duplicates_removed": 0
        }
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†é¡å™¨ã®åˆæœŸåŒ–
        if self.config.get("content_filtering", {}).get("enabled"):
            self.classifier = ContentClassifier(config)
        else:
            self.classifier = None

    def collect_all(self):
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†"""
        print("ğŸ” ãƒ‡ãƒ¼ã‚¿åé›†ã‚’é–‹å§‹...")

        # X (Twitter)
        self._collect_x_accounts()
        self._collect_x_search()

        # RSSï¼ˆGitHub Releases Atom Feedã‚’å«ã‚€ï¼‰
        # â˜… RSSå‡¦ç†ã¯run_hourly.pyã«ç§»è¡Œã—ãŸãŸã‚ç„¡åŠ¹åŒ–
        # self._collect_rss()

        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆå½“æ—¥ã®æ›´æ–°ãŒã‚ã‚Œã°å¿…ãšå«ã‚ã‚‹ï¼‰
        must_include_items = self._collect_must_include_feeds()
        self.items.extend(must_include_items)
        self.stats["must_include_fetched"] = len(must_include_items)

        # é‡è¤‡æ’é™¤
        self._deduplicate()

        print(f"âœ… åé›†å®Œäº†: {len(self.items)} ä»¶")

    def _collect_x_accounts(self):
        """Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‹ã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆåé›†ï¼ˆã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä»˜ãï¼‰"""
        accounts_config = self.config["x"]["accounts"]
        limit = self.config["x"]["limits"]["accounts"]
        fetched = 0

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãƒ•ã‚£ãƒ«ã‚¿è¨­å®š
        follower_filters = self.config["x"].get("follower_filters", {})

        # å¾Œæ–¹äº’æ›æ€§ï¼šaccountsãŒãƒªã‚¹ãƒˆã®å ´åˆã¯å¾“æ¥ã®ãƒ­ã‚¸ãƒƒã‚¯
        if isinstance(accounts_config, list):
            print("âš ï¸  æ—§å½¢å¼ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚¹ãƒˆæ¤œå‡ºã€‚æ–°å½¢å¼ï¼ˆã‚«ãƒ†ã‚´ãƒªåˆ¥ï¼‰ã¸ã®ç§»è¡Œã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
            accounts_list = accounts_config
            tier = "unknown"
            filter_config = self.config["x"].get("follower_filter", {})
        else:
            # æ–°å½¢å¼ï¼šã‚«ãƒ†ã‚´ãƒªåˆ¥å‡¦ç†
            accounts_list = []
            total_accounts = sum(len(accounts_config.get(tier, [])) for tier in ["official", "developers", "practitioners"])
            print(f"ğŸ“± Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆç›£è¦–ï¼ˆã‚«ãƒ†ã‚´ãƒªåˆ¥ï¼‰: åˆè¨ˆ {total_accounts} ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ")

            # ã‚«ãƒ†ã‚´ãƒªé †ã«å‡¦ç†ï¼ˆofficial â†’ developers â†’ practitionersï¼‰
            for tier in ["official", "developers", "practitioners"]:
                tier_accounts = accounts_config.get(tier, [])
                if not tier_accounts:
                    continue

                filter_config = follower_filters.get(tier, {})
                filter_enabled = filter_config.get("enabled", False)
                min_followers = filter_config.get("min_followers", 0)

                print(f"  ã€{tier}ã€‘ {len(tier_accounts)} ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ", end="")
                if filter_enabled:
                    print(f"ï¼ˆãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ {min_followers:,}äººä»¥ä¸Šï¼‰")
                else:
                    print("ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãªã—ï¼‰")

                for username in tier_accounts:
                    if fetched >= limit:
                        print(f"âš ï¸  ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç›£è¦–ã®ä¸Šé™ {limit} ä»¶ã«åˆ°é”")
                        self.stats["x_limit_reached"] = True
                        break

                    user_id = self.x_client.get_user_id(username)
                    if not user_id:
                        print(f"  âš ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼IDå–å¾—å¤±æ•—: @{username}")
                        continue

                    since_id = self.state.get_x_account_since_id(username)
                    tweets, users = self.x_client.get_user_tweets(user_id, since_id, max_results=10)

                    if not tweets:
                        print(f"    â„¹ï¸  @{username}: ãƒ„ã‚¤ãƒ¼ãƒˆãªã—ï¼ˆéå»24æ™‚é–“ï¼‰")
                        continue

                    print(f"    ğŸ“¥ @{username}: {len(tweets)} ä»¶å–å¾—")

                    # æœ€æ–°ã®tweet_idã‚’ä¿å­˜
                    max_id = max(int(t["id"]) for t in tweets)
                    self.state.set_x_account_since_id(username, user_id, str(max_id))

                    for tweet in tweets:
                        if fetched >= limit:
                            break

                        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                        if filter_enabled:
                            author_id = tweet.get("author_id")
                            user = users.get(author_id, {})
                            followers_count = user.get("public_metrics", {}).get("followers_count", 0)

                            if followers_count < min_followers:
                                tweet_text_short = tweet["text"][:50]
                                print(f"    â­ï¸  é™¤å¤–ï¼ˆãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°: {followers_count:,}ï¼‰: @{username}")
                                self.stats["x_followers_filtered"] += 1
                                continue

                        # è¨€èªãƒ»åœ°åŸŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                        tweet_text = tweet["text"]
                        tweet_url = f"https://twitter.com/{username}/status/{tweet['id']}"

                        if self.classifier:
                            # ç·åˆçš„ãªåˆ†é¡ï¼ˆè¨€èªãƒ»åœ°åŸŸãƒã‚§ãƒƒã‚¯ã‚’å«ã‚€ï¼‰
                            classification = self.classifier.classify(tweet_text, "", tweet_url)
                            category = classification.category

                            # éè‹±èªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¾ãŸã¯æ—¥æœ¬ç”±æ¥ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯é™¤å¤–
                            if category in ["NON_ENGLISH", "JAPAN_ORIGIN"]:
                                print(f"    â­ï¸  é™¤å¤–ï¼ˆ{category}ï¼‰: @{username}")
                                continue
                        else:
                            category = "UNKNOWN"

                        # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢è¨ˆç®—
                        initial_score = self._calculate_engagement_score(tweet)

                        # æœ€ä½ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆé–¾å€¤ãƒã‚§ãƒƒã‚¯
                        min_engagement_config = self.config["x"].get("min_engagement", {})
                        if min_engagement_config.get("enabled", False):
                            threshold = min_engagement_config.get("threshold", 10)
                            metrics = tweet.get("public_metrics", {})
                            likes = metrics.get("like_count", 0)
                            rts = metrics.get("retweet_count", 0)
                            replies = metrics.get("reply_count", 0)

                            if initial_score < threshold:
                                print(f"    â­ï¸  é™¤å¤–ï¼ˆã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆä½: {initial_score} < {threshold}ï¼‰")
                                print(f"       ğŸ‘{likes} ğŸ”„{rts} ğŸ’¬{replies} | @{username}")
                                self.stats["x_low_engagement_filtered"] = self.stats.get("x_low_engagement_filtered", 0) + 1
                                continue
                            else:
                                print(f"    âœ“ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ OK: {initial_score} (ğŸ‘{likes} ğŸ”„{rts} ğŸ’¬{replies})")

                        # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã¨ã‚¹ã‚³ã‚¢èª¿æ•´
                        if self.classifier:
                            final_score = self.classifier.calculate_final_score(
                                initial_score, category, "x_account"
                            )
                        else:
                            final_score = initial_score

                        # OpenAIé–¢é€£ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ã‚¹ãƒ¬ãƒƒãƒ‰æ¤œå‡ºãƒ»å–å¾—
                        OPENAI_ACCOUNTS = ["openai", "ChatGPTapp", "openaidevs"]
                        is_openai_account = username in OPENAI_ACCOUNTS
                        tweet_id = tweet["id"]
                        conversation_id = tweet.get("conversation_id")
                        is_thread = conversation_id and conversation_id != tweet_id

                        # ã‚¹ãƒ¬ãƒƒãƒ‰é‡è¤‡ãƒã‚§ãƒƒã‚¯
                        if is_thread and self.state.is_conversation_processed(conversation_id):
                            print(f"    â­ï¸  ã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†æ¸ˆã¿: {conversation_id}")
                            continue

                        # ã‚¹ãƒ¬ãƒƒãƒ‰å–å¾—ï¼ˆOpenAIé–¢é€£ã®ã¿ï¼‰
                        if is_openai_account and is_thread:
                            print(f"    ğŸ§µ ã‚¹ãƒ¬ãƒƒãƒ‰æ¤œå‡º: {tweet_id}")
                            try:
                                thread_tweets = self.x_client.get_conversation_thread(
                                    conversation_id, user_id, max_tweets=10
                                )

                                if len(thread_tweets) > 1:
                                    print(f"    âœ… ã‚¹ãƒ¬ãƒƒãƒ‰å–å¾—: {len(thread_tweets)}ãƒ„ã‚¤ãƒ¼ãƒˆ")

                                    # ã‚¹ãƒ¬ãƒƒãƒ‰å…¨ä½“ã‚’1ã¤ã®Itemã¨ã—ã¦å‡¦ç†
                                    item = Item(
                                        source="x_account",
                                        title=thread_tweets[0]["text"][:100],
                                        url=f"https://twitter.com/{username}/status/{conversation_id}",
                                        published_at=thread_tweets[0]["created_at"],
                                        score=final_score,
                                        metadata={
                                            "username": username,
                                            "tier": tier,
                                            "tweet": thread_tweets[0],
                                            "thread_tweets": thread_tweets,  # å…¨ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ä¿å­˜
                                            "is_thread": True,
                                            "category": category
                                        }
                                    )
                                    self.items.append(item)
                                    self.state.mark_conversation_processed(conversation_id)
                                    fetched += 1
                                    print(f"    âœ… @{username} [{tier}] ã‚¹ãƒ¬ãƒƒãƒ‰ (ã‚¹ã‚³ã‚¢: {final_score})")
                                    continue  # ã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†å®Œäº†ã€å˜ä¸€ãƒ„ã‚¤ãƒ¼ãƒˆå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
                                else:
                                    print(f"    âš ï¸  ã‚¹ãƒ¬ãƒƒãƒ‰å–å¾—å¤±æ•—ã€å˜ä¸€ãƒ„ã‚¤ãƒ¼ãƒˆã¨ã—ã¦å‡¦ç†")
                            except Exception as e:
                                print(f"    âš ï¸  ã‚¹ãƒ¬ãƒƒãƒ‰å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                                print(f"    â†’ å˜ä¸€ãƒ„ã‚¤ãƒ¼ãƒˆã¨ã—ã¦å‡¦ç†ã—ã¾ã™")

                        # å˜ä¸€ãƒ„ã‚¤ãƒ¼ãƒˆå‡¦ç†ï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
                        item = Item(
                            source="x_account",
                            title=tweet_text[:100],
                            url=tweet_url,
                            published_at=tweet["created_at"],
                            score=final_score,
                            metadata={
                                "username": username,
                                "tier": tier,  # è‘—è€…tierã‚’ä¿å­˜
                                "tweet": tweet,
                                "category": category
                            }
                        )
                        self.items.append(item)
                        fetched += 1
                        print(f"    âœ… @{username} [{tier}] (ã‚¹ã‚³ã‚¢: {final_score})")

                if fetched >= limit:
                    break

        self.stats["x_accounts_fetched"] = fetched
        self.stats["x_total_fetched"] += fetched

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµ±è¨ˆã‚’è¡¨ç¤º
        print(f"\nğŸ“Š Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆåé›†çµ±è¨ˆ:")
        print(f"  åé›†æˆåŠŸ: {fetched} ä»¶")
        if self.stats.get("x_followers_filtered", 0) > 0:
            print(f"  ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãƒ•ã‚£ãƒ«ã‚¿é™¤å¤–: {self.stats['x_followers_filtered']} ä»¶")
        if self.stats.get("x_low_engagement_filtered", 0) > 0:
            print(f"  ä½ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆé™¤å¤–: {self.stats['x_low_engagement_filtered']} ä»¶")

    def _collect_x_search(self):
        """Xã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä»˜ãï¼‰"""
        keywords = self.config["x"]["keywords"]
        limit = self.config["x"]["limits"]["search"]
        fetched = 0

        # ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãƒ•ã‚£ãƒ«ã‚¿è¨­å®š
        follower_filter = self.config["x"].get("follower_filter", {})
        filter_enabled = follower_filter.get("enabled", False)
        min_followers = follower_filter.get("min_followers", 0)

        print(f"ğŸ” Xã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢: {len(keywords)} ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
        if filter_enabled:
            print(f"   ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãƒ•ã‚£ãƒ«ã‚¿: {min_followers:,}äººä»¥ä¸Š")

        for keyword in keywords:
            if fetched >= limit:
                print(f"âš ï¸  æ¤œç´¢ã®ä¸Šé™ {limit} ä»¶ã«åˆ°é”")
                self.stats["x_limit_reached"] = True
                break

            since_id = self.state.get_x_keyword_since_id(keyword)
            tweets, users = self.x_client.search_tweets(keyword, since_id, max_results=10)

            if not tweets:
                continue

            # æœ€æ–°ã®tweet_idã‚’ä¿å­˜
            max_id = max(int(t["id"]) for t in tweets)
            self.state.set_x_keyword_since_id(keyword, str(max_id))

            for tweet in tweets:
                if fetched >= limit:
                    break

                # ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if filter_enabled:
                    author_id = tweet.get("author_id")
                    user = users.get(author_id, {})
                    followers_count = user.get("public_metrics", {}).get("followers_count", 0)

                    if followers_count < min_followers:
                        tweet_text_short = tweet["text"][:50]
                        print(f"  â­ï¸  é™¤å¤–ï¼ˆãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°: {followers_count:,}ï¼‰: {tweet_text_short}...")
                        self.stats["x_followers_filtered"] += 1
                        continue

                # è¨€èªãƒ»åœ°åŸŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                tweet_text = tweet["text"]
                tweet_url = f"https://twitter.com/i/web/status/{tweet['id']}"

                if self.classifier:
                    # ç·åˆçš„ãªåˆ†é¡ï¼ˆè¨€èªãƒ»åœ°åŸŸãƒã‚§ãƒƒã‚¯ã‚’å«ã‚€ï¼‰
                    classification = self.classifier.classify(tweet_text, "", tweet_url)
                    category = classification.category

                    # éè‹±èªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¾ãŸã¯æ—¥æœ¬ç”±æ¥ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯é™¤å¤–
                    if category in ["NON_ENGLISH", "JAPAN_ORIGIN"]:
                        print(f"  â­ï¸  é™¤å¤–ï¼ˆ{category}ï¼‰: {tweet_text[:50]}...")
                        continue
                else:
                    category = "UNKNOWN"

                # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã¨ã‚¹ã‚³ã‚¢èª¿æ•´
                initial_score = self._calculate_engagement_score(tweet)
                if self.classifier:
                    final_score = self.classifier.calculate_final_score(
                        initial_score, category, "x_search"
                    )
                else:
                    final_score = initial_score

                item = Item(
                    source="x_search",
                    title=tweet_text[:100],
                    url=tweet_url,
                    published_at=tweet["created_at"],
                    score=final_score,
                    metadata={
                        "keyword": keyword,
                        "tweet": tweet,
                        "category": category
                    }
                )
                self.items.append(item)
                fetched += 1

        self.stats["x_search_fetched"] = fetched
        self.stats["x_total_fetched"] += fetched

    def _collect_rss(self):
        """RSSåé›†ï¼ˆè¨˜äº‹URLãƒªã‚¹ãƒˆæ¯”è¼ƒæ–¹å¼ï¼‰"""
        feeds = self.config["rss"]["feeds"]

        # çµ±è¨ˆæƒ…å ±ã®åˆæœŸåŒ–
        rss_stats = {
            "total_feeds": len(feeds),
            "success_feeds": 0,
            "failed_feeds": 0,
            "total_entries": 0,
            "filtered_out": 0,
            "new_articles": 0,
            "old_articles_filtered": 0,
            "added": 0
        }

        print(f"ğŸ“° RSSåé›†: {len(feeds)} ãƒ•ã‚£ãƒ¼ãƒ‰")

        for feed_config in feeds:
            feed_url = feed_config["url"]
            feed_name = feed_config["name"]

            print(f"\nğŸ“¡ {feed_name}")
            print(f"   URL: {feed_url}")

            # ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—
            feed = feedparser.parse(feed_url)

            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯1: HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
            if hasattr(feed, 'status') and feed.status >= 400:
                print(f"   âš ï¸  HTTP {feed.status}: å–å¾—å¤±æ•—")
                rss_stats["failed_feeds"] += 1
                continue

            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯2: ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼
            if hasattr(feed, 'bozo') and feed.bozo and not feed.entries:
                print(f"   âš ï¸  ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {feed.get('bozo_exception', 'Unknown error')}")
                rss_stats["failed_feeds"] += 1
                continue

            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯3: è¨˜äº‹ãŒ0ä»¶
            if not feed.entries:
                print(f"   â„¹ï¸  è¨˜äº‹0ä»¶")
                rss_stats["failed_feeds"] += 1
                continue

            rss_stats["success_feeds"] += 1
            rss_stats["total_entries"] += len(feed.entries)
            print(f"   âœ… è¨˜äº‹å–å¾—: {len(feed.entries)}ä»¶")

            # å‰å›å–å¾—ã—ãŸè¨˜äº‹URLãƒªã‚¹ãƒˆã‚’å–å¾—
            previous_urls = self.state.get_rss_article_urls(feed_url)
            if previous_urls is None:
                previous_urls = []
                print(f"   â„¹ï¸  åˆå›å–å¾—ï¼ˆå…¨è¨˜äº‹ã‚’å¯¾è±¡ï¼‰")

            # ä»Šå›å–å¾—ã—ãŸè¨˜äº‹URLãƒªã‚¹ãƒˆ
            current_urls = [entry.link for entry in feed.entries]

            # å·®åˆ†ï¼ˆæ–°è¦è¨˜äº‹ï¼‰ã‚’æŠ½å‡º
            new_urls = set(current_urls) - set(previous_urls)

            if new_urls:
                print(f"   ğŸ†• æ–°è¦è¨˜äº‹: {len(new_urls)}ä»¶")
                rss_stats["new_articles"] += len(new_urls)
            else:
                print(f"   â„¹ï¸  æ–°è¦è¨˜äº‹ãªã—ï¼ˆå‰å›ã¨åŒã˜å†…å®¹ï¼‰")

            # 24æ™‚é–“å‰ã®ã‚«ãƒƒãƒˆã‚ªãƒ•ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¨ˆç®—
            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=24)

            feed_added = 0

            for entry in feed.entries:
                # æ–°è¦è¨˜äº‹ã®ã¿å‡¦ç†
                if entry.link not in new_urls:
                    continue

                published = entry.get("published_parsed") or entry.get("updated_parsed")
                if not published:
                    continue

                published_dt = datetime(*published[:6], tzinfo=timezone.utc)
                published_iso = published_dt.isoformat()

                # 24æ™‚é–“ä»¥å†…ã®è¨˜äº‹ã®ã¿ã‚’å¯¾è±¡
                if published_dt < cutoff_time:
                    rss_stats["old_articles_filtered"] += 1
                    continue

                # è¨€èªãƒ»åœ°åŸŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if self.classifier:
                    description = entry.get("summary", "")
                    url = entry.link

                    classification = self.classifier.classify(entry.title, description, url)
                    category = classification.category

                    if category in ["NON_ENGLISH", "JAPAN_ORIGIN"]:
                        print(f"   â­ï¸  é™¤å¤–ï¼ˆ{category}ï¼‰: {entry.title[:50]}...")
                        rss_stats["filtered_out"] += 1
                        continue
                else:
                    category = "UNKNOWN"

                # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
                base_score = self.config["slack"]["scoring"]["rss_bonus"]
                priority_bonus = self.PRIORITY_FEEDS.get(feed_url, 0)
                initial_score = base_score + priority_bonus

                if self.classifier:
                    final_score = self.classifier.calculate_final_score(
                        initial_score, category, "rss", is_official=True
                    )
                else:
                    final_score = initial_score

                item = Item(
                    source="rss",
                    title=entry.title,
                    url=entry.link,
                    published_at=published_iso,
                    score=final_score,
                    metadata={
                        "feed_name": feed_name,
                        "feed_url": feed_url,
                        "category": category
                    }
                )
                self.items.append(item)
                feed_added += 1
                rss_stats["added"] += 1

            if feed_added > 0:
                print(f"   â• è¿½åŠ : {feed_added}ä»¶")

            # ä»Šå›ã®è¨˜äº‹URLãƒªã‚¹ãƒˆã‚’ä¿å­˜ï¼ˆæœ€æ–°20ä»¶ã®ã¿ï¼‰
            self.state.set_rss_article_urls(feed_url, current_urls)
            print(f"   ğŸ’¾ è¨˜äº‹URLãƒªã‚¹ãƒˆä¿å­˜: {len(current_urls[:20])}ä»¶")

            # æœ€çµ‚ç¢ºèªæ™‚åˆ»ã‚’ä¿å­˜
            current_time = datetime.now(timezone.utc).isoformat()
            self.state.set_rss_last_checked(feed_url, current_time)

            # æœ€æ–°ã®published_atã‚‚ä¿å­˜ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
            if feed.entries:
                latest = max(feed.entries, key=lambda e: e.get("published_parsed") or e.get("updated_parsed"))
                published = latest.get("published_parsed") or latest.get("updated_parsed")
                if published:
                    published_dt = datetime(*published[:6], tzinfo=timezone.utc)
                    self.state.set_rss_last_published(feed_url, published_dt.isoformat())

        # çµ±è¨ˆå‡ºåŠ›
        print(f"\nğŸ“Š RSSåé›†çµ±è¨ˆ:")
        print(f"   å¯¾è±¡ãƒ•ã‚£ãƒ¼ãƒ‰: {rss_stats['total_feeds']}ä»¶")
        print(f"   å–å¾—æˆåŠŸ: {rss_stats['success_feeds']}ä»¶")
        print(f"   å–å¾—å¤±æ•—: {rss_stats['failed_feeds']}ä»¶")
        print(f"   ç·è¨˜äº‹æ•°: {rss_stats['total_entries']}ä»¶")
        print(f"   æ–°è¦è¨˜äº‹: {rss_stats['new_articles']}ä»¶")
        print(f"   å¤ã„è¨˜äº‹é™¤å¤–: {rss_stats['old_articles_filtered']}ä»¶")
        print(f"   ãƒ•ã‚£ãƒ«ã‚¿é™¤å¤–: {rss_stats['filtered_out']}ä»¶")
        print(f"   è¿½åŠ ä»¶æ•°: {rss_stats['added']}ä»¶")

        self.stats["rss_fetched"] = rss_stats["added"]

    def _collect_must_include_feeds(self) -> List[Item]:
        """å¿…ãšå«ã‚ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰å½“æ—¥ã®æ›´æ–°ã‚’å–å¾—"""
        must_include_config = self.config.get("rss", {}).get("must_include_feeds", [])
        must_include_items = []

        if not must_include_config:
            return must_include_items

        print(f"â­ å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰åé›†: {len(must_include_config)} ãƒ•ã‚£ãƒ¼ãƒ‰")

        # å½“æ—¥ã®æ—¥ä»˜ï¼ˆUTCã§00:00:00ï¼‰
        today = datetime.now(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)

        for feed_config in must_include_config:
            feed_url = feed_config["url"]
            feed_name = feed_config["name"]
            max_items = feed_config.get("max_items", 3)

            feed = fetch_rss_feed_safe(feed_url)
            count = 0

            for entry in feed.entries:
                if count >= max_items:
                    break

                published = entry.get("published_parsed") or entry.get("updated_parsed")
                if not published:
                    continue

                published_dt = datetime(*published[:6], tzinfo=timezone.utc)
                published_iso = published_dt.isoformat()

                # å½“æ—¥ã®æ›´æ–°ã®ã¿ï¼ˆ00:00:00ä»¥é™ï¼‰
                if published_dt < today:
                    continue

                # è¨€èªãƒ»åœ°åŸŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if self.classifier:
                    description = entry.get("summary", "")
                    url = entry.link

                    # ç·åˆçš„ãªåˆ†é¡ï¼ˆè¨€èªãƒ»åœ°åŸŸãƒã‚§ãƒƒã‚¯ã‚’å«ã‚€ï¼‰
                    classification = self.classifier.classify(entry.title, description, url)
                    category = classification.category

                    # éè‹±èªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¾ãŸã¯æ—¥æœ¬ç”±æ¥ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯é™¤å¤–
                    # ãŸã ã—ã€å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ãªã®ã§è­¦å‘Šã®ã¿å‡ºåŠ›
                    if category in ["NON_ENGLISH", "JAPAN_ORIGIN"]:
                        print(f"  âš ï¸  å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã ãŒéè‹±èª/æ—¥æœ¬ç”±æ¥ï¼ˆ{category}ï¼‰: {entry.title[:50]}...")
                        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ãªã®ã§é™¤å¤–ã›ãšã«å«ã‚ã‚‹ï¼ˆã‚¹ã‚³ã‚¢ã¯ä½ãã™ã‚‹ï¼‰
                        category = category  # ãã®ã¾ã¾ä½¿ã†
                    elif category == "PRACTICAL":
                        category = "PRACTICAL"  # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã®PRACTICALã¯ãã®ã¾ã¾
                else:
                    category = "PRACTICAL"  # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§PRACTICAL

                item = Item(
                    source="must_include",
                    title=entry.title,
                    url=entry.link,
                    published_at=published_iso,
                    score=9999,  # æœ€é«˜ã‚¹ã‚³ã‚¢ï¼ˆå¿…ãšå«ã¾ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ï¼‰
                    metadata={
                        "feed_name": feed_name,
                        "feed_url": feed_url,
                        "category": category,
                        "must_include": True
                    }
                )
                must_include_items.append(item)
                count += 1
                print(f"  âœ“ {feed_name}: {entry.title[:50]}...")

        return must_include_items

    def _select_diverse_provider_items(self, sorted_items: List[Item], limit: int) -> List[Item]:
        """
        å„ªå…ˆåº¦ã®é«˜ã„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æœ€æ–°è¨˜äº‹ã‚’1ä»¶ãšã¤ãƒãƒ©ãƒ³ã‚¹ã‚ˆãé¸æŠ

        Args:
            sorted_items: ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã®ã‚¢ã‚¤ãƒ†ãƒ ãƒªã‚¹ãƒˆ
            limit: é¸æŠã™ã‚‹æœ€å¤§ä»¶æ•°

        Returns:
            ãƒãƒ©ãƒ³ã‚¹ã‚ˆãé¸æŠã•ã‚ŒãŸRSSã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆ
        """
        from collections import defaultdict

        # RSSã‚¢ã‚¤ãƒ†ãƒ ã®ã¿æŠ½å‡º
        rss_items = [i for i in sorted_items if i.source == "rss"]

        # ãƒ•ã‚£ãƒ¼ãƒ‰åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        feed_groups = defaultdict(list)
        for item in rss_items:
            feed_url = item.metadata.get("feed_url", "")
            if feed_url:
                feed_groups[feed_url].append(item)

        # å„ªå…ˆé †ä½é †ã«ã‚½ãƒ¼ãƒˆï¼ˆPRIORITY_FEEDS ã®å®šç¾©é †ï¼‰
        priority_order = list(self.PRIORITY_FEEDS.keys())
        sorted_feed_urls = sorted(
            feed_groups.keys(),
            key=lambda url: priority_order.index(url) if url in priority_order else 9999
        )

        # ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ­ãƒ“ãƒ³æ–¹å¼ã§é¸æŠï¼ˆå„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰1ä»¶ãšã¤ï¼‰
        selected = []
        round_num = 0

        while len(selected) < limit:
            added_this_round = False

            for feed_url in sorted_feed_urls:
                if len(selected) >= limit:
                    break

                items = feed_groups[feed_url]
                if round_num < len(items):
                    selected.append(items[round_num])
                    added_this_round = True

            # å…¨ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰é¸æŠã—çµ‚ãˆãŸ
            if not added_this_round:
                break

            round_num += 1

        return selected[:limit]

    def _calculate_engagement_score(self, tweet: Dict) -> int:
        """ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢è¨ˆç®—"""
        metrics = tweet.get("public_metrics", {})
        scoring = self.config["slack"]["scoring"]

        score = (
            metrics.get("like_count", 0) * scoring["like_weight"] +
            metrics.get("retweet_count", 0) * scoring["retweet_weight"] +
            metrics.get("reply_count", 0) * scoring["reply_weight"]
        )
        return score

    def _deduplicate(self):
        """é‡è¤‡æ’é™¤ï¼ˆURLåŸºæº– + éå»3æ—¥åˆ†ã®drafts.jsonãƒã‚§ãƒƒã‚¯ï¼‰"""
        seen_urls = set()

        # éå»3æ—¥åˆ†ã®drafts.jsonã‹ã‚‰æ—¢å­˜URLã‚’èª­ã¿è¾¼ã‚€
        dedup_config = self.config["slack"].get("deduplication", {})
        if dedup_config.get("enabled", False):
            lookback_days = dedup_config.get("lookback_days", 3)
            past_urls = self._load_past_urls_from_drafts(lookback_days)
            seen_urls.update(past_urls)
            print(f"  ğŸ” éå»{lookback_days}æ—¥åˆ†ã®URL: {len(past_urls)}ä»¶ã‚’é™¤å¤–å¯¾è±¡ã«è¿½åŠ ")

        unique_items = []

        for item in self.items:
            if item.url not in seen_urls:
                seen_urls.add(item.url)
                unique_items.append(item)
            else:
                self.stats["duplicates_removed"] += 1

        self.items = unique_items

    def _load_past_urls_from_drafts(self, lookback_days: int) -> set:
        """éå»Næ—¥åˆ†ã®drafts.jsonã‹ã‚‰URLã‚’å–å¾—"""
        import os
        drafts_path = os.path.join(os.path.dirname(__file__), "..", "data", "drafts.json")

        if not os.path.exists(drafts_path):
            print(f"  âš ï¸  drafts.jsonãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {drafts_path}")
            return set()

        try:
            with open(drafts_path, 'r', encoding='utf-8') as f:
                drafts_data = json.load(f)

            past_urls = set()
            cutoff_date = datetime.now(timezone.utc) - timedelta(days=lookback_days)

            for draft in drafts_data.get("drafts", []):
                created_at_str = draft.get("created_at", "")
                if not created_at_str:
                    continue

                # ISOå½¢å¼ã®æ—¥æ™‚ã‚’ãƒ‘ãƒ¼ã‚¹
                created_at = datetime.fromisoformat(created_at_str.replace('Z', '+00:00'))

                # éå»Næ—¥ä»¥å†…ã®URLã‚’åé›†
                if created_at >= cutoff_date:
                    url = draft.get("item", {}).get("url")
                    if url:
                        past_urls.add(url)

            return past_urls
        except Exception as e:
            print(f"  âš ï¸  drafts.jsonèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return set()


class SlackReporter:
    """Slackãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ»æŠ•ç¨¿"""

    # DataCollectorã®PRIORITY_FEEDSã¨åŒã˜å®šç¾©
    PRIORITY_FEEDS = {
        # å…¬å¼ãƒ–ãƒ­ã‚°
        "https://www.anthropic.com/news/rss.xml": 1000,
        "https://openai.com/blog/rss.xml": 1000,
        "https://github.blog/feed/": 800,
        "https://code.visualstudio.com/updates/feed.xml": 800,

        # GitHub Releases Atom Feedï¼ˆå¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã¯must_include_feedsã§ç®¡ç†ï¼‰
        "https://github.com/anthropics/claude-code/releases.atom": 800,
        "https://github.blog/changelog/label/copilot/feed/": 800,
        "https://github.com/langchain-ai/langchain/releases.atom": 800,
        "https://github.com/openai/openai-python/releases.atom": 800,
        "https://github.com/run-llama/llama_index/releases.atom": 600,
        "https://github.com/huggingface/transformers/releases.atom": 600,
    }

    def __init__(self, webhook_url: str, config: Dict, items: List[Item], stats: Dict):
        self.webhook_url = webhook_url
        self.config = config
        self.items = items
        self.stats = stats
        # æ¤œè¨¼å™¨ã®åˆæœŸåŒ–
        self.validator = ContentValidator(config)
        # AI-lintãƒã‚§ãƒƒã‚«ãƒ¼ã®åˆæœŸåŒ–
        rules_path = os.path.join(os.path.dirname(__file__), "..", "ai-lint", ".claude", "skills", "ai-lint", "rules", "ai-lint-rules.yml")
        if os.path.exists(rules_path):
            self.ai_lint_checker = AILintChecker(rules_path)
        else:
            self.ai_lint_checker = AILintChecker()  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ«ãƒ¼ãƒ«

    def _select_diverse_provider_items(self, sorted_items: List[Item], limit: int) -> List[Item]:
        """
        å„ªå…ˆåº¦ã®é«˜ã„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æœ€æ–°è¨˜äº‹ã‚’1ä»¶ãšã¤ãƒãƒ©ãƒ³ã‚¹ã‚ˆãé¸æŠ

        Args:
            sorted_items: ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã®ã‚¢ã‚¤ãƒ†ãƒ ãƒªã‚¹ãƒˆ
            limit: é¸æŠã™ã‚‹æœ€å¤§ä»¶æ•°

        Returns:
            ãƒãƒ©ãƒ³ã‚¹ã‚ˆãé¸æŠã•ã‚ŒãŸRSSã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆ
        """
        from collections import defaultdict

        # RSSã‚¢ã‚¤ãƒ†ãƒ ã®ã¿æŠ½å‡º
        rss_items = [i for i in sorted_items if i.source == "rss"]

        # ãƒ•ã‚£ãƒ¼ãƒ‰åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        feed_groups = defaultdict(list)
        for item in rss_items:
            feed_url = item.metadata.get("feed_url", "")
            if feed_url:
                feed_groups[feed_url].append(item)

        # å„ªå…ˆé †ä½é †ã«ã‚½ãƒ¼ãƒˆï¼ˆPRIORITY_FEEDS ã®å®šç¾©é †ï¼‰
        priority_order = list(self.PRIORITY_FEEDS.keys())
        sorted_feed_urls = sorted(
            feed_groups.keys(),
            key=lambda url: priority_order.index(url) if url in priority_order else 9999
        )

        # ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ­ãƒ“ãƒ³æ–¹å¼ã§é¸æŠï¼ˆå„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰1ä»¶ãšã¤ï¼‰
        selected = []
        round_num = 0

        while len(selected) < limit:
            added_this_round = False

            for feed_url in sorted_feed_urls:
                if len(selected) >= limit:
                    break

                items = feed_groups[feed_url]
                if round_num < len(items):
                    selected.append(items[round_num])
                    added_this_round = True

            # å…¨ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰é¸æŠã—çµ‚ãˆãŸ
            if not added_this_round:
                break

            round_num += 1

        return selected[:limit]

    def _select_items_with_source_quotas(self, items: List[Item]) -> List[Item]:
        """ã‚½ãƒ¼ã‚¹åˆ¥æœ€ä½ä¿è¨¼æ ã‚’è€ƒæ…®ã—ãŸã‚¢ã‚¤ãƒ†ãƒ é¸æŠ"""
        quotas_config = self.config["slack"].get("source_quotas", {})

        if not quotas_config.get("enabled", False):
            # ä¿è¨¼æ ç„¡åŠ¹ã®å ´åˆã¯å¾“æ¥ã®ã‚¹ã‚³ã‚¢é †
            print("  ğŸ“Š ã‚½ãƒ¼ã‚¹åˆ¥ä¿è¨¼æ : ç„¡åŠ¹ï¼ˆã‚¹ã‚³ã‚¢é †ã®ã¿ï¼‰")
            return sorted(items, key=lambda x: (x.published_at, x.score), reverse=True)[:15]

        print("  ğŸ“Š ã‚½ãƒ¼ã‚¹åˆ¥ä¿è¨¼æ : æœ‰åŠ¹")

        # 1. å¿…é ˆè¡¨ç¤ºã‚¢ã‚¤ãƒ†ãƒ ã‚’æŠ½å‡ºï¼ˆmust_include_feedsï¼‰
        selected = []
        if quotas_config.get("must_include", False):
            must_include_items = [
                item for item in items
                if item.metadata.get("must_include", False)
            ]
            selected.extend(must_include_items)
            print(f"    âœ… å¿…é ˆè¡¨ç¤º: {len(must_include_items)}ä»¶")
            # å¿…é ˆã‚¢ã‚¤ãƒ†ãƒ ã‚’æ®‹ã‚Šã®ãƒ—ãƒ¼ãƒ«ã‹ã‚‰é™¤å¤–
            items = [item for item in items if not item.metadata.get("must_include", False)]

        # 2. ã‚½ãƒ¼ã‚¹åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        by_source = defaultdict(list)
        for item in items:
            # x_account ã¨ x_search ã‚’ "x" ã«ã¾ã¨ã‚ã‚‹
            source = "x" if item.source in ["x_account", "x_search"] else item.source
            by_source[source].append(item)

        # å„ã‚½ãƒ¼ã‚¹ã‚’ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        for source in by_source:
            by_source[source].sort(key=lambda x: (x.published_at, x.score), reverse=True)

        # 3. ã‚½ãƒ¼ã‚¹åˆ¥ä¿è¨¼æ ã‚’ç¢ºä¿
        for source in ["rss", "x"]:
            quota = quotas_config.get(source, 0)
            selected.extend(by_source[source][:quota])
            print(f"    âœ… {source.upper()}ä¿è¨¼æ : {len(by_source[source][:quota])}ä»¶ / {quota}ä»¶")

        # 4. æ®‹ã‚Šæ ã‚’ã‚¹ã‚³ã‚¢é †ã«åŸ‹ã‚ã‚‹
        remaining_quota = quotas_config.get("remaining", 7)

        # ä¿è¨¼æ ã§ä½¿ã‚ã‚Œãªã‹ã£ãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ—ãƒ¼ãƒ«ã«å…¥ã‚Œã‚‹
        pool = []
        for source in ["rss", "x"]:
            quota = quotas_config.get(source, 0)
            pool.extend(by_source[source][quota:])  # ä¿è¨¼æ ä»¥é™

        # ãƒ—ãƒ¼ãƒ«ã‚’ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        pool.sort(key=lambda x: (x.published_at, x.score), reverse=True)
        selected.extend(pool[:remaining_quota])
        print(f"    âœ… æ®‹ã‚Šã‚¹ã‚³ã‚¢é †: {len(pool[:remaining_quota])}ä»¶ / {remaining_quota}ä»¶")

        # æœ€çµ‚çš„ã«æ—¥ä»˜Ã—ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        selected.sort(key=lambda x: (x.published_at, x.score), reverse=True)

        print(f"  ğŸ“Š åˆè¨ˆé¸æŠ: {len(selected)}ä»¶")
        return selected

    def send(self):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¦Slackã«æŠ•ç¨¿ï¼ˆæŠ•ç¨¿æ¡ˆã”ã¨ã«å€‹åˆ¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰"""
        print("ğŸ“¤ Slackãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")

        # ã‚½ãƒ¼ã‚¹åˆ¥ä¿è¨¼æ ã‚’è€ƒæ…®ã—ãŸã‚¢ã‚¤ãƒ†ãƒ é¸æŠ
        sorted_items = self._select_items_with_source_quotas(self.items)

        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†ã‘
        provider_items = self._select_diverse_provider_items(sorted_items, self.config["slack"]["limits"]["provider_official"])

        # â‘  ãƒ˜ãƒƒãƒ€ãƒ¼ + ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é›†è¨ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆ1å›ã®ã¿ï¼‰
        header_blocks = [
            {
                "type": "header",
                "text": {"type": "plain_text", "text": f"ğŸ¦ XæŠ•ç¨¿ç´ æ¡ˆ - {datetime.now().strftime('%Y-%m-%d')}"}
            },
            {
                "type": "section",
                "text": {"type": "mrkdwn", "text": self._format_source_summary(self._count_sources())}
            }
        ]
        self._send_blocks(header_blocks)
        print("âœ… ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é€ä¿¡ã—ã¾ã—ãŸ")

        # â‘¡ æŠ•ç¨¿æ¡ˆã‚’å€‹åˆ¥é€ä¿¡
        # å¿…è¦‹ã®æ›´æ–°ã‚¢ã‚¤ãƒ†ãƒ ã‚’æŠ½å‡º
        must_include_items = [i for i in sorted_items if i.metadata.get("must_include")]

        # Xç”±æ¥ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’æŠ½å‡º
        x_items = [i for i in sorted_items if i.source in ["x_account", "x_search"]]

        # æŠ•ç¨¿æ¡ˆã‚’å€‹åˆ¥é€ä¿¡
        self._send_individual_draft_posts(
            must_include_items=must_include_items,
            provider_items=provider_items,
            x_items=x_items
        )

        print("âœ… å…¨ã¦ã®æŠ•ç¨¿æ¡ˆã‚’Slackã«é€ä¿¡ã—ã¾ã—ãŸ")

    def _send_blocks(self, blocks: List[Dict]) -> None:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ–ãƒ­ãƒƒã‚¯ã‚’Slackã«é€ä¿¡ã™ã‚‹æ±ç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼

        Args:
            blocks: Slack Block Kitå½¢å¼ã®ãƒ–ãƒ­ãƒƒã‚¯ãƒªã‚¹ãƒˆ
        """
        if not blocks:
            return

        payload = {"blocks": blocks}

        try:
            response = requests.post(
                self.webhook_url,
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=30
            )
            response.raise_for_status()
        except Exception as e:
            print(f"âŒ Slacké€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")

    def _send_single_draft_post(
        self,
        item: Item,
        draft_number: int,
        source_type: str
    ) -> None:
        """
        1ã¤ã®æŠ•ç¨¿æ¡ˆã‚’ç”Ÿæˆã—ã¦Slackã«å€‹åˆ¥é€ä¿¡

        Args:
            item: æŠ•ç¨¿æ¡ˆã®å…ƒã¨ãªã‚‹ã‚¢ã‚¤ãƒ†ãƒ 
            draft_number: æŠ•ç¨¿æ¡ˆç•ªå·ï¼ˆ1, 2, 3...ï¼‰
            source_type: ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ï¼ˆã€Œå¿…è¦‹ã®æ›´æ–°ã€ã€Œå…¬å¼ç™ºè¡¨ã€ã€ŒXæ³¨ç›®æŠ•ç¨¿ã€ï¼‰
        """
        today = datetime.now().strftime('%Y/%m/%d')

        # XæŠ•ç¨¿ã®å ´åˆã¯çŸ­ç¸®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä½¿ç”¨
        use_shorter_format = (item.source in ["x_account", "x_search"])

        # æŠ•ç¨¿æ¡ˆç”Ÿæˆ
        source_name = item.metadata.get("feed_name", "") or item.metadata.get("username", "") or item.metadata.get("keyword", "")
        post = self._create_single_post(
            title=item.title,
            url=item.url,
            source_type=source_type,
            source_name=source_name,
            date=today,
            item=item
        )

        if not post:
            return

        # ãƒ–ãƒ­ãƒƒã‚¯æ§‹ç¯‰
        blocks = [
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": (
                        f"*ã€æŠ•ç¨¿æ¡ˆ {draft_number}ã€‘{item.title}*\n"
                        f"```{post}```\n"
                        f"<{item.url}|å…ƒè¨˜äº‹ã‚’è¦‹ã‚‹>"
                    )
                }
            }
        ]

        # Slacké€ä¿¡
        self._send_blocks(blocks)
        print(f"  âœ… æŠ•ç¨¿æ¡ˆ {draft_number} ã‚’é€ä¿¡: {item.title[:50]}...")

        # â˜… ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–: 1ç§’å¾…æ©Ÿï¼ˆå¿…é ˆï¼‰
        import time
        time.sleep(1)

    def _send_individual_draft_posts(
        self,
        must_include_items: List[Item],
        provider_items: List[Item],
        x_items: List[Item]
    ) -> None:
        """
        æŠ•ç¨¿æ¡ˆã‚’1ä»¶ãšã¤ç”Ÿæˆã—ã¦å€‹åˆ¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦é€ä¿¡

        å‡¦ç†é †åº:
        1. å¿…è¦‹ã®æ›´æ–°ï¼ˆmust_include_itemsï¼‰
        2. å…¬å¼ç™ºè¡¨ï¼ˆRSSã€provider_itemsï¼‰æœ€å¤§5ä»¶
        3. Xç”±æ¥ã®æŠ•ç¨¿ï¼ˆx_itemsï¼‰æœ€å¤§2ä»¶
        """
        draft_count = 0

        # â‘  å¿…è¦‹ã®æ›´æ–°ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        if must_include_items:
            section_header_blocks = [{
                "type": "section",
                "text": {"type": "mrkdwn", "text": "*â­ å¿…è¦‹ã®æ›´æ–°*"}
            }]
            self._send_blocks(section_header_blocks)
            print("\nğŸ“Œ å¿…è¦‹ã®æ›´æ–°ã‚»ã‚¯ã‚·ãƒ§ãƒ³")

            for item in must_include_items:
                draft_count += 1
                self._send_single_draft_post(item, draft_count, "å¿…è¦‹ã®æ›´æ–°")

        # â‘¡ å…¬å¼ç™ºè¡¨ï¼ˆRSSï¼‰æœ€å¤§5ä»¶
        if provider_items:
            section_header_blocks = [{
                "type": "section",
                "text": {"type": "mrkdwn", "text": "*ğŸ“¢ å…¬å¼ç™ºè¡¨*"}
            }]
            self._send_blocks(section_header_blocks)
            print("\nğŸ“Œ å…¬å¼ç™ºè¡¨ã‚»ã‚¯ã‚·ãƒ§ãƒ³")

            for item in provider_items[:5]:
                draft_count += 1
                self._send_single_draft_post(item, draft_count, "å…¬å¼ç™ºè¡¨")

        # â‘¢ Xç”±æ¥ã®æŠ•ç¨¿ æœ€å¤§2ä»¶
        if x_items:
            section_header_blocks = [{
                "type": "section",
                "text": {"type": "mrkdwn", "text": "*ğŸ¦ XæŠ•ç¨¿ã‹ã‚‰*"}
            }]
            self._send_blocks(section_header_blocks)
            print("\nğŸ“Œ XæŠ•ç¨¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³")

            for item in x_items[:2]:
                draft_count += 1
                self._send_single_draft_post(item, draft_count, "Xæ³¨ç›®æŠ•ç¨¿")

        print(f"\nğŸ“Š åˆè¨ˆ {draft_count} ä»¶ã®æŠ•ç¨¿æ¡ˆã‚’é€ä¿¡ã—ã¾ã—ãŸ")

    def _count_sources(self) -> Dict[str, int]:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã”ã¨ã®ã‚¢ã‚¤ãƒ†ãƒ æ•°ã‚’é›†è¨ˆ"""
        counts = {
            "x_posts": 0,      # XæŠ•ç¨¿ï¼ˆx_account + x_searchï¼‰
            "rss": 0,          # RSS
            "must_include": 0  # å¿…è¦‹ã®æ›´æ–°
        }

        for item in self.items:
            if item.source in ["x_account", "x_search"]:
                counts["x_posts"] += 1
            elif item.source == "rss":
                counts["rss"] += 1
            elif item.source == "must_include":
                counts["must_include"] += 1

        return counts

    def _format_source_summary(self, counts: Dict[str, int]) -> str:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é›†è¨ˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆå…¨ã‚½ãƒ¼ã‚¹ã‚’å¸¸ã«è¡¨ç¤ºï¼‰"""
        parts = []

        # XæŠ•ç¨¿ï¼ˆå¸¸ã«è¡¨ç¤ºï¼‰
        parts.append(f"XæŠ•ç¨¿ {counts['x_posts']}ä»¶")

        # RSSï¼ˆå¸¸ã«è¡¨ç¤ºï¼‰
        parts.append(f"RSS {counts['rss']}ä»¶")

        # å¿…è¦‹ã®æ›´æ–°ï¼ˆå¸¸ã«è¡¨ç¤ºï¼‰
        parts.append(f"å¿…è¦‹ã®æ›´æ–° {counts['must_include']}ä»¶")

        return "ğŸ“Š åˆ†æå¯¾è±¡: " + "ã€".join(parts)

    def _generate_x_post_draft(self, top_items: List[Item], provider_items: List[Item], github_items: List[Item]) -> str:
        """XæŠ•ç¨¿ç´ æ¡ˆã‚’ç”Ÿæˆï¼ˆè¨˜äº‹ã”ã¨ã«å€‹åˆ¥æŠ•ç¨¿ã‚’ä½œæˆï¼‰"""
        drafts = []
        seen_urls = set()  # URLé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨
        today = datetime.now().strftime('%Y/%m/%d')

        # RSSï¼ˆå…¬å¼ç™ºè¡¨ï¼‰ã‚’å„ªå…ˆçš„ã«æŠ•ç¨¿ç´ æ¡ˆä½œæˆï¼ˆAnthropicãªã©ã®é‡è¦ãªå…¬å¼ç™ºè¡¨ã‚’ç¢ºå®Ÿã«å«ã‚ã‚‹ï¼‰
        for item in provider_items[:7]:  # 5â†’7ã«å¢—åŠ 
            if item.url in seen_urls:
                continue
            seen_urls.add(item.url)

            feed_name = item.metadata.get("feed_name", "")
            post = self._create_single_post(
                title=item.title,
                url=item.url,
                source_type="å…¬å¼ç™ºè¡¨",
                source_name=feed_name,
                date=today,
                item=item
            )
            drafts.append(f"ã€æŠ•ç¨¿æ¡ˆ {len(drafts) + 1}ã€‘\n{post}")

        # GitHub Releaseã¯å‰Šé™¤ï¼ˆRSSã§å–å¾—ã™ã‚‹ãŸã‚ä¸è¦ï¼‰

        # ãƒˆãƒƒãƒ—ãƒã‚¤ãƒ©ã‚¤ãƒˆã‹ã‚‰è¿½åŠ ï¼ˆ2â†’1ã«å‰Šæ¸›ï¼‰
        for item in top_items[:1]:
            if item.url in seen_urls:
                continue
            if item.source in ["rss", "github"]:
                continue  # æ—¢ã«è¿½åŠ æ¸ˆã¿

            seen_urls.add(item.url)

            source_name = item.metadata.get("username", "") or item.metadata.get("keyword", "")
            post = self._create_single_post(
                title=item.title,
                url=item.url,
                source_type="Xæ³¨ç›®æŠ•ç¨¿",
                source_name=source_name,
                date=today,
                item=item
            )
            drafts.append(f"ã€æŠ•ç¨¿æ¡ˆ {len(drafts) + 1}ã€‘\n{post}")

        return "\n\n" + ("-" * 50) + "\n\n".join(drafts) if drafts else ""

    def _generate_x_post_draft_blocks(self, top_items: List[Item], provider_items: List[Item], github_items: List[Item], all_items: List[Item]) -> List[Dict]:
        """XæŠ•ç¨¿ç´ æ¡ˆã‚’Slack Blocksã¨ã—ã¦ç”Ÿæˆï¼ˆå„æŠ•ç¨¿ã‚’å€‹åˆ¥ãƒ–ãƒ­ãƒƒã‚¯ã«ï¼‰"""
        blocks = []
        seen_urls = set()
        today = datetime.now().strftime('%Y/%m/%d')
        draft_count = 0

        # ã€å¿…è¦‹ã®æ›´æ–°ã€‘ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        must_include_items = [i for i in all_items if i.metadata.get("must_include")]
        must_include_config = self.config.get("rss", {}).get("must_include_feeds", [])

        if must_include_config:
            blocks.append({"type": "divider"})
            blocks.append({
                "type": "header",
                "text": {"type": "plain_text", "text": "â­ å¿…è¦‹ã®æ›´æ–°"}
            })

            if must_include_items:
                # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
                from collections import defaultdict
                grouped = defaultdict(list)
                for item in must_include_items:
                    feed_name = item.metadata.get("feed_name", "Unknown")
                    grouped[feed_name].append(item)

                # å„ãƒ•ã‚£ãƒ¼ãƒ‰ã®æ›´æ–°ã‚’è¡¨ç¤º
                for feed_name, items in grouped.items():
                    for item in items:
                        seen_urls.add(item.url)
                        draft_count += 1

                        post = self._create_single_post(
                            title=item.title,
                            url=item.url,
                            source_type="å¿…è¦‹ã®æ›´æ–°",
                            source_name=feed_name,
                            date=today,
                            item=item
                        )

                        # æ¤œè¨¼å¤±æ•—æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—
                        if post is None:
                            print(f"â­ï¸  æŠ•ç¨¿æ¡ˆã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ¤œè¨¼å¤±æ•—ï¼‰: {item.title[:50]}...")
                            draft_count -= 1
                            continue

                        blocks.append({
                            "type": "section",
                            "text": {"type": "mrkdwn", "text": f"*ã€æŠ•ç¨¿æ¡ˆ {draft_count}ã€‘{feed_name}*"}
                        })
                        blocks.append({
                            "type": "section",
                            "text": {"type": "mrkdwn", "text": f"```{post}```"}
                        })
                        blocks.append({"type": "divider"})
            else:
                # æ›´æ–°ãŒãªã„å ´åˆ
                feed_names = [f["name"] for f in must_include_config]
                blocks.append({
                    "type": "section",
                    "text": {"type": "mrkdwn", "text": f"ğŸ“­ æœ¬æ—¥ã®æ›´æ–°ãªã—\nå¯¾è±¡: {', '.join(feed_names)}"}
                })
                blocks.append({"type": "divider"})

        # RSSï¼ˆå…¬å¼ç™ºè¡¨ï¼‰ã‚’5ä»¶ã«å‰Šæ¸›ï¼ˆ7â†’5ï¼‰
        for item in provider_items[:5]:
            if item.url in seen_urls:
                continue
            seen_urls.add(item.url)
            draft_count += 1

            feed_name = item.metadata.get("feed_name", "")
            post = self._create_single_post(
                title=item.title,
                url=item.url,
                source_type="å…¬å¼ç™ºè¡¨",
                source_name=feed_name,
                date=today,
                item=item
            )

            # æ¤œè¨¼å¤±æ•—æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—
            if post is None:
                print(f"â­ï¸  æŠ•ç¨¿æ¡ˆã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ¤œè¨¼å¤±æ•—ï¼‰: {item.title[:50]}...")
                draft_count -= 1
                continue

            # å„æŠ•ç¨¿ã‚’å€‹åˆ¥ã®sectionãƒ–ãƒ­ãƒƒã‚¯ã«ï¼ˆ3000æ–‡å­—åˆ¶é™å›é¿ï¼‰
            blocks.append({
                "type": "section",
                "text": {"type": "mrkdwn", "text": f"```ã€æŠ•ç¨¿æ¡ˆ {draft_count}ã€‘\n{post}```"}
            })

            # åŒºåˆ‡ã‚Šç·šã‚’è¿½åŠ ï¼ˆæœ€å¾Œä»¥å¤–ï¼‰
            if draft_count < 7:
                blocks.append({"type": "divider"})

        # Xç”±æ¥ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ç¢ºå®Ÿã«å«ã‚ã‚‹ï¼ˆæ–°è¦è¿½åŠ ï¼‰
        # é‡è¦: top_items ã§ã¯ãªã all_items ã‹ã‚‰æŠ½å‡ºï¼ˆtop_items ã¯3ä»¶ã—ã‹ãªã„ãŸã‚ï¼‰
        x_items = [i for i in all_items if i.source in ["x_account", "x_search"]]
        for item in x_items[:2]:  # Xç”±æ¥ã‚’æœ€å¤§2ä»¶è¿½åŠ 
            if item.url in seen_urls:
                continue
            seen_urls.add(item.url)
            draft_count += 1

            source_name = item.metadata.get("username", "") or item.metadata.get("keyword", "")
            post = self._create_single_post(
                title=item.title,
                url=item.url,
                source_type="Xæ³¨ç›®æŠ•ç¨¿",
                source_name=source_name,
                date=today,
                item=item
            )

            # æ¤œè¨¼å¤±æ•—æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—
            if post is None:
                print(f"â­ï¸  æŠ•ç¨¿æ¡ˆã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ¤œè¨¼å¤±æ•—ï¼‰: {item.title[:50]}...")
                draft_count -= 1
                continue

            blocks.append({
                "type": "section",
                "text": {"type": "mrkdwn", "text": f"```ã€æŠ•ç¨¿æ¡ˆ {draft_count}ã€‘\n{post}```"}
            })

            # åŒºåˆ‡ã‚Šç·šã‚’è¿½åŠ ï¼ˆæœ€å¾Œä»¥å¤–ï¼‰
            if draft_count < 7:
                blocks.append({"type": "divider"})

        return blocks

    def _create_single_post(self, title: str, url: str, source_type: str, source_name: str, date: str, item: Item) -> str:
        """å€‹åˆ¥ã®XæŠ•ç¨¿ã‚’ç”Ÿæˆ"""
        # Twitterã®URLã‚’x.comã«å¤‰æ›
        if "twitter.com" in url:
            url = url.replace("twitter.com", "x.com")

        # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’å–å¾—
        category = item.metadata.get("category", "UNKNOWN")

        # XæŠ•ç¨¿ã®å ´åˆã¯å…¨æ–‡ã‚‚å–å¾—
        tweet_text = None
        thread_tweets = None
        is_thread = False
        if item.source in ["x_account", "x_search"]:
            is_thread = item.metadata.get("is_thread", False)
            if is_thread:
                # ã‚¹ãƒ¬ãƒƒãƒ‰ã®å ´åˆ
                thread_tweets = item.metadata.get("thread_tweets", [])
            else:
                # å˜ä¸€ãƒ„ã‚¤ãƒ¼ãƒˆã®å ´åˆ
                tweet_text = item.metadata.get("tweet", {}).get("text", "")

        # Claude API ã§ã‚µãƒãƒ©ã‚¤ã‚ºç”Ÿæˆ
        summary = self._generate_summary_with_claude(
            title, url, source_type, category, tweet_text=tweet_text, thread_tweets=thread_tweets
        )

        return summary

    def _generate_summary_with_claude(self, title: str, url: str, source_type: str, category: str = "UNKNOWN", tweet_text: Optional[str] = None, thread_tweets: Optional[List[Dict]] = None) -> str:
        """Claude API ã§é«˜å“è³ªãªXæŠ•ç¨¿ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ç”Ÿæˆ"""
        try:
            import anthropic

            api_key = os.environ.get("ANTHROPIC_API_KEY")
            if not api_key:
                print("âš ï¸  ANTHROPIC_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚æŠ•ç¨¿æ¡ˆç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return None

            client = anthropic.Anthropic(api_key=api_key)

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæƒ…å ±ã‚’å–å¾—
            target = self.config.get("target_audience", {})
            target_name = target.get("name", "AIã«é–¢å¿ƒã®ã‚ã‚‹ãƒ“ã‚¸ãƒã‚¹ãƒ‘ãƒ¼ã‚½ãƒ³ã‚„ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢")

            # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®ãƒ•ã‚©ãƒ¼ã‚«ã‚¹
            category_focus = {
                "PRACTICAL": "å®Ÿè£…æ–¹æ³•ã€å…·ä½“çš„ãªæ©Ÿèƒ½ã€ä½¿ã„æ–¹ã€çµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³ã€å®Ÿè·µçš„ãªTipsã‚’é‡è¦–ã—ã¦ãã ã•ã„ã€‚",
                "TECHNICAL": "æŠ€è¡“çš„ãªä»•çµ„ã¿ã€æ¯”è¼ƒåˆ†æã€è©³ç´°ãªè§£èª¬ã‚’é‡è¦–ã—ã¦ãã ã•ã„ã€‚",
                "GENERAL": "æ–°æ©Ÿèƒ½ã®æ¦‚è¦ã€åˆ©ç”¨é–‹å§‹æ™‚æœŸã€å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’é‡è¦–ã—ã¦ãã ã•ã„ã€‚"
            }.get(category, "")

            # å…±é€šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
            system_prompt = get_system_prompt()

            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            if thread_tweets:
                # Xã‚¹ãƒ¬ãƒƒãƒ‰ã®å ´åˆï¼šã‚¹ãƒ¬ãƒƒãƒ‰å†…ã®URLã‹ã‚‰è¨˜äº‹æœ¬æ–‡ã‚’å–å¾—
                import re
                article_content = None

                for tweet in thread_tweets:
                    urls_in_tweet = re.findall(r'https?://[^\s]+', tweet["text"])
                    for url_in_tweet in urls_in_tweet:
                        _, content = fetch_article_content_safe(url_in_tweet)
                        if content:
                            article_content = content
                            break
                    if article_content:
                        break

                user_prompt = create_user_prompt_from_thread(url, thread_tweets, article_content)
            elif tweet_text:
                # XæŠ•ç¨¿ã®å ´åˆï¼šãƒ„ã‚¤ãƒ¼ãƒˆå†…ã®URLã‹ã‚‰è¨˜äº‹æœ¬æ–‡ã‚’å–å¾—ï¼ˆãƒ•ã‚§ãƒ¼ã‚º3ï¼‰
                import re
                urls_in_tweet = re.findall(r'https?://[^\s]+', tweet_text)
                article_content = None

                if urls_in_tweet:
                    for url_in_tweet in urls_in_tweet:
                        _, content = fetch_article_content_safe(url_in_tweet)
                        if content:
                            article_content = content
                            break

                user_prompt = create_user_prompt_from_tweet(url, tweet_text, article_content)
            else:
                # RSSè¨˜äº‹ã®å ´åˆï¼šè¨˜äº‹æœ¬æ–‡ã‚’å–å¾—ï¼ˆãƒ•ã‚§ãƒ¼ã‚º2ï¼‰
                article_title, article_content = fetch_article_content_safe(url)

                if not article_content:
                    print(f"âš ï¸ è¨˜äº‹æœ¬æ–‡å–å¾—å¤±æ•—ã€ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ã§å‡¦ç†: {url}")
                    article_content = title

                user_prompt = create_user_prompt_from_article(
                    url,
                    article_title or title,
                    article_content
                )

            # AI-lintè‡ªå‹•ä¿®æ­£ï¼ˆæœ€å¤§2å›è©¦è¡Œã€è‡ªå‹•ãƒ•ãƒ­ãƒ¼ãªã®ã§é…å»¶æœ€å°åŒ–ï¼‰
            max_retries = 1
            score_threshold = 15
            generated_text = None
            detected_issues = None

            for attempt in range(max_retries + 1):
                message = client.messages.create(
                    model="claude-sonnet-4-5-20250929",
                    max_tokens=1500,  # 800æ–‡å­—è¦æ±‚ãªã®ã§ä½™è£•ã‚’æŒãŸã›ã‚‹
                    system=system_prompt,  # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¿½åŠ 
                    messages=[{
                        "role": "user",
                        "content": user_prompt if attempt == 0 else user_prompt + f"\n\nã€é‡è¦ï¼šä»¥ä¸‹ã®è¡¨ç¾ãŒæ¤œå‡ºã•ã‚ŒãŸã®ã§å¿…ãšä¿®æ­£ã—ã¦ãã ã•ã„ã€‘\n" + "\n".join([f"âŒ ã€Œ{issue.matched_text}ã€â†’ {issue.suggestion}" for issue in detected_issues[:5]])
                    }]
                )

                generated_text = message.content[0].text

                # AI-lintãƒã‚§ãƒƒã‚¯
                lint_result = self.ai_lint_checker.check(generated_text)

                if lint_result.score == 0:
                    break  # AIçš„è¡¨ç¾ãªã—
                elif lint_result.score < score_threshold:
                    break  # è¨±å®¹ç¯„å›²å†…
                else:
                    if attempt < max_retries:
                        detected_issues = lint_result.detections
                        # å†ç”Ÿæˆï¼ˆæ¬¡ã®ãƒ«ãƒ¼ãƒ—ã§ä¿®æ­£æŒ‡ç¤ºã‚’è¿½åŠ ï¼‰
                    else:
                        # æœ€å¤§è©¦è¡Œå›æ•°åˆ°é”ã€è­¦å‘Šã‚’å‡ºã™ãŒç¶šè¡Œ
                        print(f"âš ï¸  AI-lint: ã‚¹ã‚³ã‚¢ {lint_result.score} (é–¾å€¤è¶…éã€ç¶šè¡Œ)")

            # æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º1: æ­£è¦è¡¨ç¾ãƒ™ãƒ¼ã‚¹
            validation_result = self.validator.validate_post(generated_text, title)

            if not validation_result.is_valid:
                print(f"âš ï¸  æŠ•ç¨¿æ¡ˆãŒæ¤œè¨¼å¤±æ•—: {validation_result.rejection_reason}")
                print(f"    ã‚¿ã‚¤ãƒˆãƒ«: {title[:50]}...")
                print(f"    æ¤œå‡ºå•é¡Œ: {validation_result.detected_issues}")
                return None

            # æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º2: Claude APIãƒ¬ãƒ“ãƒ¥ãƒ¼
            review_result = self.validator.review_post_with_claude(generated_text, title, url)

            if not review_result.is_valid:
                print(f"âš ï¸  æŠ•ç¨¿æ¡ˆãŒãƒ¬ãƒ“ãƒ¥ãƒ¼å¤±æ•—: {review_result.rejection_reason}")
                print(f"    ã‚¿ã‚¤ãƒˆãƒ«: {title[:50]}...")
                print(f"    æ¤œå‡ºå•é¡Œ: {review_result.detected_issues}")
                return None

            return generated_text

        except Exception as e:
            print(f"âš ï¸ Claude API ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯è¡Œã‚ãšã€None ã‚’è¿”ã™ï¼ˆæ¤œè¨¼å¤±æ•—ã¨åŒã˜æ‰±ã„ï¼‰
            return None



def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("AIãƒ‡ã‚¤ãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆ")
    print("=" * 60)

    # è¨­å®šèª­ã¿è¾¼ã¿
    with open("config.yaml", 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
    x_bearer_token = os.environ.get("X_BEARER_TOKEN")
    slack_webhook_url = os.environ.get("SLACK_WEBHOOK_URL")

    if not x_bearer_token:
        raise ValueError("ç’°å¢ƒå¤‰æ•° X_BEARER_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    if not slack_webhook_url:
        raise ValueError("ç’°å¢ƒå¤‰æ•° SLACK_WEBHOOK_URL ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

    # åˆæœŸåŒ–
    state = StateManager("data/state.json")
    x_client = XAPIClient(x_bearer_token)
    collector = DataCollector(config, state, x_client)

    try:
        # ãƒ‡ãƒ¼ã‚¿åé›†
        collector.collect_all()

        # Slackãƒ¬ãƒãƒ¼ãƒˆé€ä¿¡
        reporter = SlackReporter(slack_webhook_url, config, collector.items, collector.stats)
        reporter.send()

        # çŠ¶æ…‹ä¿å­˜ï¼ˆæœ€å¾Œã«å®Ÿè¡Œï¼‰
        state.save()
        print("ğŸ’¾ çŠ¶æ…‹ã‚’ä¿å­˜ã—ã¾ã—ãŸ")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        raise

    print("=" * 60)
    print("âœ… å®Œäº†")
    print("=" * 60)


if __name__ == "__main__":
    main()
