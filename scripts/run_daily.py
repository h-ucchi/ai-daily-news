#!/usr/bin/env python3
"""
AIãƒ‡ã‚¤ãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
X API Basic ($200/æœˆ) ã«åã¾ã‚‹ã‚ˆã†è¨­è¨ˆ
"""

import os
import json
import yaml
import requests
import feedparser
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from collections import defaultdict
import time
from content_classifier import ContentClassifier, ClassificationResult


@dataclass
class Item:
    """åé›†ã—ãŸæƒ…å ±ã‚¢ã‚¤ãƒ†ãƒ """
    source: str  # "x_account", "x_search", "rss", "github"
    title: str
    url: str
    published_at: str
    score: int = 0
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class StateManager:
    """çŠ¶æ…‹ç®¡ç†ï¼ˆstate.jsonï¼‰"""

    def __init__(self, state_path: str = "data/state.json"):
        self.state_path = state_path
        self.state = self._load()

    def _load(self) -> Dict:
        """state.json ã‚’èª­ã¿è¾¼ã¿"""
        if os.path.exists(self.state_path):
            with open(self.state_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {
            "x_accounts": {},
            "x_keywords": {},
            "rss": {},
            "github": {},
            "meta": {"last_run_at": None, "version": "1.0.0"}
        }

    def save(self):
        """state.json ã‚’ä¿å­˜"""
        self.state["meta"]["last_run_at"] = datetime.now(timezone.utc).isoformat()
        with open(self.state_path, 'w', encoding='utf-8') as f:
            json.dump(self.state, f, indent=2, ensure_ascii=False)

    def get_x_account_since_id(self, username: str) -> Optional[str]:
        """Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆã® since_id ã‚’å–å¾—"""
        return self.state["x_accounts"].get(username, {}).get("since_id")

    def set_x_account_since_id(self, username: str, user_id: str, since_id: str):
        """Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆã® since_id ã‚’æ›´æ–°"""
        if username not in self.state["x_accounts"]:
            self.state["x_accounts"][username] = {}
        self.state["x_accounts"][username]["user_id"] = user_id
        self.state["x_accounts"][username]["since_id"] = since_id

    def get_x_keyword_since_id(self, keyword: str) -> Optional[str]:
        """Xã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã® since_id ã‚’å–å¾—"""
        return self.state["x_keywords"].get(keyword, {}).get("since_id")

    def set_x_keyword_since_id(self, keyword: str, since_id: str):
        """Xã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã® since_id ã‚’æ›´æ–°"""
        if keyword not in self.state["x_keywords"]:
            self.state["x_keywords"][keyword] = {}
        self.state["x_keywords"][keyword]["since_id"] = since_id

    def get_rss_last_published(self, feed_url: str) -> Optional[str]:
        """RSSã®æœ€çµ‚å–å¾—æ—¥æ™‚ã‚’å–å¾—"""
        return self.state["rss"].get(feed_url)

    def set_rss_last_published(self, feed_url: str, published_at: str):
        """RSSã®æœ€çµ‚å–å¾—æ—¥æ™‚ã‚’æ›´æ–°"""
        self.state["rss"][feed_url] = published_at

    def get_github_last_tag(self, repo: str) -> Optional[str]:
        """GitHubãƒªãƒã‚¸ãƒˆãƒªã®æœ€çµ‚tagã‚’å–å¾—"""
        return self.state["github"].get(repo, {}).get("tag")

    def set_github_last_tag(self, repo: str, tag: str):
        """GitHubãƒªãƒã‚¸ãƒˆãƒªã®æœ€çµ‚tagã‚’æ›´æ–°"""
        if repo not in self.state["github"]:
            self.state["github"][repo] = {}
        self.state["github"][repo]["tag"] = tag


class XAPIClient:
    """X (Twitter) API v2 ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""

    def __init__(self, bearer_token: str):
        self.bearer_token = bearer_token
        self.base_url = "https://api.twitter.com/2"
        self.headers = {"Authorization": f"Bearer {bearer_token}"}

    def get_user_id(self, username: str) -> Optional[str]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’å–å¾—"""
        url = f"{self.base_url}/users/by/username/{username}"
        response = requests.get(url, headers=self.headers)
        if response.status_code == 200:
            data = response.json()
            return data.get("data", {}).get("id")
        return None

    def get_user_tweets(self, user_id: str, since_id: Optional[str] = None, max_results: int = 10) -> List[Dict]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å–å¾—ï¼ˆæ–°ç€ã®ã¿ï¼‰"""
        url = f"{self.base_url}/users/{user_id}/tweets"
        params = {
            "max_results": min(max_results, 100),
            "tweet.fields": "created_at,public_metrics",
            "expansions": "author_id"
        }
        if since_id:
            params["since_id"] = since_id
        else:
            # åˆå›å®Ÿè¡Œæ™‚ã¯ç›´è¿‘24æ™‚é–“
            start_time = (datetime.now(timezone.utc) - timedelta(hours=24)).isoformat()
            params["start_time"] = start_time

        response = requests.get(url, headers=self.headers, params=params)
        if response.status_code == 200:
            data = response.json()
            return data.get("data", [])
        return []

    def search_tweets(self, query: str, since_id: Optional[str] = None, max_results: int = 10) -> List[Dict]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ„ã‚¤ãƒ¼ãƒˆæ¤œç´¢ï¼ˆæ–°ç€ã®ã¿ï¼‰"""
        url = f"{self.base_url}/tweets/search/recent"
        params = {
            "query": query,
            "max_results": min(max_results, 100),
            "tweet.fields": "created_at,public_metrics",
            "expansions": "author_id"
        }
        if since_id:
            params["since_id"] = since_id
        else:
            # åˆå›å®Ÿè¡Œæ™‚ã¯ç›´è¿‘24æ™‚é–“
            start_time = (datetime.now(timezone.utc) - timedelta(hours=24)).isoformat()
            params["start_time"] = start_time

        response = requests.get(url, headers=self.headers, params=params)
        if response.status_code == 200:
            data = response.json()
            return data.get("data", [])
        return []


class DataCollector:
    """ãƒ‡ãƒ¼ã‚¿åé›†ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼"""

    # å„ªå…ˆåº¦ã®é«˜ã„RSSãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆé‡è¦ãƒ™ãƒ³ãƒ€ãƒ¼ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¼ã‚‰ã•ãªã„ãŸã‚ï¼‰
    PRIORITY_FEEDS = {
        # å…¬å¼ãƒ–ãƒ­ã‚°
        "https://www.anthropic.com/news/rss.xml": 1000,  # Anthropicæœ€å„ªå…ˆ
        "https://openai.com/blog/rss.xml": 1000,          # OpenAIæœ€å„ªå…ˆ
        "https://github.blog/feed/": 800,                 # GitHub Blog
        "https://code.visualstudio.com/updates/feed.xml": 800,  # VSCode Updates

        # GitHub Releases Atom Feedï¼ˆå¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã¯must_include_feedsã§ç®¡ç†ï¼‰
        "https://github.com/anthropics/claude-code/releases.atom": 800,  # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã«ç§»è¡Œ
        "https://github.blog/changelog/label/copilot/feed/": 800,        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã«ç§»è¡Œ
        "https://github.com/langchain-ai/langchain/releases.atom": 800,   # LangChain
        "https://github.com/openai/openai-python/releases.atom": 800,     # OpenAI Python SDK
        "https://github.com/run-llama/llama_index/releases.atom": 600,    # LlamaIndex
        "https://github.com/huggingface/transformers/releases.atom": 600, # Transformers
    }

    def __init__(self, config: Dict, state: StateManager, x_client: XAPIClient):
        self.config = config
        self.state = state
        self.x_client = x_client
        self.items: List[Item] = []
        self.stats = {
            "x_accounts_fetched": 0,
            "x_search_fetched": 0,
            "x_total_fetched": 0,
            "x_limit_reached": False,
            "rss_fetched": 0,
            "github_fetched": 0,
            "duplicates_removed": 0
        }
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†é¡å™¨ã®åˆæœŸåŒ–
        if self.config.get("content_filtering", {}).get("enabled"):
            self.classifier = ContentClassifier(config)
        else:
            self.classifier = None

    def collect_all(self):
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†"""
        print("ğŸ” ãƒ‡ãƒ¼ã‚¿åé›†ã‚’é–‹å§‹...")

        # X (Twitter)
        self._collect_x_accounts()
        self._collect_x_search()

        # RSSï¼ˆGitHub Releases Atom Feedã‚’å«ã‚€ï¼‰
        self._collect_rss()

        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆå½“æ—¥ã®æ›´æ–°ãŒã‚ã‚Œã°å¿…ãšå«ã‚ã‚‹ï¼‰
        must_include_items = self._collect_must_include_feeds()
        self.items.extend(must_include_items)
        self.stats["must_include_fetched"] = len(must_include_items)

        # é‡è¤‡æ’é™¤
        self._deduplicate()

        print(f"âœ… åé›†å®Œäº†: {len(self.items)} ä»¶")

    def _collect_x_accounts(self):
        """Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‹ã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆåé›†"""
        accounts = self.config["x"]["accounts"]
        limit = self.config["x"]["limits"]["accounts"]
        fetched = 0

        print(f"ğŸ“± Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆç›£è¦–: {len(accounts)} ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ")

        for username in accounts:
            if fetched >= limit:
                print(f"âš ï¸  ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç›£è¦–ã®ä¸Šé™ {limit} ä»¶ã«åˆ°é”")
                self.stats["x_limit_reached"] = True
                break

            user_id = self.x_client.get_user_id(username)
            if not user_id:
                continue

            since_id = self.state.get_x_account_since_id(username)
            tweets = self.x_client.get_user_tweets(user_id, since_id, max_results=10)

            if not tweets:
                continue

            # æœ€æ–°ã®tweet_idã‚’ä¿å­˜
            max_id = max(int(t["id"]) for t in tweets)
            self.state.set_x_account_since_id(username, user_id, str(max_id))

            for tweet in tweets:
                if fetched >= limit:
                    break

                # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã¨ã‚¹ã‚³ã‚¢èª¿æ•´
                initial_score = self._calculate_engagement_score(tweet)
                category = "UNKNOWN"
                if self.classifier:
                    classification = self.classifier.classify_by_keywords(tweet["text"])
                    category = classification.category
                    final_score = self.classifier.calculate_final_score(initial_score, category, "x_account")
                else:
                    final_score = initial_score

                item = Item(
                    source="x_account",
                    title=tweet["text"][:100],
                    url=f"https://twitter.com/{username}/status/{tweet['id']}",
                    published_at=tweet["created_at"],
                    score=final_score,
                    metadata={
                        "username": username,
                        "tweet": tweet,
                        "category": category
                    }
                )
                self.items.append(item)
                fetched += 1

        self.stats["x_accounts_fetched"] = fetched
        self.stats["x_total_fetched"] += fetched

    def _collect_x_search(self):
        """Xã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢"""
        keywords = self.config["x"]["keywords"]
        limit = self.config["x"]["limits"]["search"]
        fetched = 0

        print(f"ğŸ” Xã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢: {len(keywords)} ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")

        for keyword in keywords:
            if fetched >= limit:
                print(f"âš ï¸  æ¤œç´¢ã®ä¸Šé™ {limit} ä»¶ã«åˆ°é”")
                self.stats["x_limit_reached"] = True
                break

            since_id = self.state.get_x_keyword_since_id(keyword)
            tweets = self.x_client.search_tweets(keyword, since_id, max_results=10)

            if not tweets:
                continue

            # æœ€æ–°ã®tweet_idã‚’ä¿å­˜
            max_id = max(int(t["id"]) for t in tweets)
            self.state.set_x_keyword_since_id(keyword, str(max_id))

            for tweet in tweets:
                if fetched >= limit:
                    break

                # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã¨ã‚¹ã‚³ã‚¢èª¿æ•´
                initial_score = self._calculate_engagement_score(tweet)
                category = "UNKNOWN"
                if self.classifier:
                    classification = self.classifier.classify_by_keywords(tweet["text"])
                    category = classification.category
                    final_score = self.classifier.calculate_final_score(initial_score, category, "x_search")
                else:
                    final_score = initial_score

                item = Item(
                    source="x_search",
                    title=tweet["text"][:100],
                    url=f"https://twitter.com/i/web/status/{tweet['id']}",
                    published_at=tweet["created_at"],
                    score=final_score,
                    metadata={
                        "keyword": keyword,
                        "tweet": tweet,
                        "category": category
                    }
                )
                self.items.append(item)
                fetched += 1

        self.stats["x_search_fetched"] = fetched
        self.stats["x_total_fetched"] += fetched

    def _collect_rss(self):
        """RSSåé›†"""
        feeds = self.config["rss"]["feeds"]
        fetched = 0

        print(f"ğŸ“° RSSåé›†: {len(feeds)} ãƒ•ã‚£ãƒ¼ãƒ‰")

        for feed_config in feeds:
            feed_url = feed_config["url"]
            feed_name = feed_config["name"]

            feed = feedparser.parse(feed_url)
            last_published = self.state.get_rss_last_published(feed_url)

            for entry in feed.entries:
                published = entry.get("published_parsed") or entry.get("updated_parsed")
                if not published:
                    continue

                published_dt = datetime(*published[:6], tzinfo=timezone.utc)
                published_iso = published_dt.isoformat()

                # æ–°ç€ã®ã¿
                if last_published and published_iso <= last_published:
                    continue

                # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°: åŸºæœ¬ã‚¹ã‚³ã‚¢ + å„ªå…ˆåº¦ãƒœãƒ¼ãƒŠã‚¹
                base_score = self.config["slack"]["scoring"]["rss_bonus"]  # 500
                priority_bonus = self.PRIORITY_FEEDS.get(feed_url, 0)
                initial_score = base_score + priority_bonus

                # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã¨ã‚¹ã‚³ã‚¢èª¿æ•´
                category = "UNKNOWN"
                if self.classifier:
                    description = entry.get("summary", "")
                    classification = self.classifier.classify_by_keywords(entry.title, description)
                    category = classification.category
                    final_score = self.classifier.calculate_final_score(initial_score, category, "rss")
                else:
                    final_score = initial_score

                item = Item(
                    source="rss",
                    title=entry.title,
                    url=entry.link,
                    published_at=published_iso,
                    score=final_score,  # é‡è¦ãƒ•ã‚£ãƒ¼ãƒ‰ + ã‚«ãƒ†ã‚´ãƒªãƒœãƒ¼ãƒŠã‚¹
                    metadata={
                        "feed_name": feed_name,
                        "feed_url": feed_url,
                        "category": category
                    }
                )
                self.items.append(item)
                fetched += 1

            # æœ€æ–°ã® published_at ã‚’ä¿å­˜
            if feed.entries:
                latest = max(feed.entries, key=lambda e: e.get("published_parsed") or e.get("updated_parsed"))
                published = latest.get("published_parsed") or latest.get("updated_parsed")
                if published:
                    published_dt = datetime(*published[:6], tzinfo=timezone.utc)
                    self.state.set_rss_last_published(feed_url, published_dt.isoformat())

        self.stats["rss_fetched"] = fetched

    def _collect_must_include_feeds(self) -> List[Item]:
        """å¿…ãšå«ã‚ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰å½“æ—¥ã®æ›´æ–°ã‚’å–å¾—"""
        must_include_config = self.config.get("rss", {}).get("must_include_feeds", [])
        must_include_items = []

        if not must_include_config:
            return must_include_items

        print(f"â­ å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰åé›†: {len(must_include_config)} ãƒ•ã‚£ãƒ¼ãƒ‰")

        # å½“æ—¥ã®æ—¥ä»˜ï¼ˆUTCã§00:00:00ï¼‰
        today = datetime.now(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)

        for feed_config in must_include_config:
            feed_url = feed_config["url"]
            feed_name = feed_config["name"]
            max_items = feed_config.get("max_items", 3)

            feed = feedparser.parse(feed_url)
            count = 0

            for entry in feed.entries:
                if count >= max_items:
                    break

                published = entry.get("published_parsed") or entry.get("updated_parsed")
                if not published:
                    continue

                published_dt = datetime(*published[:6], tzinfo=timezone.utc)
                published_iso = published_dt.isoformat()

                # å½“æ—¥ã®æ›´æ–°ã®ã¿ï¼ˆ00:00:00ä»¥é™ï¼‰
                if published_dt < today:
                    continue

                # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
                category = "PRACTICAL"  # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§PRACTICAL
                if self.classifier:
                    description = entry.get("summary", "")
                    classification = self.classifier.classify_by_keywords(entry.title, description)
                    category = classification.category

                item = Item(
                    source="must_include",
                    title=entry.title,
                    url=entry.link,
                    published_at=published_iso,
                    score=9999,  # æœ€é«˜ã‚¹ã‚³ã‚¢ï¼ˆå¿…ãšå«ã¾ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ï¼‰
                    metadata={
                        "feed_name": feed_name,
                        "feed_url": feed_url,
                        "category": category,
                        "must_include": True
                    }
                )
                must_include_items.append(item)
                count += 1
                print(f"  âœ“ {feed_name}: {entry.title[:50]}...")

        return must_include_items

    def _select_diverse_provider_items(self, sorted_items: List[Item], limit: int) -> List[Item]:
        """
        å„ªå…ˆåº¦ã®é«˜ã„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æœ€æ–°è¨˜äº‹ã‚’1ä»¶ãšã¤ãƒãƒ©ãƒ³ã‚¹ã‚ˆãé¸æŠ

        Args:
            sorted_items: ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã®ã‚¢ã‚¤ãƒ†ãƒ ãƒªã‚¹ãƒˆ
            limit: é¸æŠã™ã‚‹æœ€å¤§ä»¶æ•°

        Returns:
            ãƒãƒ©ãƒ³ã‚¹ã‚ˆãé¸æŠã•ã‚ŒãŸRSSã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆ
        """
        from collections import defaultdict

        # RSSã‚¢ã‚¤ãƒ†ãƒ ã®ã¿æŠ½å‡º
        rss_items = [i for i in sorted_items if i.source == "rss"]

        # ãƒ•ã‚£ãƒ¼ãƒ‰åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        feed_groups = defaultdict(list)
        for item in rss_items:
            feed_url = item.metadata.get("feed_url", "")
            if feed_url:
                feed_groups[feed_url].append(item)

        # å„ªå…ˆé †ä½é †ã«ã‚½ãƒ¼ãƒˆï¼ˆPRIORITY_FEEDS ã®å®šç¾©é †ï¼‰
        priority_order = list(self.PRIORITY_FEEDS.keys())
        sorted_feed_urls = sorted(
            feed_groups.keys(),
            key=lambda url: priority_order.index(url) if url in priority_order else 9999
        )

        # ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ­ãƒ“ãƒ³æ–¹å¼ã§é¸æŠï¼ˆå„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰1ä»¶ãšã¤ï¼‰
        selected = []
        round_num = 0

        while len(selected) < limit:
            added_this_round = False

            for feed_url in sorted_feed_urls:
                if len(selected) >= limit:
                    break

                items = feed_groups[feed_url]
                if round_num < len(items):
                    selected.append(items[round_num])
                    added_this_round = True

            # å…¨ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰é¸æŠã—çµ‚ãˆãŸ
            if not added_this_round:
                break

            round_num += 1

        return selected[:limit]

    def _calculate_engagement_score(self, tweet: Dict) -> int:
        """ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢è¨ˆç®—"""
        metrics = tweet.get("public_metrics", {})
        scoring = self.config["slack"]["scoring"]

        score = (
            metrics.get("like_count", 0) * scoring["like_weight"] +
            metrics.get("retweet_count", 0) * scoring["retweet_weight"] +
            metrics.get("reply_count", 0) * scoring["reply_weight"]
        )
        return score

    def _deduplicate(self):
        """é‡è¤‡æ’é™¤ï¼ˆURLåŸºæº–ï¼‰"""
        seen_urls = set()
        unique_items = []

        for item in self.items:
            if item.url not in seen_urls:
                seen_urls.add(item.url)
                unique_items.append(item)
            else:
                self.stats["duplicates_removed"] += 1

        self.items = unique_items


class SlackReporter:
    """Slackãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ»æŠ•ç¨¿"""

    # DataCollectorã®PRIORITY_FEEDSã¨åŒã˜å®šç¾©
    PRIORITY_FEEDS = {
        # å…¬å¼ãƒ–ãƒ­ã‚°
        "https://www.anthropic.com/news/rss.xml": 1000,
        "https://openai.com/blog/rss.xml": 1000,
        "https://github.blog/feed/": 800,
        "https://code.visualstudio.com/updates/feed.xml": 800,

        # GitHub Releases Atom Feedï¼ˆå¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã¯must_include_feedsã§ç®¡ç†ï¼‰
        "https://github.com/anthropics/claude-code/releases.atom": 800,
        "https://github.blog/changelog/label/copilot/feed/": 800,
        "https://github.com/langchain-ai/langchain/releases.atom": 800,
        "https://github.com/openai/openai-python/releases.atom": 800,
        "https://github.com/run-llama/llama_index/releases.atom": 600,
        "https://github.com/huggingface/transformers/releases.atom": 600,
    }

    def __init__(self, webhook_url: str, config: Dict, items: List[Item], stats: Dict):
        self.webhook_url = webhook_url
        self.config = config
        self.items = items
        self.stats = stats

    def _select_diverse_provider_items(self, sorted_items: List[Item], limit: int) -> List[Item]:
        """
        å„ªå…ˆåº¦ã®é«˜ã„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æœ€æ–°è¨˜äº‹ã‚’1ä»¶ãšã¤ãƒãƒ©ãƒ³ã‚¹ã‚ˆãé¸æŠ

        Args:
            sorted_items: ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã®ã‚¢ã‚¤ãƒ†ãƒ ãƒªã‚¹ãƒˆ
            limit: é¸æŠã™ã‚‹æœ€å¤§ä»¶æ•°

        Returns:
            ãƒãƒ©ãƒ³ã‚¹ã‚ˆãé¸æŠã•ã‚ŒãŸRSSã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆ
        """
        from collections import defaultdict

        # RSSã‚¢ã‚¤ãƒ†ãƒ ã®ã¿æŠ½å‡º
        rss_items = [i for i in sorted_items if i.source == "rss"]

        # ãƒ•ã‚£ãƒ¼ãƒ‰åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        feed_groups = defaultdict(list)
        for item in rss_items:
            feed_url = item.metadata.get("feed_url", "")
            if feed_url:
                feed_groups[feed_url].append(item)

        # å„ªå…ˆé †ä½é †ã«ã‚½ãƒ¼ãƒˆï¼ˆPRIORITY_FEEDS ã®å®šç¾©é †ï¼‰
        priority_order = list(self.PRIORITY_FEEDS.keys())
        sorted_feed_urls = sorted(
            feed_groups.keys(),
            key=lambda url: priority_order.index(url) if url in priority_order else 9999
        )

        # ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ­ãƒ“ãƒ³æ–¹å¼ã§é¸æŠï¼ˆå„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰1ä»¶ãšã¤ï¼‰
        selected = []
        round_num = 0

        while len(selected) < limit:
            added_this_round = False

            for feed_url in sorted_feed_urls:
                if len(selected) >= limit:
                    break

                items = feed_groups[feed_url]
                if round_num < len(items):
                    selected.append(items[round_num])
                    added_this_round = True

            # å…¨ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰é¸æŠã—çµ‚ãˆãŸ
            if not added_this_round:
                break

            round_num += 1

        return selected[:limit]

    def send(self):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¦Slackã«æŠ•ç¨¿"""
        print("ğŸ“¤ Slackãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")

        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_items = sorted(self.items, key=lambda x: x.score, reverse=True)

        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†ã‘
        top_items = sorted_items[:self.config["slack"]["limits"]["top"]]
        provider_items = self._select_diverse_provider_items(sorted_items, self.config["slack"]["limits"]["provider_official"])
        github_items = [i for i in sorted_items if i.source == "github"][:self.config["slack"]["limits"]["github_updates"]]

        # Slack Blocksæ§‹ç¯‰
        blocks = []

        # ãƒ˜ãƒƒãƒ€ãƒ¼
        blocks.append({
            "type": "header",
            "text": {"type": "plain_text", "text": f"ğŸ¦ XæŠ•ç¨¿ç´ æ¡ˆ - {datetime.now().strftime('%Y-%m-%d')}"}
        })

        # XæŠ•ç¨¿ç´ æ¡ˆã‚’ç”Ÿæˆï¼ˆå€‹åˆ¥ã®ãƒ–ãƒ­ãƒƒã‚¯ã¨ã—ã¦è¿½åŠ ï¼‰
        draft_blocks = self._generate_x_post_draft_blocks(top_items, provider_items, github_items, sorted_items)
        blocks.extend(draft_blocks)

        # é€ä¿¡
        payload = {"blocks": blocks}
        response = requests.post(self.webhook_url, json=payload)

        if response.status_code == 200:
            print("âœ… Slackã«æŠ•ç¨¿ã—ã¾ã—ãŸ")
        else:
            print(f"âŒ SlackæŠ•ç¨¿å¤±æ•—: {response.status_code} {response.text}")
            raise Exception("SlackæŠ•ç¨¿ã«å¤±æ•—ã—ã¾ã—ãŸ")

    def _generate_x_post_draft(self, top_items: List[Item], provider_items: List[Item], github_items: List[Item]) -> str:
        """XæŠ•ç¨¿ç´ æ¡ˆã‚’ç”Ÿæˆï¼ˆè¨˜äº‹ã”ã¨ã«å€‹åˆ¥æŠ•ç¨¿ã‚’ä½œæˆï¼‰"""
        drafts = []
        seen_urls = set()  # URLé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨
        today = datetime.now().strftime('%Y/%m/%d')

        # RSSï¼ˆå…¬å¼ç™ºè¡¨ï¼‰ã‚’å„ªå…ˆçš„ã«æŠ•ç¨¿ç´ æ¡ˆä½œæˆï¼ˆAnthropicãªã©ã®é‡è¦ãªå…¬å¼ç™ºè¡¨ã‚’ç¢ºå®Ÿã«å«ã‚ã‚‹ï¼‰
        for item in provider_items[:7]:  # 5â†’7ã«å¢—åŠ 
            if item.url in seen_urls:
                continue
            seen_urls.add(item.url)

            feed_name = item.metadata.get("feed_name", "")
            post = self._create_single_post(
                title=item.title,
                url=item.url,
                source_type="å…¬å¼ç™ºè¡¨",
                source_name=feed_name,
                date=today,
                item=item
            )
            drafts.append(f"ã€æŠ•ç¨¿æ¡ˆ {len(drafts) + 1}ã€‘\n{post}")

        # GitHub Releaseã¯å‰Šé™¤ï¼ˆRSSã§å–å¾—ã™ã‚‹ãŸã‚ä¸è¦ï¼‰

        # ãƒˆãƒƒãƒ—ãƒã‚¤ãƒ©ã‚¤ãƒˆã‹ã‚‰è¿½åŠ ï¼ˆ2â†’1ã«å‰Šæ¸›ï¼‰
        for item in top_items[:1]:
            if item.url in seen_urls:
                continue
            if item.source in ["rss", "github"]:
                continue  # æ—¢ã«è¿½åŠ æ¸ˆã¿

            seen_urls.add(item.url)

            source_name = item.metadata.get("username", "") or item.metadata.get("keyword", "")
            post = self._create_single_post(
                title=item.title,
                url=item.url,
                source_type="Xæ³¨ç›®æŠ•ç¨¿",
                source_name=source_name,
                date=today,
                item=item
            )
            drafts.append(f"ã€æŠ•ç¨¿æ¡ˆ {len(drafts) + 1}ã€‘\n{post}")

        return "\n\n" + ("-" * 50) + "\n\n".join(drafts) if drafts else ""

    def _generate_x_post_draft_blocks(self, top_items: List[Item], provider_items: List[Item], github_items: List[Item], all_items: List[Item]) -> List[Dict]:
        """XæŠ•ç¨¿ç´ æ¡ˆã‚’Slack Blocksã¨ã—ã¦ç”Ÿæˆï¼ˆå„æŠ•ç¨¿ã‚’å€‹åˆ¥ãƒ–ãƒ­ãƒƒã‚¯ã«ï¼‰"""
        blocks = []
        seen_urls = set()
        today = datetime.now().strftime('%Y/%m/%d')
        draft_count = 0

        # ã€å¿…è¦‹ã®æ›´æ–°ã€‘ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        must_include_items = [i for i in all_items if i.metadata.get("must_include")]
        must_include_config = self.config.get("rss", {}).get("must_include_feeds", [])

        if must_include_config:
            blocks.append({"type": "divider"})
            blocks.append({
                "type": "header",
                "text": {"type": "plain_text", "text": "â­ å¿…è¦‹ã®æ›´æ–°"}
            })

            if must_include_items:
                # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
                from collections import defaultdict
                grouped = defaultdict(list)
                for item in must_include_items:
                    feed_name = item.metadata.get("feed_name", "Unknown")
                    grouped[feed_name].append(item)

                # å„ãƒ•ã‚£ãƒ¼ãƒ‰ã®æ›´æ–°ã‚’è¡¨ç¤º
                for feed_name, items in grouped.items():
                    for item in items:
                        seen_urls.add(item.url)
                        draft_count += 1

                        post = self._create_single_post(
                            title=item.title,
                            url=item.url,
                            source_type="å¿…è¦‹ã®æ›´æ–°",
                            source_name=feed_name,
                            date=today,
                            item=item
                        )

                        blocks.append({
                            "type": "section",
                            "text": {"type": "mrkdwn", "text": f"*ã€æŠ•ç¨¿æ¡ˆ {draft_count}ã€‘{feed_name}*"}
                        })
                        blocks.append({
                            "type": "section",
                            "text": {"type": "mrkdwn", "text": f"```{post}```"}
                        })
                        blocks.append({"type": "divider"})
            else:
                # æ›´æ–°ãŒãªã„å ´åˆ
                feed_names = [f["name"] for f in must_include_config]
                blocks.append({
                    "type": "section",
                    "text": {"type": "mrkdwn", "text": f"ğŸ“­ æœ¬æ—¥ã®æ›´æ–°ãªã—\nå¯¾è±¡: {', '.join(feed_names)}"}
                })
                blocks.append({"type": "divider"})

        # RSSï¼ˆå…¬å¼ç™ºè¡¨ï¼‰ã‚’5ä»¶ã«å‰Šæ¸›ï¼ˆ7â†’5ï¼‰
        for item in provider_items[:5]:
            if item.url in seen_urls:
                continue
            seen_urls.add(item.url)
            draft_count += 1

            feed_name = item.metadata.get("feed_name", "")
            post = self._create_single_post(
                title=item.title,
                url=item.url,
                source_type="å…¬å¼ç™ºè¡¨",
                source_name=feed_name,
                date=today,
                item=item
            )

            # å„æŠ•ç¨¿ã‚’å€‹åˆ¥ã®sectionãƒ–ãƒ­ãƒƒã‚¯ã«ï¼ˆ3000æ–‡å­—åˆ¶é™å›é¿ï¼‰
            blocks.append({
                "type": "section",
                "text": {"type": "mrkdwn", "text": f"```ã€æŠ•ç¨¿æ¡ˆ {draft_count}ã€‘\n{post}```"}
            })

            # åŒºåˆ‡ã‚Šç·šã‚’è¿½åŠ ï¼ˆæœ€å¾Œä»¥å¤–ï¼‰
            if draft_count < 7:
                blocks.append({"type": "divider"})

        # Xç”±æ¥ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ç¢ºå®Ÿã«å«ã‚ã‚‹ï¼ˆæ–°è¦è¿½åŠ ï¼‰
        # é‡è¦: top_items ã§ã¯ãªã all_items ã‹ã‚‰æŠ½å‡ºï¼ˆtop_items ã¯3ä»¶ã—ã‹ãªã„ãŸã‚ï¼‰
        x_items = [i for i in all_items if i.source in ["x_account", "x_search"]]
        for item in x_items[:2]:  # Xç”±æ¥ã‚’æœ€å¤§2ä»¶è¿½åŠ 
            if item.url in seen_urls:
                continue
            seen_urls.add(item.url)
            draft_count += 1

            source_name = item.metadata.get("username", "") or item.metadata.get("keyword", "")
            post = self._create_single_post(
                title=item.title,
                url=item.url,
                source_type="Xæ³¨ç›®æŠ•ç¨¿",
                source_name=source_name,
                date=today,
                item=item
            )

            blocks.append({
                "type": "section",
                "text": {"type": "mrkdwn", "text": f"```ã€æŠ•ç¨¿æ¡ˆ {draft_count}ã€‘\n{post}```"}
            })

            # åŒºåˆ‡ã‚Šç·šã‚’è¿½åŠ ï¼ˆæœ€å¾Œä»¥å¤–ï¼‰
            if draft_count < 7:
                blocks.append({"type": "divider"})

        return blocks

    def _create_single_post(self, title: str, url: str, source_type: str, source_name: str, date: str, item: Item) -> str:
        """å€‹åˆ¥ã®XæŠ•ç¨¿ã‚’ç”Ÿæˆ"""
        # Twitterã®URLã‚’x.comã«å¤‰æ›
        if "twitter.com" in url:
            url = url.replace("twitter.com", "x.com")

        # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’å–å¾—
        category = item.metadata.get("category", "UNKNOWN")

        # Claude API ã§ã‚µãƒãƒ©ã‚¤ã‚ºç”Ÿæˆ
        summary = self._generate_summary_with_claude(title, url, source_type, category)

        return summary

    def _generate_summary_with_claude(self, title: str, url: str, source_type: str, category: str = "UNKNOWN") -> str:
        """Claude API ã§é«˜å“è³ªãªXæŠ•ç¨¿ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ç”Ÿæˆ"""
        try:
            import anthropic

            api_key = os.environ.get("ANTHROPIC_API_KEY")
            if not api_key:
                print("âš ï¸  ANTHROPIC_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç°¡æ˜“è¦ç´„ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
                return self._generate_simple_summary(title, source_type, url)

            client = anthropic.Anthropic(api_key=api_key)

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæƒ…å ±ã‚’å–å¾—
            target = self.config.get("target_audience", {})
            target_name = target.get("name", "AIã«é–¢å¿ƒã®ã‚ã‚‹ãƒ“ã‚¸ãƒã‚¹ãƒ‘ãƒ¼ã‚½ãƒ³ã‚„ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢")

            # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®ãƒ•ã‚©ãƒ¼ã‚«ã‚¹
            category_focus = {
                "PRACTICAL": "å®Ÿè£…æ–¹æ³•ã€å…·ä½“çš„ãªæ©Ÿèƒ½ã€ä½¿ã„æ–¹ã€çµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³ã€å®Ÿè·µçš„ãªTipsã‚’é‡è¦–ã—ã¦ãã ã•ã„ã€‚",
                "TECHNICAL": "æŠ€è¡“çš„ãªä»•çµ„ã¿ã€æ¯”è¼ƒåˆ†æã€è©³ç´°ãªè§£èª¬ã‚’é‡è¦–ã—ã¦ãã ã•ã„ã€‚",
                "GENERAL": "æ–°æ©Ÿèƒ½ã®æ¦‚è¦ã€åˆ©ç”¨é–‹å§‹æ™‚æœŸã€å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’é‡è¦–ã—ã¦ãã ã•ã„ã€‚"
            }.get(category, "")

            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå…¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆå…±é€šï¼‰
            system_prompt = f"""ã‚ãªãŸã¯AIæ¥­ç•Œã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è¿½ã†Xï¼ˆTwitterï¼‰ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®æŠ•ç¨¿ä½œæˆè€…ã§ã™ã€‚
èª­è€…ã¯{target_name}ã§ã™ã€‚

ã€è¨˜äº‹ã‚«ãƒ†ã‚´ãƒªã€‘{category}
{category_focus}

ã€é‡è¦ãªåŸå‰‡ã€‘
- å…·ä½“çš„ã§å®Ÿç”¨çš„ãªæƒ…å ±ã‚’æä¾›ã™ã‚‹
- èª­è€…ãŒã€Œè‡ªåˆ†ã‚‚ä½¿ã£ã¦ã¿ãŸã„ã€ã¨æ€ãˆã‚‹å†…å®¹ã«ã™ã‚‹
- æŠ½è±¡çš„ãªè¡¨ç¾ï¼ˆã€Œé©æ–°çš„ã€ã€Œç”»æœŸçš„ã€ï¼‰ã¯é¿ã‘ã€ä½•ãŒã§ãã‚‹ã‹ã‚’æ˜ç¤ºã™ã‚‹
- çµµæ–‡å­—ã¯æœ€å°é™ã«æŠ‘ãˆã‚‹
- ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç•ªå·ã¯ã€Œ1.ã€ã€Œ2.ã€ã€Œ3.ã€ã®å½¢å¼ã§æ§‹é€ åŒ–ã™ã‚‹"""

            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            user_prompt = f"""ä»¥ä¸‹ã®AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«ã¤ã„ã¦ã€XæŠ•ç¨¿ã‚¹ãƒ¬ãƒƒãƒ‰ã®ç´ æ¡ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã€‘
{title}

ã€ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã€‘
{source_type}

ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘

ã€é€Ÿå ±ã€‘ã¾ãŸã¯ã€æ³¨ç›®ã€‘ä¼æ¥­åã€ã€Œè£½å“/æ©Ÿèƒ½åã€ã‚’ç™ºè¡¨/ãƒªãƒªãƒ¼ã‚¹

[2-3æ–‡ã§æ ¸å¿ƒã‚’è¦ç´„ã€‚ä½•ãŒæ–°ã—ã„ã®ã‹ã€ãªãœé‡è¦ãªã®ã‹ã‚’æ˜ç¢ºã«]

{url}

1. [ã‚»ã‚¯ã‚·ãƒ§ãƒ³å]

ãƒ»å…·ä½“çš„ãªæ©Ÿèƒ½ã‚„ç‰¹å¾´1
ãƒ»å…·ä½“çš„ãªæ©Ÿèƒ½ã‚„ç‰¹å¾´2
ãƒ»å…·ä½“çš„ãªæ©Ÿèƒ½ã‚„ç‰¹å¾´3

[ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è£œè¶³èª¬æ˜ã‚’ä¸€æ–‡ã§]

2. [ã‚»ã‚¯ã‚·ãƒ§ãƒ³å]

ãƒ»å…·ä½“çš„ãªæ©Ÿèƒ½ã‚„ç‰¹å¾´4
ãƒ»å…·ä½“çš„ãªæ©Ÿèƒ½ã‚„ç‰¹å¾´5

[ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è£œè¶³èª¬æ˜ã‚’ä¸€æ–‡ã§]

3. [åˆ©ç”¨æ–¹æ³•ãƒ»å¯¾è±¡è€…]

ãƒ»å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼: [å…·ä½“çš„ã«]
ãƒ»æä¾›é–‹å§‹: [ã„ã¤ã‹ã‚‰]
ãƒ»ä»Šå¾Œã®äºˆå®š: [ã‚ã‚Œã°]

ã€é‡è¦ãªåˆ¶ç´„ã€‘
- å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ç®‡æ¡æ›¸ãã¯3-5é …ç›®
- ç®‡æ¡æ›¸ãã«ã¯ã€Œãƒ»ã€ï¼ˆä¸­é»’ï¼‰ã®ã¿ä½¿ç”¨
- â– ã€â–¸ã€ğŸ’¡ã€ğŸ”— ãªã©ã®è£…é£¾è¨˜å·ã¯ä¸è¦
- URLã¯å†’é ­ã®è¦ç´„ã®ç›´å¾Œã«é…ç½®
- ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç•ªå·ã¯ã€Œ1.ã€ã€Œ2.ã€ã€Œ3.ã€ã®å½¢å¼ã§æ§‹é€ åŒ–
- å…¨ä½“ã§600-800æ–‡å­—ç¨‹åº¦ï¼ˆXã‚¹ãƒ¬ãƒƒãƒ‰ã¨ã—ã¦é©åˆ‡ãªé•·ã•ï¼‰
- ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æ¨æ¸¬ã—ã¦å…·ä½“çš„ã«è¨˜è¿°ã—ã¦ãã ã•ã„"""

            message = client.messages.create(
                model="claude-sonnet-4-5-20250929",
                max_tokens=1500,  # 800æ–‡å­—è¦æ±‚ãªã®ã§ä½™è£•ã‚’æŒãŸã›ã‚‹
                system=system_prompt,  # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¿½åŠ 
                messages=[{
                    "role": "user",
                    "content": user_prompt
                }]
            )

            return message.content[0].text

        except Exception as e:
            print(f"âš ï¸ Claude API ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç°¡æ˜“è¦ç´„
            return self._generate_simple_summary(title, source_type, url)

    def _generate_simple_summary(self, title: str, source_type: str, url: str) -> str:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ç°¡æ˜“è¦ç´„"""
        if source_type == "å…¬å¼ç™ºè¡¨":
            emoji = "ğŸš€"
            comment = "é‡è¦ãªå…¬å¼ã‚¢ãƒŠã‚¦ãƒ³ã‚¹ã§ã™"
        elif source_type == "GitHub Release":
            emoji = "ğŸ“¦"
            comment = "æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã¾ã—ãŸ"
        else:
            emoji = "ğŸ’¡"
            comment = "æ³¨ç›®ã®è©±é¡Œã§ã™"

        # ç°¡æ½”ãªå½¢å¼
        lines = [
            f"ã€{emoji} {title[:60]}{'...' if len(title) > 60 else ''}ã€‘",
            "",
            f"ğŸ’¡ {comment}ã€‚è©³ç´°ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã—ã‚‡ã†ã€‚",
            "",
            f"ğŸ”— {url}"
        ]

        return "\n".join(lines)


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("AIãƒ‡ã‚¤ãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆ")
    print("=" * 60)

    # è¨­å®šèª­ã¿è¾¼ã¿
    with open("config.yaml", 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
    x_bearer_token = os.environ.get("X_BEARER_TOKEN")
    slack_webhook_url = os.environ.get("SLACK_WEBHOOK_URL")

    if not x_bearer_token:
        raise ValueError("ç’°å¢ƒå¤‰æ•° X_BEARER_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    if not slack_webhook_url:
        raise ValueError("ç’°å¢ƒå¤‰æ•° SLACK_WEBHOOK_URL ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

    # åˆæœŸåŒ–
    state = StateManager()
    x_client = XAPIClient(x_bearer_token)
    collector = DataCollector(config, state, x_client)

    try:
        # ãƒ‡ãƒ¼ã‚¿åé›†
        collector.collect_all()

        # Slackãƒ¬ãƒãƒ¼ãƒˆé€ä¿¡
        reporter = SlackReporter(slack_webhook_url, config, collector.items, collector.stats)
        reporter.send()

        # çŠ¶æ…‹ä¿å­˜ï¼ˆæœ€å¾Œã«å®Ÿè¡Œï¼‰
        state.save()
        print("ğŸ’¾ çŠ¶æ…‹ã‚’ä¿å­˜ã—ã¾ã—ãŸ")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        raise

    print("=" * 60)
    print("âœ… å®Œäº†")
    print("=" * 60)


if __name__ == "__main__":
    main()
