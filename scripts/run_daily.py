#!/usr/bin/env python3
"""
AIãƒ‡ã‚¤ãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
X API Basic ($200/æœˆ) ã«åã¾ã‚‹ã‚ˆã†è¨­è¨ˆ
"""

import os
import json
import yaml
import requests
import feedparser
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from collections import defaultdict
import time
from content_classifier import ContentClassifier, ClassificationResult
from content_validator import ContentValidator


@dataclass
class Item:
    """åé›†ã—ãŸæƒ…å ±ã‚¢ã‚¤ãƒ†ãƒ """
    source: str  # "x_account", "x_search", "rss", "github"
    title: str
    url: str
    published_at: str
    score: int = 0
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class StateManager:
    """çŠ¶æ…‹ç®¡ç†ï¼ˆstate.jsonï¼‰"""

    def __init__(self, state_path: str = "data/state.json"):
        self.state_path = state_path
        self.state = self._load()

    def _load(self) -> Dict:
        """state.json ã‚’èª­ã¿è¾¼ã¿"""
        if os.path.exists(self.state_path):
            with open(self.state_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {
            "x_accounts": {},
            "x_keywords": {},
            "rss": {},
            "github": {},
            "meta": {"last_run_at": None, "version": "1.0.0"}
        }

    def save(self):
        """state.json ã‚’ä¿å­˜"""
        self.state["meta"]["last_run_at"] = datetime.now(timezone.utc).isoformat()
        with open(self.state_path, 'w', encoding='utf-8') as f:
            json.dump(self.state, f, indent=2, ensure_ascii=False)

    def get_x_account_since_id(self, username: str) -> Optional[str]:
        """Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆã® since_id ã‚’å–å¾—"""
        return self.state["x_accounts"].get(username, {}).get("since_id")

    def set_x_account_since_id(self, username: str, user_id: str, since_id: str):
        """Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆã® since_id ã‚’æ›´æ–°"""
        if username not in self.state["x_accounts"]:
            self.state["x_accounts"][username] = {}
        self.state["x_accounts"][username]["user_id"] = user_id
        self.state["x_accounts"][username]["since_id"] = since_id

    def get_x_keyword_since_id(self, keyword: str) -> Optional[str]:
        """Xã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã® since_id ã‚’å–å¾—"""
        return self.state["x_keywords"].get(keyword, {}).get("since_id")

    def set_x_keyword_since_id(self, keyword: str, since_id: str):
        """Xã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã® since_id ã‚’æ›´æ–°"""
        if keyword not in self.state["x_keywords"]:
            self.state["x_keywords"][keyword] = {}
        self.state["x_keywords"][keyword]["since_id"] = since_id

    def get_rss_last_published(self, feed_url: str) -> Optional[str]:
        """RSSã®æœ€çµ‚å–å¾—æ—¥æ™‚ã‚’å–å¾—"""
        return self.state["rss"].get(feed_url)

    def set_rss_last_published(self, feed_url: str, published_at: str):
        """RSSã®æœ€çµ‚å–å¾—æ—¥æ™‚ã‚’æ›´æ–°"""
        self.state["rss"][feed_url] = published_at

    def get_github_last_tag(self, repo: str) -> Optional[str]:
        """GitHubãƒªãƒã‚¸ãƒˆãƒªã®æœ€çµ‚tagã‚’å–å¾—"""
        return self.state["github"].get(repo, {}).get("tag")

    def set_github_last_tag(self, repo: str, tag: str):
        """GitHubãƒªãƒã‚¸ãƒˆãƒªã®æœ€çµ‚tagã‚’æ›´æ–°"""
        if repo not in self.state["github"]:
            self.state["github"][repo] = {}
        self.state["github"][repo]["tag"] = tag

    def is_recently_posted(self, url: str, hours: int = 24) -> bool:
        """éå»Næ™‚é–“ä»¥å†…ã«æŠ•ç¨¿æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯"""
        posted_urls = self.state.get("recently_posted_urls", {})
        cutoff = (datetime.now(timezone.utc) - timedelta(hours=hours)).isoformat()

        if url in posted_urls:
            posted_at = posted_urls[url]
            return posted_at > cutoff
        return False

    def mark_as_posted(self, url: str):
        """æŠ•ç¨¿æ¸ˆã¿ã«ãƒãƒ¼ã‚¯"""
        if "recently_posted_urls" not in self.state:
            self.state["recently_posted_urls"] = {}
        self.state["recently_posted_urls"][url] = datetime.now(timezone.utc).isoformat()

    def cleanup_old_posted_urls(self, hours: int = 24):
        """å¤ã„æŠ•ç¨¿å±¥æ­´ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if "recently_posted_urls" not in self.state:
            return

        cutoff = (datetime.now(timezone.utc) - timedelta(hours=hours)).isoformat()
        posted_urls = self.state["recently_posted_urls"]
        self.state["recently_posted_urls"] = {
            url: posted_at
            for url, posted_at in posted_urls.items()
            if posted_at > cutoff
        }


class XAPIClient:
    """X (Twitter) API v2 ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""

    def __init__(self, bearer_token: str, oauth_credentials: Optional[Dict] = None):
        # èª­ã¿å–ã‚Šç”¨ï¼ˆæ—¢å­˜ï¼‰
        self.bearer_token = bearer_token
        self.base_url = "https://api.twitter.com/2"
        self.headers = {"Authorization": f"Bearer {bearer_token}"}

        # æ›¸ãè¾¼ã¿ç”¨ï¼ˆæ–°è¦ï¼‰
        if oauth_credentials:
            from requests_oauthlib import OAuth1Session
            self.oauth = OAuth1Session(
                client_key=oauth_credentials['api_key'],
                client_secret=oauth_credentials['api_secret'],
                resource_owner_key=oauth_credentials['access_token'],
                resource_owner_secret=oauth_credentials['access_token_secret']
            )
        else:
            self.oauth = None

    def get_user_id(self, username: str) -> Optional[str]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’å–å¾—"""
        url = f"{self.base_url}/users/by/username/{username}"
        response = requests.get(url, headers=self.headers)
        if response.status_code == 200:
            data = response.json()
            return data.get("data", {}).get("id")
        return None

    def get_user_tweets(self, user_id: str, since_id: Optional[str] = None, max_results: int = 10) -> List[Dict]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å–å¾—ï¼ˆæ–°ç€ã®ã¿ï¼‰"""
        url = f"{self.base_url}/users/{user_id}/tweets"
        params = {
            "max_results": min(max_results, 100),
            "tweet.fields": "created_at,public_metrics",
            "expansions": "author_id"
        }
        if since_id:
            params["since_id"] = since_id
        else:
            # åˆå›å®Ÿè¡Œæ™‚ã¯ç›´è¿‘24æ™‚é–“
            start_time = (datetime.now(timezone.utc) - timedelta(hours=24)).isoformat()
            params["start_time"] = start_time

        response = requests.get(url, headers=self.headers, params=params)
        if response.status_code == 200:
            data = response.json()
            return data.get("data", [])
        return []

    def search_tweets(self, query: str, since_id: Optional[str] = None, max_results: int = 10) -> List[Dict]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ„ã‚¤ãƒ¼ãƒˆæ¤œç´¢ï¼ˆæ–°ç€ã®ã¿ï¼‰"""
        url = f"{self.base_url}/tweets/search/recent"
        params = {
            "query": query,
            "max_results": min(max_results, 100),
            "tweet.fields": "created_at,public_metrics",
            "expansions": "author_id"
        }
        if since_id:
            params["since_id"] = since_id
        else:
            # åˆå›å®Ÿè¡Œæ™‚ã¯ç›´è¿‘24æ™‚é–“
            start_time = (datetime.now(timezone.utc) - timedelta(hours=24)).isoformat()
            params["start_time"] = start_time

        response = requests.get(url, headers=self.headers, params=params)
        if response.status_code == 200:
            data = response.json()
            return data.get("data", [])
        return []

    def post_tweet(self, text: str) -> Dict:
        """ãƒ„ã‚¤ãƒ¼ãƒˆæŠ•ç¨¿"""
        if not self.oauth:
            raise ValueError("OAuth credentials not configured")

        url = f"{self.base_url}/tweets"
        payload = {"text": text}

        response = self.oauth.post(url, json=payload)

        if response.status_code == 201:
            return response.json()
        else:
            raise Exception(
                f"Failed to post tweet: {response.status_code} {response.text}"
            )


class DataCollector:
    """ãƒ‡ãƒ¼ã‚¿åé›†ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼"""

    # å„ªå…ˆåº¦ã®é«˜ã„RSSãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆé‡è¦ãƒ™ãƒ³ãƒ€ãƒ¼ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¼ã‚‰ã•ãªã„ãŸã‚ï¼‰
    PRIORITY_FEEDS = {
        # å…¬å¼ãƒ–ãƒ­ã‚°
        "https://www.anthropic.com/news/rss.xml": 1000,  # Anthropicæœ€å„ªå…ˆ
        "https://openai.com/blog/rss.xml": 1000,          # OpenAIæœ€å„ªå…ˆ
        "https://github.blog/feed/": 800,                 # GitHub Blog
        "https://code.visualstudio.com/updates/feed.xml": 800,  # VSCode Updates

        # GitHub Releases Atom Feedï¼ˆå¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã¯must_include_feedsã§ç®¡ç†ï¼‰
        "https://github.com/anthropics/claude-code/releases.atom": 800,  # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã«ç§»è¡Œ
        "https://github.blog/changelog/label/copilot/feed/": 800,        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã«ç§»è¡Œ
        "https://github.com/langchain-ai/langchain/releases.atom": 800,   # LangChain
        "https://github.com/openai/openai-python/releases.atom": 800,     # OpenAI Python SDK
        "https://github.com/run-llama/llama_index/releases.atom": 600,    # LlamaIndex
        "https://github.com/huggingface/transformers/releases.atom": 600, # Transformers
    }

    def __init__(self, config: Dict, state: StateManager, x_client: XAPIClient):
        self.config = config
        self.state = state
        self.x_client = x_client
        self.items: List[Item] = []
        self.stats = {
            "x_accounts_fetched": 0,
            "x_search_fetched": 0,
            "x_total_fetched": 0,
            "x_limit_reached": False,
            "rss_fetched": 0,
            "github_fetched": 0,
            "duplicates_removed": 0
        }
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†é¡å™¨ã®åˆæœŸåŒ–
        if self.config.get("content_filtering", {}).get("enabled"):
            self.classifier = ContentClassifier(config)
        else:
            self.classifier = None

    def collect_all(self):
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†"""
        print("ğŸ” ãƒ‡ãƒ¼ã‚¿åé›†ã‚’é–‹å§‹...")

        # X (Twitter)
        self._collect_x_accounts()
        self._collect_x_search()

        # RSSï¼ˆGitHub Releases Atom Feedã‚’å«ã‚€ï¼‰
        self._collect_rss()

        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆå½“æ—¥ã®æ›´æ–°ãŒã‚ã‚Œã°å¿…ãšå«ã‚ã‚‹ï¼‰
        must_include_items = self._collect_must_include_feeds()
        self.items.extend(must_include_items)
        self.stats["must_include_fetched"] = len(must_include_items)

        # é‡è¤‡æ’é™¤
        self._deduplicate()

        print(f"âœ… åé›†å®Œäº†: {len(self.items)} ä»¶")

    def _collect_x_accounts(self):
        """Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‹ã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆåé›†"""
        accounts = self.config["x"]["accounts"]
        limit = self.config["x"]["limits"]["accounts"]
        fetched = 0

        print(f"ğŸ“± Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆç›£è¦–: {len(accounts)} ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ")

        for username in accounts:
            if fetched >= limit:
                print(f"âš ï¸  ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç›£è¦–ã®ä¸Šé™ {limit} ä»¶ã«åˆ°é”")
                self.stats["x_limit_reached"] = True
                break

            user_id = self.x_client.get_user_id(username)
            if not user_id:
                continue

            since_id = self.state.get_x_account_since_id(username)
            tweets = self.x_client.get_user_tweets(user_id, since_id, max_results=10)

            if not tweets:
                continue

            # æœ€æ–°ã®tweet_idã‚’ä¿å­˜
            max_id = max(int(t["id"]) for t in tweets)
            self.state.set_x_account_since_id(username, user_id, str(max_id))

            for tweet in tweets:
                if fetched >= limit:
                    break

                # è¨€èªãƒ»åœ°åŸŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                tweet_text = tweet["text"]
                tweet_url = f"https://twitter.com/{username}/status/{tweet['id']}"

                if self.classifier:
                    # ç·åˆçš„ãªåˆ†é¡ï¼ˆè¨€èªãƒ»åœ°åŸŸãƒã‚§ãƒƒã‚¯ã‚’å«ã‚€ï¼‰
                    classification = self.classifier.classify(tweet_text, "", tweet_url)
                    category = classification.category

                    # éè‹±èªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¾ãŸã¯æ—¥æœ¬ç”±æ¥ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯é™¤å¤–
                    if category in ["NON_ENGLISH", "JAPAN_ORIGIN"]:
                        print(f"  â­ï¸  é™¤å¤–ï¼ˆ{category}ï¼‰: {tweet_text[:50]}...")
                        continue
                else:
                    category = "UNKNOWN"

                # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã¨ã‚¹ã‚³ã‚¢èª¿æ•´
                initial_score = self._calculate_engagement_score(tweet)
                if self.classifier:
                    final_score = self.classifier.calculate_final_score(
                        initial_score, category, "x_account"
                    )
                else:
                    final_score = initial_score

                item = Item(
                    source="x_account",
                    title=tweet_text[:100],
                    url=tweet_url,
                    published_at=tweet["created_at"],
                    score=final_score,
                    metadata={
                        "username": username,
                        "tweet": tweet,
                        "category": category
                    }
                )
                self.items.append(item)
                fetched += 1

        self.stats["x_accounts_fetched"] = fetched
        self.stats["x_total_fetched"] += fetched

    def _collect_x_search(self):
        """Xã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢"""
        keywords = self.config["x"]["keywords"]
        limit = self.config["x"]["limits"]["search"]
        fetched = 0

        print(f"ğŸ” Xã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢: {len(keywords)} ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")

        for keyword in keywords:
            if fetched >= limit:
                print(f"âš ï¸  æ¤œç´¢ã®ä¸Šé™ {limit} ä»¶ã«åˆ°é”")
                self.stats["x_limit_reached"] = True
                break

            since_id = self.state.get_x_keyword_since_id(keyword)
            tweets = self.x_client.search_tweets(keyword, since_id, max_results=10)

            if not tweets:
                continue

            # æœ€æ–°ã®tweet_idã‚’ä¿å­˜
            max_id = max(int(t["id"]) for t in tweets)
            self.state.set_x_keyword_since_id(keyword, str(max_id))

            for tweet in tweets:
                if fetched >= limit:
                    break

                # è¨€èªãƒ»åœ°åŸŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                tweet_text = tweet["text"]
                tweet_url = f"https://twitter.com/i/web/status/{tweet['id']}"

                if self.classifier:
                    # ç·åˆçš„ãªåˆ†é¡ï¼ˆè¨€èªãƒ»åœ°åŸŸãƒã‚§ãƒƒã‚¯ã‚’å«ã‚€ï¼‰
                    classification = self.classifier.classify(tweet_text, "", tweet_url)
                    category = classification.category

                    # éè‹±èªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¾ãŸã¯æ—¥æœ¬ç”±æ¥ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯é™¤å¤–
                    if category in ["NON_ENGLISH", "JAPAN_ORIGIN"]:
                        print(f"  â­ï¸  é™¤å¤–ï¼ˆ{category}ï¼‰: {tweet_text[:50]}...")
                        continue
                else:
                    category = "UNKNOWN"

                # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã¨ã‚¹ã‚³ã‚¢èª¿æ•´
                initial_score = self._calculate_engagement_score(tweet)
                if self.classifier:
                    final_score = self.classifier.calculate_final_score(
                        initial_score, category, "x_search"
                    )
                else:
                    final_score = initial_score

                item = Item(
                    source="x_search",
                    title=tweet_text[:100],
                    url=tweet_url,
                    published_at=tweet["created_at"],
                    score=final_score,
                    metadata={
                        "keyword": keyword,
                        "tweet": tweet,
                        "category": category
                    }
                )
                self.items.append(item)
                fetched += 1

        self.stats["x_search_fetched"] = fetched
        self.stats["x_total_fetched"] += fetched

    def _collect_rss(self):
        """RSSåé›†"""
        feeds = self.config["rss"]["feeds"]
        fetched = 0

        print(f"ğŸ“° RSSåé›†: {len(feeds)} ãƒ•ã‚£ãƒ¼ãƒ‰")

        for feed_config in feeds:
            feed_url = feed_config["url"]
            feed_name = feed_config["name"]

            feed = feedparser.parse(feed_url)
            last_published = self.state.get_rss_last_published(feed_url)

            for entry in feed.entries:
                published = entry.get("published_parsed") or entry.get("updated_parsed")
                if not published:
                    continue

                published_dt = datetime(*published[:6], tzinfo=timezone.utc)
                published_iso = published_dt.isoformat()

                # æ–°ç€ã®ã¿
                if last_published and published_iso <= last_published:
                    continue

                # è¨€èªãƒ»åœ°åŸŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if self.classifier:
                    description = entry.get("summary", "")
                    url = entry.link

                    # ç·åˆçš„ãªåˆ†é¡ï¼ˆè¨€èªãƒ»åœ°åŸŸãƒã‚§ãƒƒã‚¯ã‚’å«ã‚€ï¼‰
                    classification = self.classifier.classify(entry.title, description, url)
                    category = classification.category

                    # éè‹±èªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¾ãŸã¯æ—¥æœ¬ç”±æ¥ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯é™¤å¤–
                    if category in ["NON_ENGLISH", "JAPAN_ORIGIN"]:
                        print(f"  â­ï¸  é™¤å¤–ï¼ˆ{category}ï¼‰: {entry.title[:50]}...")
                        continue
                else:
                    category = "UNKNOWN"

                # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°: åŸºæœ¬ã‚¹ã‚³ã‚¢ + å„ªå…ˆåº¦ãƒœãƒ¼ãƒŠã‚¹
                base_score = self.config["slack"]["scoring"]["rss_bonus"]  # 500
                priority_bonus = self.PRIORITY_FEEDS.get(feed_url, 0)
                initial_score = base_score + priority_bonus

                # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã¨ã‚¹ã‚³ã‚¢èª¿æ•´
                if self.classifier:
                    final_score = self.classifier.calculate_final_score(
                        initial_score, category, "rss", is_official=True
                    )
                else:
                    final_score = initial_score

                item = Item(
                    source="rss",
                    title=entry.title,
                    url=entry.link,
                    published_at=published_iso,
                    score=final_score,  # é‡è¦ãƒ•ã‚£ãƒ¼ãƒ‰ + ã‚«ãƒ†ã‚´ãƒªãƒœãƒ¼ãƒŠã‚¹
                    metadata={
                        "feed_name": feed_name,
                        "feed_url": feed_url,
                        "category": category
                    }
                )
                self.items.append(item)
                fetched += 1

            # æœ€æ–°ã® published_at ã‚’ä¿å­˜
            if feed.entries:
                latest = max(feed.entries, key=lambda e: e.get("published_parsed") or e.get("updated_parsed"))
                published = latest.get("published_parsed") or latest.get("updated_parsed")
                if published:
                    published_dt = datetime(*published[:6], tzinfo=timezone.utc)
                    self.state.set_rss_last_published(feed_url, published_dt.isoformat())

        self.stats["rss_fetched"] = fetched

    def _collect_must_include_feeds(self) -> List[Item]:
        """å¿…ãšå«ã‚ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰å½“æ—¥ã®æ›´æ–°ã‚’å–å¾—"""
        must_include_config = self.config.get("rss", {}).get("must_include_feeds", [])
        must_include_items = []

        if not must_include_config:
            return must_include_items

        print(f"â­ å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰åé›†: {len(must_include_config)} ãƒ•ã‚£ãƒ¼ãƒ‰")

        # å½“æ—¥ã®æ—¥ä»˜ï¼ˆUTCã§00:00:00ï¼‰
        today = datetime.now(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)

        for feed_config in must_include_config:
            feed_url = feed_config["url"]
            feed_name = feed_config["name"]
            max_items = feed_config.get("max_items", 3)

            feed = feedparser.parse(feed_url)
            count = 0

            for entry in feed.entries:
                if count >= max_items:
                    break

                published = entry.get("published_parsed") or entry.get("updated_parsed")
                if not published:
                    continue

                published_dt = datetime(*published[:6], tzinfo=timezone.utc)
                published_iso = published_dt.isoformat()

                # å½“æ—¥ã®æ›´æ–°ã®ã¿ï¼ˆ00:00:00ä»¥é™ï¼‰
                if published_dt < today:
                    continue

                # è¨€èªãƒ»åœ°åŸŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if self.classifier:
                    description = entry.get("summary", "")
                    url = entry.link

                    # ç·åˆçš„ãªåˆ†é¡ï¼ˆè¨€èªãƒ»åœ°åŸŸãƒã‚§ãƒƒã‚¯ã‚’å«ã‚€ï¼‰
                    classification = self.classifier.classify(entry.title, description, url)
                    category = classification.category

                    # éè‹±èªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¾ãŸã¯æ—¥æœ¬ç”±æ¥ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯é™¤å¤–
                    # ãŸã ã—ã€å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ãªã®ã§è­¦å‘Šã®ã¿å‡ºåŠ›
                    if category in ["NON_ENGLISH", "JAPAN_ORIGIN"]:
                        print(f"  âš ï¸  å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã ãŒéè‹±èª/æ—¥æœ¬ç”±æ¥ï¼ˆ{category}ï¼‰: {entry.title[:50]}...")
                        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ãªã®ã§é™¤å¤–ã›ãšã«å«ã‚ã‚‹ï¼ˆã‚¹ã‚³ã‚¢ã¯ä½ãã™ã‚‹ï¼‰
                        category = category  # ãã®ã¾ã¾ä½¿ã†
                    elif category == "PRACTICAL":
                        category = "PRACTICAL"  # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã®PRACTICALã¯ãã®ã¾ã¾
                else:
                    category = "PRACTICAL"  # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§PRACTICAL

                item = Item(
                    source="must_include",
                    title=entry.title,
                    url=entry.link,
                    published_at=published_iso,
                    score=9999,  # æœ€é«˜ã‚¹ã‚³ã‚¢ï¼ˆå¿…ãšå«ã¾ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ï¼‰
                    metadata={
                        "feed_name": feed_name,
                        "feed_url": feed_url,
                        "category": category,
                        "must_include": True
                    }
                )
                must_include_items.append(item)
                count += 1
                print(f"  âœ“ {feed_name}: {entry.title[:50]}...")

        return must_include_items

    def _select_diverse_provider_items(self, sorted_items: List[Item], limit: int) -> List[Item]:
        """
        å„ªå…ˆåº¦ã®é«˜ã„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æœ€æ–°è¨˜äº‹ã‚’1ä»¶ãšã¤ãƒãƒ©ãƒ³ã‚¹ã‚ˆãé¸æŠ

        Args:
            sorted_items: ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã®ã‚¢ã‚¤ãƒ†ãƒ ãƒªã‚¹ãƒˆ
            limit: é¸æŠã™ã‚‹æœ€å¤§ä»¶æ•°

        Returns:
            ãƒãƒ©ãƒ³ã‚¹ã‚ˆãé¸æŠã•ã‚ŒãŸRSSã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆ
        """
        from collections import defaultdict

        # RSSã‚¢ã‚¤ãƒ†ãƒ ã®ã¿æŠ½å‡º
        rss_items = [i for i in sorted_items if i.source == "rss"]

        # ãƒ•ã‚£ãƒ¼ãƒ‰åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        feed_groups = defaultdict(list)
        for item in rss_items:
            feed_url = item.metadata.get("feed_url", "")
            if feed_url:
                feed_groups[feed_url].append(item)

        # å„ªå…ˆé †ä½é †ã«ã‚½ãƒ¼ãƒˆï¼ˆPRIORITY_FEEDS ã®å®šç¾©é †ï¼‰
        priority_order = list(self.PRIORITY_FEEDS.keys())
        sorted_feed_urls = sorted(
            feed_groups.keys(),
            key=lambda url: priority_order.index(url) if url in priority_order else 9999
        )

        # ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ­ãƒ“ãƒ³æ–¹å¼ã§é¸æŠï¼ˆå„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰1ä»¶ãšã¤ï¼‰
        selected = []
        round_num = 0

        while len(selected) < limit:
            added_this_round = False

            for feed_url in sorted_feed_urls:
                if len(selected) >= limit:
                    break

                items = feed_groups[feed_url]
                if round_num < len(items):
                    selected.append(items[round_num])
                    added_this_round = True

            # å…¨ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰é¸æŠã—çµ‚ãˆãŸ
            if not added_this_round:
                break

            round_num += 1

        return selected[:limit]

    def _calculate_engagement_score(self, tweet: Dict) -> int:
        """ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢è¨ˆç®—"""
        metrics = tweet.get("public_metrics", {})
        scoring = self.config["slack"]["scoring"]

        score = (
            metrics.get("like_count", 0) * scoring["like_weight"] +
            metrics.get("retweet_count", 0) * scoring["retweet_weight"] +
            metrics.get("reply_count", 0) * scoring["reply_weight"]
        )
        return score

    def _deduplicate(self):
        """é‡è¤‡æ’é™¤ï¼ˆURLåŸºæº–ï¼‰"""
        seen_urls = set()
        unique_items = []

        for item in self.items:
            if item.url not in seen_urls:
                seen_urls.add(item.url)
                unique_items.append(item)
            else:
                self.stats["duplicates_removed"] += 1

        self.items = unique_items


class SlackReporter:
    """Slackãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ»æŠ•ç¨¿"""

    # DataCollectorã®PRIORITY_FEEDSã¨åŒã˜å®šç¾©
    PRIORITY_FEEDS = {
        # å…¬å¼ãƒ–ãƒ­ã‚°
        "https://www.anthropic.com/news/rss.xml": 1000,
        "https://openai.com/blog/rss.xml": 1000,
        "https://github.blog/feed/": 800,
        "https://code.visualstudio.com/updates/feed.xml": 800,

        # GitHub Releases Atom Feedï¼ˆå¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã¯must_include_feedsã§ç®¡ç†ï¼‰
        "https://github.com/anthropics/claude-code/releases.atom": 800,
        "https://github.blog/changelog/label/copilot/feed/": 800,
        "https://github.com/langchain-ai/langchain/releases.atom": 800,
        "https://github.com/openai/openai-python/releases.atom": 800,
        "https://github.com/run-llama/llama_index/releases.atom": 600,
        "https://github.com/huggingface/transformers/releases.atom": 600,
    }

    def __init__(self, webhook_url: str, config: Dict, items: List[Item], stats: Dict):
        self.webhook_url = webhook_url
        self.config = config
        self.items = items
        self.stats = stats
        # æ¤œè¨¼å™¨ã®åˆæœŸåŒ–
        self.validator = ContentValidator(config)

    def _select_diverse_provider_items(self, sorted_items: List[Item], limit: int) -> List[Item]:
        """
        å„ªå…ˆåº¦ã®é«˜ã„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æœ€æ–°è¨˜äº‹ã‚’1ä»¶ãšã¤ãƒãƒ©ãƒ³ã‚¹ã‚ˆãé¸æŠ

        Args:
            sorted_items: ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã®ã‚¢ã‚¤ãƒ†ãƒ ãƒªã‚¹ãƒˆ
            limit: é¸æŠã™ã‚‹æœ€å¤§ä»¶æ•°

        Returns:
            ãƒãƒ©ãƒ³ã‚¹ã‚ˆãé¸æŠã•ã‚ŒãŸRSSã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆ
        """
        from collections import defaultdict

        # RSSã‚¢ã‚¤ãƒ†ãƒ ã®ã¿æŠ½å‡º
        rss_items = [i for i in sorted_items if i.source == "rss"]

        # ãƒ•ã‚£ãƒ¼ãƒ‰åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        feed_groups = defaultdict(list)
        for item in rss_items:
            feed_url = item.metadata.get("feed_url", "")
            if feed_url:
                feed_groups[feed_url].append(item)

        # å„ªå…ˆé †ä½é †ã«ã‚½ãƒ¼ãƒˆï¼ˆPRIORITY_FEEDS ã®å®šç¾©é †ï¼‰
        priority_order = list(self.PRIORITY_FEEDS.keys())
        sorted_feed_urls = sorted(
            feed_groups.keys(),
            key=lambda url: priority_order.index(url) if url in priority_order else 9999
        )

        # ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ­ãƒ“ãƒ³æ–¹å¼ã§é¸æŠï¼ˆå„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰1ä»¶ãšã¤ï¼‰
        selected = []
        round_num = 0

        while len(selected) < limit:
            added_this_round = False

            for feed_url in sorted_feed_urls:
                if len(selected) >= limit:
                    break

                items = feed_groups[feed_url]
                if round_num < len(items):
                    selected.append(items[round_num])
                    added_this_round = True

            # å…¨ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰é¸æŠã—çµ‚ãˆãŸ
            if not added_this_round:
                break

            round_num += 1

        return selected[:limit]

    def send(self):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¦Slackã«æŠ•ç¨¿"""
        print("ğŸ“¤ Slackãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")

        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_items = sorted(self.items, key=lambda x: x.score, reverse=True)

        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†ã‘
        top_items = sorted_items[:self.config["slack"]["limits"]["top"]]
        provider_items = self._select_diverse_provider_items(sorted_items, self.config["slack"]["limits"]["provider_official"])
        github_items = [i for i in sorted_items if i.source == "github"][:self.config["slack"]["limits"]["github_updates"]]

        # Slack Blocksæ§‹ç¯‰
        blocks = []

        # ãƒ˜ãƒƒãƒ€ãƒ¼
        blocks.append({
            "type": "header",
            "text": {"type": "plain_text", "text": f"ğŸ¦ XæŠ•ç¨¿ç´ æ¡ˆ - {datetime.now().strftime('%Y-%m-%d')}"}
        })

        # åˆ†æå¯¾è±¡ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        source_counts = self._count_sources()
        source_summary = self._format_source_summary(source_counts)
        blocks.append({
            "type": "section",
            "text": {"type": "mrkdwn", "text": source_summary}
        })

        # XæŠ•ç¨¿ç´ æ¡ˆã‚’ç”Ÿæˆï¼ˆå€‹åˆ¥ã®ãƒ–ãƒ­ãƒƒã‚¯ã¨ã—ã¦è¿½åŠ ï¼‰
        draft_blocks = self._generate_x_post_draft_blocks(top_items, provider_items, github_items, sorted_items)
        blocks.extend(draft_blocks)

        # é€ä¿¡
        payload = {"blocks": blocks}
        response = requests.post(self.webhook_url, json=payload)

        if response.status_code == 200:
            print("âœ… Slackã«æŠ•ç¨¿ã—ã¾ã—ãŸ")
        else:
            print(f"âŒ SlackæŠ•ç¨¿å¤±æ•—: {response.status_code} {response.text}")
            raise Exception("SlackæŠ•ç¨¿ã«å¤±æ•—ã—ã¾ã—ãŸ")

    def _count_sources(self) -> Dict[str, int]:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã”ã¨ã®ã‚¢ã‚¤ãƒ†ãƒ æ•°ã‚’é›†è¨ˆ"""
        counts = {
            "x_posts": 0,      # XæŠ•ç¨¿ï¼ˆx_account + x_searchï¼‰
            "rss": 0,          # RSS
            "must_include": 0  # å¿…è¦‹ã®æ›´æ–°
        }

        for item in self.items:
            if item.source in ["x_account", "x_search"]:
                counts["x_posts"] += 1
            elif item.source == "rss":
                counts["rss"] += 1
            elif item.source == "must_include":
                counts["must_include"] += 1

        return counts

    def _format_source_summary(self, counts: Dict[str, int]) -> str:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é›†è¨ˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆå…¨ã‚½ãƒ¼ã‚¹ã‚’å¸¸ã«è¡¨ç¤ºï¼‰"""
        parts = []

        # XæŠ•ç¨¿ï¼ˆå¸¸ã«è¡¨ç¤ºï¼‰
        parts.append(f"XæŠ•ç¨¿ {counts['x_posts']}ä»¶")

        # RSSï¼ˆå¸¸ã«è¡¨ç¤ºï¼‰
        parts.append(f"RSS {counts['rss']}ä»¶")

        # å¿…è¦‹ã®æ›´æ–°ï¼ˆå¸¸ã«è¡¨ç¤ºï¼‰
        parts.append(f"å¿…è¦‹ã®æ›´æ–° {counts['must_include']}ä»¶")

        return "ğŸ“Š åˆ†æå¯¾è±¡: " + "ã€".join(parts)

    def _generate_x_post_draft(self, top_items: List[Item], provider_items: List[Item], github_items: List[Item]) -> str:
        """XæŠ•ç¨¿ç´ æ¡ˆã‚’ç”Ÿæˆï¼ˆè¨˜äº‹ã”ã¨ã«å€‹åˆ¥æŠ•ç¨¿ã‚’ä½œæˆï¼‰"""
        drafts = []
        seen_urls = set()  # URLé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨
        today = datetime.now().strftime('%Y/%m/%d')

        # RSSï¼ˆå…¬å¼ç™ºè¡¨ï¼‰ã‚’å„ªå…ˆçš„ã«æŠ•ç¨¿ç´ æ¡ˆä½œæˆï¼ˆAnthropicãªã©ã®é‡è¦ãªå…¬å¼ç™ºè¡¨ã‚’ç¢ºå®Ÿã«å«ã‚ã‚‹ï¼‰
        for item in provider_items[:7]:  # 5â†’7ã«å¢—åŠ 
            if item.url in seen_urls:
                continue
            seen_urls.add(item.url)

            feed_name = item.metadata.get("feed_name", "")
            post = self._create_single_post(
                title=item.title,
                url=item.url,
                source_type="å…¬å¼ç™ºè¡¨",
                source_name=feed_name,
                date=today,
                item=item
            )
            drafts.append(f"ã€æŠ•ç¨¿æ¡ˆ {len(drafts) + 1}ã€‘\n{post}")

        # GitHub Releaseã¯å‰Šé™¤ï¼ˆRSSã§å–å¾—ã™ã‚‹ãŸã‚ä¸è¦ï¼‰

        # ãƒˆãƒƒãƒ—ãƒã‚¤ãƒ©ã‚¤ãƒˆã‹ã‚‰è¿½åŠ ï¼ˆ2â†’1ã«å‰Šæ¸›ï¼‰
        for item in top_items[:1]:
            if item.url in seen_urls:
                continue
            if item.source in ["rss", "github"]:
                continue  # æ—¢ã«è¿½åŠ æ¸ˆã¿

            seen_urls.add(item.url)

            source_name = item.metadata.get("username", "") or item.metadata.get("keyword", "")
            post = self._create_single_post(
                title=item.title,
                url=item.url,
                source_type="Xæ³¨ç›®æŠ•ç¨¿",
                source_name=source_name,
                date=today,
                item=item
            )
            drafts.append(f"ã€æŠ•ç¨¿æ¡ˆ {len(drafts) + 1}ã€‘\n{post}")

        return "\n\n" + ("-" * 50) + "\n\n".join(drafts) if drafts else ""

    def _generate_x_post_draft_blocks(self, top_items: List[Item], provider_items: List[Item], github_items: List[Item], all_items: List[Item]) -> List[Dict]:
        """XæŠ•ç¨¿ç´ æ¡ˆã‚’Slack Blocksã¨ã—ã¦ç”Ÿæˆï¼ˆå„æŠ•ç¨¿ã‚’å€‹åˆ¥ãƒ–ãƒ­ãƒƒã‚¯ã«ï¼‰"""
        blocks = []
        seen_urls = set()
        today = datetime.now().strftime('%Y/%m/%d')
        draft_count = 0

        # ã€å¿…è¦‹ã®æ›´æ–°ã€‘ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        must_include_items = [i for i in all_items if i.metadata.get("must_include")]
        must_include_config = self.config.get("rss", {}).get("must_include_feeds", [])

        if must_include_config:
            blocks.append({"type": "divider"})
            blocks.append({
                "type": "header",
                "text": {"type": "plain_text", "text": "â­ å¿…è¦‹ã®æ›´æ–°"}
            })

            if must_include_items:
                # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
                from collections import defaultdict
                grouped = defaultdict(list)
                for item in must_include_items:
                    feed_name = item.metadata.get("feed_name", "Unknown")
                    grouped[feed_name].append(item)

                # å„ãƒ•ã‚£ãƒ¼ãƒ‰ã®æ›´æ–°ã‚’è¡¨ç¤º
                for feed_name, items in grouped.items():
                    for item in items:
                        seen_urls.add(item.url)
                        draft_count += 1

                        post = self._create_single_post(
                            title=item.title,
                            url=item.url,
                            source_type="å¿…è¦‹ã®æ›´æ–°",
                            source_name=feed_name,
                            date=today,
                            item=item
                        )

                        # æ¤œè¨¼å¤±æ•—æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—
                        if post is None:
                            print(f"â­ï¸  æŠ•ç¨¿æ¡ˆã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ¤œè¨¼å¤±æ•—ï¼‰: {item.title[:50]}...")
                            draft_count -= 1
                            continue

                        blocks.append({
                            "type": "section",
                            "text": {"type": "mrkdwn", "text": f"*ã€æŠ•ç¨¿æ¡ˆ {draft_count}ã€‘{feed_name}*"}
                        })
                        blocks.append({
                            "type": "section",
                            "text": {"type": "mrkdwn", "text": f"```{post}```"}
                        })
                        blocks.append({"type": "divider"})
            else:
                # æ›´æ–°ãŒãªã„å ´åˆ
                feed_names = [f["name"] for f in must_include_config]
                blocks.append({
                    "type": "section",
                    "text": {"type": "mrkdwn", "text": f"ğŸ“­ æœ¬æ—¥ã®æ›´æ–°ãªã—\nå¯¾è±¡: {', '.join(feed_names)}"}
                })
                blocks.append({"type": "divider"})

        # RSSï¼ˆå…¬å¼ç™ºè¡¨ï¼‰ã‚’5ä»¶ã«å‰Šæ¸›ï¼ˆ7â†’5ï¼‰
        for item in provider_items[:5]:
            if item.url in seen_urls:
                continue
            seen_urls.add(item.url)
            draft_count += 1

            feed_name = item.metadata.get("feed_name", "")
            post = self._create_single_post(
                title=item.title,
                url=item.url,
                source_type="å…¬å¼ç™ºè¡¨",
                source_name=feed_name,
                date=today,
                item=item
            )

            # æ¤œè¨¼å¤±æ•—æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—
            if post is None:
                print(f"â­ï¸  æŠ•ç¨¿æ¡ˆã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ¤œè¨¼å¤±æ•—ï¼‰: {item.title[:50]}...")
                draft_count -= 1
                continue

            # å„æŠ•ç¨¿ã‚’å€‹åˆ¥ã®sectionãƒ–ãƒ­ãƒƒã‚¯ã«ï¼ˆ3000æ–‡å­—åˆ¶é™å›é¿ï¼‰
            blocks.append({
                "type": "section",
                "text": {"type": "mrkdwn", "text": f"```ã€æŠ•ç¨¿æ¡ˆ {draft_count}ã€‘\n{post}```"}
            })

            # åŒºåˆ‡ã‚Šç·šã‚’è¿½åŠ ï¼ˆæœ€å¾Œä»¥å¤–ï¼‰
            if draft_count < 7:
                blocks.append({"type": "divider"})

        # Xç”±æ¥ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ç¢ºå®Ÿã«å«ã‚ã‚‹ï¼ˆæ–°è¦è¿½åŠ ï¼‰
        # é‡è¦: top_items ã§ã¯ãªã all_items ã‹ã‚‰æŠ½å‡ºï¼ˆtop_items ã¯3ä»¶ã—ã‹ãªã„ãŸã‚ï¼‰
        x_items = [i for i in all_items if i.source in ["x_account", "x_search"]]
        for item in x_items[:2]:  # Xç”±æ¥ã‚’æœ€å¤§2ä»¶è¿½åŠ 
            if item.url in seen_urls:
                continue
            seen_urls.add(item.url)
            draft_count += 1

            source_name = item.metadata.get("username", "") or item.metadata.get("keyword", "")
            post = self._create_single_post(
                title=item.title,
                url=item.url,
                source_type="Xæ³¨ç›®æŠ•ç¨¿",
                source_name=source_name,
                date=today,
                item=item
            )

            # æ¤œè¨¼å¤±æ•—æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—
            if post is None:
                print(f"â­ï¸  æŠ•ç¨¿æ¡ˆã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ¤œè¨¼å¤±æ•—ï¼‰: {item.title[:50]}...")
                draft_count -= 1
                continue

            blocks.append({
                "type": "section",
                "text": {"type": "mrkdwn", "text": f"```ã€æŠ•ç¨¿æ¡ˆ {draft_count}ã€‘\n{post}```"}
            })

            # åŒºåˆ‡ã‚Šç·šã‚’è¿½åŠ ï¼ˆæœ€å¾Œä»¥å¤–ï¼‰
            if draft_count < 7:
                blocks.append({"type": "divider"})

        return blocks

    def _create_single_post(self, title: str, url: str, source_type: str, source_name: str, date: str, item: Item) -> str:
        """å€‹åˆ¥ã®XæŠ•ç¨¿ã‚’ç”Ÿæˆ"""
        # Twitterã®URLã‚’x.comã«å¤‰æ›
        if "twitter.com" in url:
            url = url.replace("twitter.com", "x.com")

        # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’å–å¾—
        category = item.metadata.get("category", "UNKNOWN")

        # XæŠ•ç¨¿ã®å ´åˆã¯å…¨æ–‡ã‚‚å–å¾—
        tweet_text = None
        if item.source in ["x_account", "x_search"]:
            tweet_text = item.metadata.get("tweet", {}).get("text", "")

        # Claude API ã§ã‚µãƒãƒ©ã‚¤ã‚ºç”Ÿæˆ
        summary = self._generate_summary_with_claude(
            title, url, source_type, category, tweet_text=tweet_text
        )

        return summary

    def _generate_summary_with_claude(self, title: str, url: str, source_type: str, category: str = "UNKNOWN", tweet_text: Optional[str] = None) -> str:
        """Claude API ã§é«˜å“è³ªãªXæŠ•ç¨¿ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ç”Ÿæˆ"""
        try:
            import anthropic

            api_key = os.environ.get("ANTHROPIC_API_KEY")
            if not api_key:
                print("âš ï¸  ANTHROPIC_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç°¡æ˜“è¦ç´„ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
                return self._generate_simple_summary(title, source_type, url)

            client = anthropic.Anthropic(api_key=api_key)

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæƒ…å ±ã‚’å–å¾—
            target = self.config.get("target_audience", {})
            target_name = target.get("name", "AIã«é–¢å¿ƒã®ã‚ã‚‹ãƒ“ã‚¸ãƒã‚¹ãƒ‘ãƒ¼ã‚½ãƒ³ã‚„ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢")

            # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®ãƒ•ã‚©ãƒ¼ã‚«ã‚¹
            category_focus = {
                "PRACTICAL": "å®Ÿè£…æ–¹æ³•ã€å…·ä½“çš„ãªæ©Ÿèƒ½ã€ä½¿ã„æ–¹ã€çµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³ã€å®Ÿè·µçš„ãªTipsã‚’é‡è¦–ã—ã¦ãã ã•ã„ã€‚",
                "TECHNICAL": "æŠ€è¡“çš„ãªä»•çµ„ã¿ã€æ¯”è¼ƒåˆ†æã€è©³ç´°ãªè§£èª¬ã‚’é‡è¦–ã—ã¦ãã ã•ã„ã€‚",
                "GENERAL": "æ–°æ©Ÿèƒ½ã®æ¦‚è¦ã€åˆ©ç”¨é–‹å§‹æ™‚æœŸã€å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’é‡è¦–ã—ã¦ãã ã•ã„ã€‚"
            }.get(category, "")

            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå…¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆå…±é€šï¼‰
            system_prompt = f"""ã‚ãªãŸã¯AIæ¥­ç•Œã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è¿½ã†Xï¼ˆTwitterï¼‰ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®æŠ•ç¨¿ä½œæˆè€…ã§ã™ã€‚
èª­è€…ã¯{target_name}ã§ã™ã€‚

ã€è¨˜äº‹ã‚«ãƒ†ã‚´ãƒªã€‘{category}
{category_focus}

ã€é‡è¦ãªåŸå‰‡ã€‘
- å…·ä½“çš„ã§å®Ÿç”¨çš„ãªæƒ…å ±ã‚’æä¾›ã™ã‚‹
- èª­è€…ãŒã€Œè‡ªåˆ†ã‚‚ä½¿ã£ã¦ã¿ãŸã„ã€ã¨æ€ãˆã‚‹å†…å®¹ã«ã™ã‚‹
- æŠ½è±¡çš„ãªè¡¨ç¾ï¼ˆã€Œé©æ–°çš„ã€ã€Œç”»æœŸçš„ã€ï¼‰ã¯é¿ã‘ã€ä½•ãŒã§ãã‚‹ã‹ã‚’æ˜ç¤ºã™ã‚‹
- çµµæ–‡å­—ã¯æœ€å°é™ã«æŠ‘ãˆã‚‹
- ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç•ªå·ã¯ã€Œ1.ã€ã€Œ2.ã€ã€Œ3.ã€ã®å½¢å¼ã§æ§‹é€ åŒ–ã™ã‚‹

ã€ğŸ’¡ æ¥­ç•Œã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®åˆ¤æ–­åŸºæº–ã€‘
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ã€è¨˜äº‹å†…å®¹ã«æ˜ç¢ºãªæ¥­ç•Œã¸ã®ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆãŒã‚ã‚‹å ´åˆã®ã¿å«ã‚ã¦ãã ã•ã„ã€‚

å«ã‚ã‚‹ã¹ãå ´åˆ:
- å®šé‡çš„ãªãƒ“ã‚¸ãƒã‚¹åŠ¹æœãŒæ˜ç¤ºã•ã‚Œã¦ã„ã‚‹ï¼ˆã‚³ã‚¹ãƒˆå‰Šæ¸›ã€æ™‚é–“çŸ­ç¸®ã€ç”Ÿç”£æ€§å‘ä¸Šãªã©ï¼‰
- æŠ€è¡“çš„ãªå„ªä½æ€§ãƒ»æ”¹å–„ãŒæ˜ç¢ºï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã€æ–°æ©Ÿèƒ½å®Ÿç¾ãªã©ï¼‰
- æ¥­ç•Œã¸ã®å½±éŸ¿ãŒæ¨æ¸¬ã§ãã‚‹ï¼ˆç«¶äº‰å„ªä½æ€§ã€å¸‚å ´ã¸ã®å½±éŸ¿ãªã©ï¼‰
- Webæ¥­ç•Œã®ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å®Ÿå‹™ã«ç›´æ¥çš„ãªãƒ¡ãƒªãƒƒãƒˆãŒã‚ã‚‹
- å®šæ€§çš„ãªåŠ¹æœã§ã‚‚æ¥­ç•Œè¦–ç‚¹ã§æ„ç¾©ãŒã‚ã‚Œã°å«ã‚ã‚‹

é™¤å¤–ã™ã¹ãå ´åˆ:
- ç´”ç²‹ãªæ•™è‚²ãƒ»å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã€å…¥é–€è¨˜äº‹ãªã©ï¼‰
- ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆãŒä¸æ˜ç¢ºãƒ»æ¨æ¸¬çš„ã™ãã‚‹
- ç†è«–çš„ãƒ»æŠ½è±¡çš„ã™ãã¦å®Ÿå‹™åŠ¹æœãŒè¦‹ãˆãªã„
- å˜ãªã‚‹ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆæƒ…å ±ã§å½±éŸ¿ç¯„å›²ãŒé™å®šçš„

é©ç”¨ã®ç›®å®‰: ã‚„ã‚„æŸ”è»Ÿã«åˆ¤æ–­ã—ã€æŠ•ç¨¿ã®50-60%ç¨‹åº¦ã«å«ã¾ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ãã ã•ã„ã€‚"""

            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆXæŠ•ç¨¿ã®å ´åˆã¯å…¨æ–‡ã‚’å«ã‚ã‚‹ï¼‰
            if tweet_text:
                user_prompt = f"""ä»¥ä¸‹ã®XæŠ•ç¨¿ã«ã¤ã„ã¦ã€XæŠ•ç¨¿ã‚¹ãƒ¬ãƒƒãƒ‰ã®ç´ æ¡ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€XæŠ•ç¨¿å…¨æ–‡ã€‘
{tweet_text}

ã€URLã€‘
{url}

ã€ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã€‘
{source_type}

ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘"""
            else:
                user_prompt = f"""ä»¥ä¸‹ã®AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«ã¤ã„ã¦ã€XæŠ•ç¨¿ã‚¹ãƒ¬ãƒƒãƒ‰ã®ç´ æ¡ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã€‘
{title}

ã€ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã€‘
{source_type}

ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘"""

ä¼æ¥­åã€ã€Œè£½å“/æ©Ÿèƒ½åã€ã‚’ç™ºè¡¨/ãƒªãƒªãƒ¼ã‚¹

[2-3æ–‡ã§æ ¸å¿ƒã‚’è¦ç´„ã€‚ä½•ãŒæ–°ã—ã„ã®ã‹ã€ãªãœé‡è¦ãªã®ã‹ã‚’æ˜ç¢ºã«]

{url}

ğŸ’¡ æ¥­ç•Œã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ
ãƒ»[å…·ä½“çš„ãªãƒ“ã‚¸ãƒã‚¹åŠ¹æœã‚„æŠ€è¡“çš„å„ªä½æ€§]
ãƒ»[ã‚³ã‚¹ãƒˆå‰Šæ¸›ã€ç”Ÿç”£æ€§å‘ä¸Šã€æ–°æ©Ÿèƒ½å®Ÿç¾ãªã©ã®å®šé‡çš„ãƒ»å®šæ€§çš„åŠ¹æœ]
ãƒ»[æ¥­ç•Œã¸ã®å½±éŸ¿ã‚„å®Ÿè£…ä¸Šã®ãƒ¡ãƒªãƒƒãƒˆ]

â€»è¨˜äº‹å†…å®¹ã‹ã‚‰æ¥­ç•Œã¸ã®ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆãŒæ˜ç¢ºã«ç¤ºã›ãªã„å ´åˆã¯ã€ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³å…¨ä½“ã‚’å‰Šé™¤ã™ã‚‹ã“ã¨

1. [ã‚»ã‚¯ã‚·ãƒ§ãƒ³å]

ãƒ»å…·ä½“çš„ãªæ©Ÿèƒ½ã‚„ç‰¹å¾´1
ãƒ»å…·ä½“çš„ãªæ©Ÿèƒ½ã‚„ç‰¹å¾´2
ãƒ»å…·ä½“çš„ãªæ©Ÿèƒ½ã‚„ç‰¹å¾´3

[ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è£œè¶³èª¬æ˜ã‚’ä¸€æ–‡ã§]

2. [ã‚»ã‚¯ã‚·ãƒ§ãƒ³å]

ãƒ»å…·ä½“çš„ãªæ©Ÿèƒ½ã‚„ç‰¹å¾´4
ãƒ»å…·ä½“çš„ãªæ©Ÿèƒ½ã‚„ç‰¹å¾´5

[ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è£œè¶³èª¬æ˜ã‚’ä¸€æ–‡ã§]

3. [åˆ©ç”¨æ–¹æ³•ãƒ»å¯¾è±¡è€…]

ãƒ»å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼: [å…·ä½“çš„ã«]
ãƒ»æä¾›é–‹å§‹: [ã„ã¤ã‹ã‚‰]
ãƒ»ä»Šå¾Œã®äºˆå®š: [ã‚ã‚Œã°]

ã€é‡è¦ãªåˆ¶ç´„ã€‘
- å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ç®‡æ¡æ›¸ãã¯3-5é …ç›®
- ç®‡æ¡æ›¸ãã«ã¯ã€Œãƒ»ã€ï¼ˆä¸­é»’ï¼‰ã®ã¿ä½¿ç”¨
- â– ã€â–¸ã€ğŸ”— ãªã©ã®è£…é£¾è¨˜å·ã¯ä¸è¦ï¼ˆğŸ’¡ã¯æ¥­ç•Œã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã®ã¿ä½¿ç”¨ï¼‰
- URLã¯å†’é ­ã®è¦ç´„ã®ç›´å¾Œã«é…ç½®
- ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç•ªå·ã¯ã€Œ1.ã€ã€Œ2.ã€ã€Œ3.ã€ã®å½¢å¼ã§æ§‹é€ åŒ–
- å…¨ä½“ã§600-800æ–‡å­—ç¨‹åº¦ï¼ˆXã‚¹ãƒ¬ãƒƒãƒ‰ã¨ã—ã¦é©åˆ‡ãªé•·ã•ï¼‰
- ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æ¨æ¸¬ã—ã¦å…·ä½“çš„ã«è¨˜è¿°ã—ã¦ãã ã•ã„"""

            message = client.messages.create(
                model="claude-sonnet-4-5-20250929",
                max_tokens=1500,  # 800æ–‡å­—è¦æ±‚ãªã®ã§ä½™è£•ã‚’æŒãŸã›ã‚‹
                system=system_prompt,  # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¿½åŠ 
                messages=[{
                    "role": "user",
                    "content": user_prompt
                }]
            )

            generated_text = message.content[0].text

            # æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º1: æ­£è¦è¡¨ç¾ãƒ™ãƒ¼ã‚¹
            validation_result = self.validator.validate_post(generated_text, title)

            if not validation_result.is_valid:
                print(f"âš ï¸  æŠ•ç¨¿æ¡ˆãŒæ¤œè¨¼å¤±æ•—: {validation_result.rejection_reason}")
                print(f"    ã‚¿ã‚¤ãƒˆãƒ«: {title[:50]}...")
                print(f"    æ¤œå‡ºå•é¡Œ: {validation_result.detected_issues}")
                return None

            # æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º2: Claude APIãƒ¬ãƒ“ãƒ¥ãƒ¼
            review_result = self.validator.review_post_with_claude(generated_text, title, url)

            if not review_result.is_valid:
                print(f"âš ï¸  æŠ•ç¨¿æ¡ˆãŒãƒ¬ãƒ“ãƒ¥ãƒ¼å¤±æ•—: {review_result.rejection_reason}")
                print(f"    ã‚¿ã‚¤ãƒˆãƒ«: {title[:50]}...")
                print(f"    æ¤œå‡ºå•é¡Œ: {review_result.detected_issues}")
                return None

            return generated_text

        except Exception as e:
            print(f"âš ï¸ Claude API ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç°¡æ˜“è¦ç´„
            return self._generate_simple_summary(title, source_type, url)

    def _generate_simple_summary(self, title: str, source_type: str, url: str) -> str:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ç°¡æ˜“è¦ç´„"""
        if source_type == "å…¬å¼ç™ºè¡¨":
            emoji = "ğŸš€"
            comment = "é‡è¦ãªå…¬å¼ã‚¢ãƒŠã‚¦ãƒ³ã‚¹ã§ã™"
        elif source_type == "GitHub Release":
            emoji = "ğŸ“¦"
            comment = "æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã¾ã—ãŸ"
        else:
            emoji = "ğŸ’¡"
            comment = "æ³¨ç›®ã®è©±é¡Œã§ã™"

        # ç°¡æ½”ãªå½¢å¼
        lines = [
            f"ã€{emoji} {title[:60]}{'...' if len(title) > 60 else ''}ã€‘",
            "",
            f"ğŸ’¡ {comment}ã€‚è©³ç´°ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã—ã‚‡ã†ã€‚",
            "",
            f"ğŸ”— {url}"
        ]

        return "\n".join(lines)


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("AIãƒ‡ã‚¤ãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆ")
    print("=" * 60)

    # è¨­å®šèª­ã¿è¾¼ã¿
    with open("config.yaml", 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
    x_bearer_token = os.environ.get("X_BEARER_TOKEN")
    slack_webhook_url = os.environ.get("SLACK_WEBHOOK_URL")

    if not x_bearer_token:
        raise ValueError("ç’°å¢ƒå¤‰æ•° X_BEARER_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    if not slack_webhook_url:
        raise ValueError("ç’°å¢ƒå¤‰æ•° SLACK_WEBHOOK_URL ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

    # åˆæœŸåŒ–
    state = StateManager()
    x_client = XAPIClient(x_bearer_token)
    collector = DataCollector(config, state, x_client)

    try:
        # ãƒ‡ãƒ¼ã‚¿åé›†
        collector.collect_all()

        # Slackãƒ¬ãƒãƒ¼ãƒˆé€ä¿¡
        reporter = SlackReporter(slack_webhook_url, config, collector.items, collector.stats)
        reporter.send()

        # çŠ¶æ…‹ä¿å­˜ï¼ˆæœ€å¾Œã«å®Ÿè¡Œï¼‰
        state.save()
        print("ğŸ’¾ çŠ¶æ…‹ã‚’ä¿å­˜ã—ã¾ã—ãŸ")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        raise

    print("=" * 60)
    print("âœ… å®Œäº†")
    print("=" * 60)


if __name__ == "__main__":
    main()
