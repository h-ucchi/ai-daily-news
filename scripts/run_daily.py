#!/usr/bin/env python3
"""
AIãƒ‡ã‚¤ãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
X API Basic ($200/æœˆ) ã«åã¾ã‚‹ã‚ˆã†è¨­è¨ˆ
"""

import os
import json
import yaml
import requests
import feedparser
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from collections import defaultdict
import time
from content_classifier import ContentClassifier, ClassificationResult
from content_validator import ContentValidator
from post_prompt import get_system_prompt, create_user_prompt_from_tweet, create_user_prompt_from_article
from article_fetcher import fetch_article_content_safe
from state_manager import StateManager
from ai_lint_checker import AILintChecker


@dataclass
class Item:
    """åé›†ã—ãŸæƒ…å ±ã‚¢ã‚¤ãƒ†ãƒ """
    source: str  # "x_account", "x_search", "rss", "github"
    title: str
    url: str
    published_at: str
    score: int = 0
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class XAPIClient:
    """X (Twitter) API v2 ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""

    def __init__(self, bearer_token: str, oauth_credentials: Optional[Dict] = None):
        # èª­ã¿å–ã‚Šç”¨ï¼ˆæ—¢å­˜ï¼‰
        self.bearer_token = bearer_token
        self.base_url = "https://api.twitter.com/2"
        self.headers = {"Authorization": f"Bearer {bearer_token}"}

        # æ›¸ãè¾¼ã¿ç”¨ï¼ˆæ–°è¦ï¼‰
        if oauth_credentials:
            from requests_oauthlib import OAuth1Session
            self.oauth = OAuth1Session(
                client_key=oauth_credentials['api_key'],
                client_secret=oauth_credentials['api_secret'],
                resource_owner_key=oauth_credentials['access_token'],
                resource_owner_secret=oauth_credentials['access_token_secret']
            )
        else:
            self.oauth = None

    def get_user_id(self, username: str) -> Optional[str]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’å–å¾—"""
        url = f"{self.base_url}/users/by/username/{username}"
        response = requests.get(url, headers=self.headers)
        if response.status_code == 200:
            data = response.json()
            return data.get("data", {}).get("id")
        return None

    def get_user_tweets(self, user_id: str, since_id: Optional[str] = None, max_results: int = 10) -> tuple:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å–å¾—ï¼ˆæ–°ç€ã®ã¿ã€ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°æƒ…å ±ä»˜ãï¼‰"""
        url = f"{self.base_url}/users/{user_id}/tweets"
        params = {
            "max_results": min(max_results, 100),
            "tweet.fields": "created_at,public_metrics",
            "expansions": "author_id",
            "user.fields": "public_metrics"
        }
        if since_id:
            params["since_id"] = since_id
        else:
            # åˆå›å®Ÿè¡Œæ™‚ã¯ç›´è¿‘24æ™‚é–“
            start_time = (datetime.now(timezone.utc) - timedelta(hours=24)).isoformat()
            params["start_time"] = start_time

        response = requests.get(url, headers=self.headers, params=params)
        if response.status_code == 200:
            data = response.json()
            tweets = data.get("data", [])
            users = {u["id"]: u for u in data.get("includes", {}).get("users", [])}
            return tweets, users
        return [], {}

    def search_tweets(self, query: str, since_id: Optional[str] = None, max_results: int = 10) -> tuple:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ„ã‚¤ãƒ¼ãƒˆæ¤œç´¢ï¼ˆæ–°ç€ã®ã¿ã€ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°æƒ…å ±ä»˜ãï¼‰"""
        url = f"{self.base_url}/tweets/search/recent"
        params = {
            "query": query,
            "max_results": min(max_results, 100),
            "tweet.fields": "created_at,public_metrics",
            "expansions": "author_id",
            "user.fields": "public_metrics"
        }
        if since_id:
            params["since_id"] = since_id
        else:
            # åˆå›å®Ÿè¡Œæ™‚ã¯ç›´è¿‘24æ™‚é–“
            start_time = (datetime.now(timezone.utc) - timedelta(hours=24)).isoformat()
            params["start_time"] = start_time

        response = requests.get(url, headers=self.headers, params=params)
        if response.status_code == 200:
            data = response.json()
            tweets = data.get("data", [])
            users = {u["id"]: u for u in data.get("includes", {}).get("users", [])}
            return tweets, users
        return [], {}

    def post_tweet(self, text: str) -> Dict:
        """ãƒ„ã‚¤ãƒ¼ãƒˆæŠ•ç¨¿"""
        if not self.oauth:
            raise ValueError("OAuth credentials not configured")

        url = f"{self.base_url}/tweets"
        payload = {"text": text}

        response = self.oauth.post(url, json=payload)

        if response.status_code == 201:
            return response.json()
        else:
            raise Exception(
                f"Failed to post tweet: {response.status_code} {response.text}"
            )


class DataCollector:
    """ãƒ‡ãƒ¼ã‚¿åé›†ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼"""

    # å„ªå…ˆåº¦ã®é«˜ã„RSSãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆé‡è¦ãƒ™ãƒ³ãƒ€ãƒ¼ã®æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¼ã‚‰ã•ãªã„ãŸã‚ï¼‰
    PRIORITY_FEEDS = {
        # å…¬å¼ãƒ–ãƒ­ã‚°
        "https://www.anthropic.com/news/rss.xml": 1000,  # Anthropicæœ€å„ªå…ˆ
        "https://openai.com/blog/rss.xml": 1000,          # OpenAIæœ€å„ªå…ˆ
        "https://github.blog/feed/": 800,                 # GitHub Blog
        "https://code.visualstudio.com/updates/feed.xml": 800,  # VSCode Updates

        # GitHub Releases Atom Feedï¼ˆå¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã¯must_include_feedsã§ç®¡ç†ï¼‰
        "https://github.com/anthropics/claude-code/releases.atom": 800,  # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã«ç§»è¡Œ
        "https://github.blog/changelog/label/copilot/feed/": 800,        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã«ç§»è¡Œ
        "https://github.com/langchain-ai/langchain/releases.atom": 800,   # LangChain
        "https://github.com/openai/openai-python/releases.atom": 800,     # OpenAI Python SDK
        "https://github.com/run-llama/llama_index/releases.atom": 600,    # LlamaIndex
        "https://github.com/huggingface/transformers/releases.atom": 600, # Transformers
    }

    def __init__(self, config: Dict, state: StateManager, x_client: XAPIClient):
        self.config = config
        self.state = state
        self.x_client = x_client
        self.items: List[Item] = []
        self.stats = {
            "x_accounts_fetched": 0,
            "x_search_fetched": 0,
            "x_total_fetched": 0,
            "x_limit_reached": False,
            "x_followers_filtered": 0,
            "rss_fetched": 0,
            "github_fetched": 0,
            "duplicates_removed": 0
        }
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†é¡å™¨ã®åˆæœŸåŒ–
        if self.config.get("content_filtering", {}).get("enabled"):
            self.classifier = ContentClassifier(config)
        else:
            self.classifier = None

    def collect_all(self):
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†"""
        print("ğŸ” ãƒ‡ãƒ¼ã‚¿åé›†ã‚’é–‹å§‹...")

        # X (Twitter)
        self._collect_x_accounts()
        self._collect_x_search()

        # RSSï¼ˆGitHub Releases Atom Feedã‚’å«ã‚€ï¼‰
        # â˜… RSSå‡¦ç†ã¯run_hourly.pyã«ç§»è¡Œã—ãŸãŸã‚ç„¡åŠ¹åŒ–
        # self._collect_rss()

        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆå½“æ—¥ã®æ›´æ–°ãŒã‚ã‚Œã°å¿…ãšå«ã‚ã‚‹ï¼‰
        must_include_items = self._collect_must_include_feeds()
        self.items.extend(must_include_items)
        self.stats["must_include_fetched"] = len(must_include_items)

        # é‡è¤‡æ’é™¤
        self._deduplicate()

        print(f"âœ… åé›†å®Œäº†: {len(self.items)} ä»¶")

    def _collect_x_accounts(self):
        """Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‹ã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆåé›†ï¼ˆãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä»˜ãï¼‰"""
        accounts = self.config["x"]["accounts"]
        limit = self.config["x"]["limits"]["accounts"]
        fetched = 0

        # ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãƒ•ã‚£ãƒ«ã‚¿è¨­å®š
        follower_filter = self.config["x"].get("follower_filter", {})
        filter_enabled = follower_filter.get("enabled", False)
        min_followers = follower_filter.get("min_followers", 0)

        print(f"ğŸ“± Xã‚¢ã‚«ã‚¦ãƒ³ãƒˆç›£è¦–: {len(accounts)} ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ")
        if filter_enabled:
            print(f"   ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãƒ•ã‚£ãƒ«ã‚¿: {min_followers:,}äººä»¥ä¸Š")

        for username in accounts:
            if fetched >= limit:
                print(f"âš ï¸  ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç›£è¦–ã®ä¸Šé™ {limit} ä»¶ã«åˆ°é”")
                self.stats["x_limit_reached"] = True
                break

            user_id = self.x_client.get_user_id(username)
            if not user_id:
                continue

            since_id = self.state.get_x_account_since_id(username)
            tweets, users = self.x_client.get_user_tweets(user_id, since_id, max_results=10)

            if not tweets:
                continue

            # æœ€æ–°ã®tweet_idã‚’ä¿å­˜
            max_id = max(int(t["id"]) for t in tweets)
            self.state.set_x_account_since_id(username, user_id, str(max_id))

            for tweet in tweets:
                if fetched >= limit:
                    break

                # ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if filter_enabled:
                    author_id = tweet.get("author_id")
                    user = users.get(author_id, {})
                    followers_count = user.get("public_metrics", {}).get("followers_count", 0)

                    if followers_count < min_followers:
                        tweet_text_short = tweet["text"][:50]
                        print(f"  â­ï¸  é™¤å¤–ï¼ˆãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°: {followers_count:,}ï¼‰: {tweet_text_short}...")
                        self.stats["x_followers_filtered"] += 1
                        continue

                # è¨€èªãƒ»åœ°åŸŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                tweet_text = tweet["text"]
                tweet_url = f"https://twitter.com/{username}/status/{tweet['id']}"

                if self.classifier:
                    # ç·åˆçš„ãªåˆ†é¡ï¼ˆè¨€èªãƒ»åœ°åŸŸãƒã‚§ãƒƒã‚¯ã‚’å«ã‚€ï¼‰
                    classification = self.classifier.classify(tweet_text, "", tweet_url)
                    category = classification.category

                    # éè‹±èªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¾ãŸã¯æ—¥æœ¬ç”±æ¥ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯é™¤å¤–
                    if category in ["NON_ENGLISH", "JAPAN_ORIGIN"]:
                        print(f"  â­ï¸  é™¤å¤–ï¼ˆ{category}ï¼‰: {tweet_text[:50]}...")
                        continue
                else:
                    category = "UNKNOWN"

                # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã¨ã‚¹ã‚³ã‚¢èª¿æ•´
                initial_score = self._calculate_engagement_score(tweet)
                if self.classifier:
                    final_score = self.classifier.calculate_final_score(
                        initial_score, category, "x_account"
                    )
                else:
                    final_score = initial_score

                item = Item(
                    source="x_account",
                    title=tweet_text[:100],
                    url=tweet_url,
                    published_at=tweet["created_at"],
                    score=final_score,
                    metadata={
                        "username": username,
                        "tweet": tweet,
                        "category": category
                    }
                )
                self.items.append(item)
                fetched += 1

        self.stats["x_accounts_fetched"] = fetched
        self.stats["x_total_fetched"] += fetched

    def _collect_x_search(self):
        """Xã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä»˜ãï¼‰"""
        keywords = self.config["x"]["keywords"]
        limit = self.config["x"]["limits"]["search"]
        fetched = 0

        # ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãƒ•ã‚£ãƒ«ã‚¿è¨­å®š
        follower_filter = self.config["x"].get("follower_filter", {})
        filter_enabled = follower_filter.get("enabled", False)
        min_followers = follower_filter.get("min_followers", 0)

        print(f"ğŸ” Xã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢: {len(keywords)} ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
        if filter_enabled:
            print(f"   ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãƒ•ã‚£ãƒ«ã‚¿: {min_followers:,}äººä»¥ä¸Š")

        for keyword in keywords:
            if fetched >= limit:
                print(f"âš ï¸  æ¤œç´¢ã®ä¸Šé™ {limit} ä»¶ã«åˆ°é”")
                self.stats["x_limit_reached"] = True
                break

            since_id = self.state.get_x_keyword_since_id(keyword)
            tweets, users = self.x_client.search_tweets(keyword, since_id, max_results=10)

            if not tweets:
                continue

            # æœ€æ–°ã®tweet_idã‚’ä¿å­˜
            max_id = max(int(t["id"]) for t in tweets)
            self.state.set_x_keyword_since_id(keyword, str(max_id))

            for tweet in tweets:
                if fetched >= limit:
                    break

                # ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if filter_enabled:
                    author_id = tweet.get("author_id")
                    user = users.get(author_id, {})
                    followers_count = user.get("public_metrics", {}).get("followers_count", 0)

                    if followers_count < min_followers:
                        tweet_text_short = tweet["text"][:50]
                        print(f"  â­ï¸  é™¤å¤–ï¼ˆãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°: {followers_count:,}ï¼‰: {tweet_text_short}...")
                        self.stats["x_followers_filtered"] += 1
                        continue

                # è¨€èªãƒ»åœ°åŸŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                tweet_text = tweet["text"]
                tweet_url = f"https://twitter.com/i/web/status/{tweet['id']}"

                if self.classifier:
                    # ç·åˆçš„ãªåˆ†é¡ï¼ˆè¨€èªãƒ»åœ°åŸŸãƒã‚§ãƒƒã‚¯ã‚’å«ã‚€ï¼‰
                    classification = self.classifier.classify(tweet_text, "", tweet_url)
                    category = classification.category

                    # éè‹±èªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¾ãŸã¯æ—¥æœ¬ç”±æ¥ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯é™¤å¤–
                    if category in ["NON_ENGLISH", "JAPAN_ORIGIN"]:
                        print(f"  â­ï¸  é™¤å¤–ï¼ˆ{category}ï¼‰: {tweet_text[:50]}...")
                        continue
                else:
                    category = "UNKNOWN"

                # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã¨ã‚¹ã‚³ã‚¢èª¿æ•´
                initial_score = self._calculate_engagement_score(tweet)
                if self.classifier:
                    final_score = self.classifier.calculate_final_score(
                        initial_score, category, "x_search"
                    )
                else:
                    final_score = initial_score

                item = Item(
                    source="x_search",
                    title=tweet_text[:100],
                    url=tweet_url,
                    published_at=tweet["created_at"],
                    score=final_score,
                    metadata={
                        "keyword": keyword,
                        "tweet": tweet,
                        "category": category
                    }
                )
                self.items.append(item)
                fetched += 1

        self.stats["x_search_fetched"] = fetched
        self.stats["x_total_fetched"] += fetched

    def _collect_rss(self):
        """RSSåé›†ï¼ˆè¨˜äº‹URLãƒªã‚¹ãƒˆæ¯”è¼ƒæ–¹å¼ï¼‰"""
        feeds = self.config["rss"]["feeds"]

        # çµ±è¨ˆæƒ…å ±ã®åˆæœŸåŒ–
        rss_stats = {
            "total_feeds": len(feeds),
            "success_feeds": 0,
            "failed_feeds": 0,
            "total_entries": 0,
            "filtered_out": 0,
            "new_articles": 0,
            "old_articles_filtered": 0,
            "added": 0
        }

        print(f"ğŸ“° RSSåé›†: {len(feeds)} ãƒ•ã‚£ãƒ¼ãƒ‰")

        for feed_config in feeds:
            feed_url = feed_config["url"]
            feed_name = feed_config["name"]

            print(f"\nğŸ“¡ {feed_name}")
            print(f"   URL: {feed_url}")

            # ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—
            feed = feedparser.parse(feed_url)

            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯1: HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
            if hasattr(feed, 'status') and feed.status >= 400:
                print(f"   âš ï¸  HTTP {feed.status}: å–å¾—å¤±æ•—")
                rss_stats["failed_feeds"] += 1
                continue

            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯2: ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼
            if hasattr(feed, 'bozo') and feed.bozo and not feed.entries:
                print(f"   âš ï¸  ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {feed.get('bozo_exception', 'Unknown error')}")
                rss_stats["failed_feeds"] += 1
                continue

            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯3: è¨˜äº‹ãŒ0ä»¶
            if not feed.entries:
                print(f"   â„¹ï¸  è¨˜äº‹0ä»¶")
                rss_stats["failed_feeds"] += 1
                continue

            rss_stats["success_feeds"] += 1
            rss_stats["total_entries"] += len(feed.entries)
            print(f"   âœ… è¨˜äº‹å–å¾—: {len(feed.entries)}ä»¶")

            # å‰å›å–å¾—ã—ãŸè¨˜äº‹URLãƒªã‚¹ãƒˆã‚’å–å¾—
            previous_urls = self.state.get_rss_article_urls(feed_url)
            if previous_urls is None:
                previous_urls = []
                print(f"   â„¹ï¸  åˆå›å–å¾—ï¼ˆå…¨è¨˜äº‹ã‚’å¯¾è±¡ï¼‰")

            # ä»Šå›å–å¾—ã—ãŸè¨˜äº‹URLãƒªã‚¹ãƒˆ
            current_urls = [entry.link for entry in feed.entries]

            # å·®åˆ†ï¼ˆæ–°è¦è¨˜äº‹ï¼‰ã‚’æŠ½å‡º
            new_urls = set(current_urls) - set(previous_urls)

            if new_urls:
                print(f"   ğŸ†• æ–°è¦è¨˜äº‹: {len(new_urls)}ä»¶")
                rss_stats["new_articles"] += len(new_urls)
            else:
                print(f"   â„¹ï¸  æ–°è¦è¨˜äº‹ãªã—ï¼ˆå‰å›ã¨åŒã˜å†…å®¹ï¼‰")

            # 24æ™‚é–“å‰ã®ã‚«ãƒƒãƒˆã‚ªãƒ•ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¨ˆç®—
            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=24)

            feed_added = 0

            for entry in feed.entries:
                # æ–°è¦è¨˜äº‹ã®ã¿å‡¦ç†
                if entry.link not in new_urls:
                    continue

                published = entry.get("published_parsed") or entry.get("updated_parsed")
                if not published:
                    continue

                published_dt = datetime(*published[:6], tzinfo=timezone.utc)
                published_iso = published_dt.isoformat()

                # 24æ™‚é–“ä»¥å†…ã®è¨˜äº‹ã®ã¿ã‚’å¯¾è±¡
                if published_dt < cutoff_time:
                    rss_stats["old_articles_filtered"] += 1
                    continue

                # è¨€èªãƒ»åœ°åŸŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if self.classifier:
                    description = entry.get("summary", "")
                    url = entry.link

                    classification = self.classifier.classify(entry.title, description, url)
                    category = classification.category

                    if category in ["NON_ENGLISH", "JAPAN_ORIGIN"]:
                        print(f"   â­ï¸  é™¤å¤–ï¼ˆ{category}ï¼‰: {entry.title[:50]}...")
                        rss_stats["filtered_out"] += 1
                        continue
                else:
                    category = "UNKNOWN"

                # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
                base_score = self.config["slack"]["scoring"]["rss_bonus"]
                priority_bonus = self.PRIORITY_FEEDS.get(feed_url, 0)
                initial_score = base_score + priority_bonus

                if self.classifier:
                    final_score = self.classifier.calculate_final_score(
                        initial_score, category, "rss", is_official=True
                    )
                else:
                    final_score = initial_score

                item = Item(
                    source="rss",
                    title=entry.title,
                    url=entry.link,
                    published_at=published_iso,
                    score=final_score,
                    metadata={
                        "feed_name": feed_name,
                        "feed_url": feed_url,
                        "category": category
                    }
                )
                self.items.append(item)
                feed_added += 1
                rss_stats["added"] += 1

            if feed_added > 0:
                print(f"   â• è¿½åŠ : {feed_added}ä»¶")

            # ä»Šå›ã®è¨˜äº‹URLãƒªã‚¹ãƒˆã‚’ä¿å­˜ï¼ˆæœ€æ–°20ä»¶ã®ã¿ï¼‰
            self.state.set_rss_article_urls(feed_url, current_urls)
            print(f"   ğŸ’¾ è¨˜äº‹URLãƒªã‚¹ãƒˆä¿å­˜: {len(current_urls[:20])}ä»¶")

            # æœ€çµ‚ç¢ºèªæ™‚åˆ»ã‚’ä¿å­˜
            current_time = datetime.now(timezone.utc).isoformat()
            self.state.set_rss_last_checked(feed_url, current_time)

            # æœ€æ–°ã®published_atã‚‚ä¿å­˜ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
            if feed.entries:
                latest = max(feed.entries, key=lambda e: e.get("published_parsed") or e.get("updated_parsed"))
                published = latest.get("published_parsed") or latest.get("updated_parsed")
                if published:
                    published_dt = datetime(*published[:6], tzinfo=timezone.utc)
                    self.state.set_rss_last_published(feed_url, published_dt.isoformat())

        # çµ±è¨ˆå‡ºåŠ›
        print(f"\nğŸ“Š RSSåé›†çµ±è¨ˆ:")
        print(f"   å¯¾è±¡ãƒ•ã‚£ãƒ¼ãƒ‰: {rss_stats['total_feeds']}ä»¶")
        print(f"   å–å¾—æˆåŠŸ: {rss_stats['success_feeds']}ä»¶")
        print(f"   å–å¾—å¤±æ•—: {rss_stats['failed_feeds']}ä»¶")
        print(f"   ç·è¨˜äº‹æ•°: {rss_stats['total_entries']}ä»¶")
        print(f"   æ–°è¦è¨˜äº‹: {rss_stats['new_articles']}ä»¶")
        print(f"   å¤ã„è¨˜äº‹é™¤å¤–: {rss_stats['old_articles_filtered']}ä»¶")
        print(f"   ãƒ•ã‚£ãƒ«ã‚¿é™¤å¤–: {rss_stats['filtered_out']}ä»¶")
        print(f"   è¿½åŠ ä»¶æ•°: {rss_stats['added']}ä»¶")

        self.stats["rss_fetched"] = rss_stats["added"]

    def _collect_must_include_feeds(self) -> List[Item]:
        """å¿…ãšå«ã‚ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰å½“æ—¥ã®æ›´æ–°ã‚’å–å¾—"""
        must_include_config = self.config.get("rss", {}).get("must_include_feeds", [])
        must_include_items = []

        if not must_include_config:
            return must_include_items

        print(f"â­ å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰åé›†: {len(must_include_config)} ãƒ•ã‚£ãƒ¼ãƒ‰")

        # å½“æ—¥ã®æ—¥ä»˜ï¼ˆUTCã§00:00:00ï¼‰
        today = datetime.now(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)

        for feed_config in must_include_config:
            feed_url = feed_config["url"]
            feed_name = feed_config["name"]
            max_items = feed_config.get("max_items", 3)

            feed = feedparser.parse(feed_url)
            count = 0

            for entry in feed.entries:
                if count >= max_items:
                    break

                published = entry.get("published_parsed") or entry.get("updated_parsed")
                if not published:
                    continue

                published_dt = datetime(*published[:6], tzinfo=timezone.utc)
                published_iso = published_dt.isoformat()

                # å½“æ—¥ã®æ›´æ–°ã®ã¿ï¼ˆ00:00:00ä»¥é™ï¼‰
                if published_dt < today:
                    continue

                # è¨€èªãƒ»åœ°åŸŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if self.classifier:
                    description = entry.get("summary", "")
                    url = entry.link

                    # ç·åˆçš„ãªåˆ†é¡ï¼ˆè¨€èªãƒ»åœ°åŸŸãƒã‚§ãƒƒã‚¯ã‚’å«ã‚€ï¼‰
                    classification = self.classifier.classify(entry.title, description, url)
                    category = classification.category

                    # éè‹±èªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¾ãŸã¯æ—¥æœ¬ç”±æ¥ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯é™¤å¤–
                    # ãŸã ã—ã€å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ãªã®ã§è­¦å‘Šã®ã¿å‡ºåŠ›
                    if category in ["NON_ENGLISH", "JAPAN_ORIGIN"]:
                        print(f"  âš ï¸  å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã ãŒéè‹±èª/æ—¥æœ¬ç”±æ¥ï¼ˆ{category}ï¼‰: {entry.title[:50]}...")
                        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ãªã®ã§é™¤å¤–ã›ãšã«å«ã‚ã‚‹ï¼ˆã‚¹ã‚³ã‚¢ã¯ä½ãã™ã‚‹ï¼‰
                        category = category  # ãã®ã¾ã¾ä½¿ã†
                    elif category == "PRACTICAL":
                        category = "PRACTICAL"  # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã®PRACTICALã¯ãã®ã¾ã¾
                else:
                    category = "PRACTICAL"  # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§PRACTICAL

                item = Item(
                    source="must_include",
                    title=entry.title,
                    url=entry.link,
                    published_at=published_iso,
                    score=9999,  # æœ€é«˜ã‚¹ã‚³ã‚¢ï¼ˆå¿…ãšå«ã¾ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ï¼‰
                    metadata={
                        "feed_name": feed_name,
                        "feed_url": feed_url,
                        "category": category,
                        "must_include": True
                    }
                )
                must_include_items.append(item)
                count += 1
                print(f"  âœ“ {feed_name}: {entry.title[:50]}...")

        return must_include_items

    def _select_diverse_provider_items(self, sorted_items: List[Item], limit: int) -> List[Item]:
        """
        å„ªå…ˆåº¦ã®é«˜ã„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æœ€æ–°è¨˜äº‹ã‚’1ä»¶ãšã¤ãƒãƒ©ãƒ³ã‚¹ã‚ˆãé¸æŠ

        Args:
            sorted_items: ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã®ã‚¢ã‚¤ãƒ†ãƒ ãƒªã‚¹ãƒˆ
            limit: é¸æŠã™ã‚‹æœ€å¤§ä»¶æ•°

        Returns:
            ãƒãƒ©ãƒ³ã‚¹ã‚ˆãé¸æŠã•ã‚ŒãŸRSSã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆ
        """
        from collections import defaultdict

        # RSSã‚¢ã‚¤ãƒ†ãƒ ã®ã¿æŠ½å‡º
        rss_items = [i for i in sorted_items if i.source == "rss"]

        # ãƒ•ã‚£ãƒ¼ãƒ‰åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        feed_groups = defaultdict(list)
        for item in rss_items:
            feed_url = item.metadata.get("feed_url", "")
            if feed_url:
                feed_groups[feed_url].append(item)

        # å„ªå…ˆé †ä½é †ã«ã‚½ãƒ¼ãƒˆï¼ˆPRIORITY_FEEDS ã®å®šç¾©é †ï¼‰
        priority_order = list(self.PRIORITY_FEEDS.keys())
        sorted_feed_urls = sorted(
            feed_groups.keys(),
            key=lambda url: priority_order.index(url) if url in priority_order else 9999
        )

        # ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ­ãƒ“ãƒ³æ–¹å¼ã§é¸æŠï¼ˆå„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰1ä»¶ãšã¤ï¼‰
        selected = []
        round_num = 0

        while len(selected) < limit:
            added_this_round = False

            for feed_url in sorted_feed_urls:
                if len(selected) >= limit:
                    break

                items = feed_groups[feed_url]
                if round_num < len(items):
                    selected.append(items[round_num])
                    added_this_round = True

            # å…¨ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰é¸æŠã—çµ‚ãˆãŸ
            if not added_this_round:
                break

            round_num += 1

        return selected[:limit]

    def _calculate_engagement_score(self, tweet: Dict) -> int:
        """ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢è¨ˆç®—"""
        metrics = tweet.get("public_metrics", {})
        scoring = self.config["slack"]["scoring"]

        score = (
            metrics.get("like_count", 0) * scoring["like_weight"] +
            metrics.get("retweet_count", 0) * scoring["retweet_weight"] +
            metrics.get("reply_count", 0) * scoring["reply_weight"]
        )
        return score

    def _deduplicate(self):
        """é‡è¤‡æ’é™¤ï¼ˆURLåŸºæº–ï¼‰"""
        seen_urls = set()
        unique_items = []

        for item in self.items:
            if item.url not in seen_urls:
                seen_urls.add(item.url)
                unique_items.append(item)
            else:
                self.stats["duplicates_removed"] += 1

        self.items = unique_items


class SlackReporter:
    """Slackãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ»æŠ•ç¨¿"""

    # DataCollectorã®PRIORITY_FEEDSã¨åŒã˜å®šç¾©
    PRIORITY_FEEDS = {
        # å…¬å¼ãƒ–ãƒ­ã‚°
        "https://www.anthropic.com/news/rss.xml": 1000,
        "https://openai.com/blog/rss.xml": 1000,
        "https://github.blog/feed/": 800,
        "https://code.visualstudio.com/updates/feed.xml": 800,

        # GitHub Releases Atom Feedï¼ˆå¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã¯must_include_feedsã§ç®¡ç†ï¼‰
        "https://github.com/anthropics/claude-code/releases.atom": 800,
        "https://github.blog/changelog/label/copilot/feed/": 800,
        "https://github.com/langchain-ai/langchain/releases.atom": 800,
        "https://github.com/openai/openai-python/releases.atom": 800,
        "https://github.com/run-llama/llama_index/releases.atom": 600,
        "https://github.com/huggingface/transformers/releases.atom": 600,
    }

    def __init__(self, webhook_url: str, config: Dict, items: List[Item], stats: Dict):
        self.webhook_url = webhook_url
        self.config = config
        self.items = items
        self.stats = stats
        # æ¤œè¨¼å™¨ã®åˆæœŸåŒ–
        self.validator = ContentValidator(config)
        # AI-lintãƒã‚§ãƒƒã‚«ãƒ¼ã®åˆæœŸåŒ–
        rules_path = os.path.join(os.path.dirname(__file__), "..", "ai-lint", ".claude", "skills", "ai-lint", "rules", "ai-lint-rules.yml")
        if os.path.exists(rules_path):
            self.ai_lint_checker = AILintChecker(rules_path)
        else:
            self.ai_lint_checker = AILintChecker()  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ«ãƒ¼ãƒ«

    def _select_diverse_provider_items(self, sorted_items: List[Item], limit: int) -> List[Item]:
        """
        å„ªå…ˆåº¦ã®é«˜ã„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æœ€æ–°è¨˜äº‹ã‚’1ä»¶ãšã¤ãƒãƒ©ãƒ³ã‚¹ã‚ˆãé¸æŠ

        Args:
            sorted_items: ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã®ã‚¢ã‚¤ãƒ†ãƒ ãƒªã‚¹ãƒˆ
            limit: é¸æŠã™ã‚‹æœ€å¤§ä»¶æ•°

        Returns:
            ãƒãƒ©ãƒ³ã‚¹ã‚ˆãé¸æŠã•ã‚ŒãŸRSSã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆ
        """
        from collections import defaultdict

        # RSSã‚¢ã‚¤ãƒ†ãƒ ã®ã¿æŠ½å‡º
        rss_items = [i for i in sorted_items if i.source == "rss"]

        # ãƒ•ã‚£ãƒ¼ãƒ‰åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        feed_groups = defaultdict(list)
        for item in rss_items:
            feed_url = item.metadata.get("feed_url", "")
            if feed_url:
                feed_groups[feed_url].append(item)

        # å„ªå…ˆé †ä½é †ã«ã‚½ãƒ¼ãƒˆï¼ˆPRIORITY_FEEDS ã®å®šç¾©é †ï¼‰
        priority_order = list(self.PRIORITY_FEEDS.keys())
        sorted_feed_urls = sorted(
            feed_groups.keys(),
            key=lambda url: priority_order.index(url) if url in priority_order else 9999
        )

        # ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ­ãƒ“ãƒ³æ–¹å¼ã§é¸æŠï¼ˆå„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰1ä»¶ãšã¤ï¼‰
        selected = []
        round_num = 0

        while len(selected) < limit:
            added_this_round = False

            for feed_url in sorted_feed_urls:
                if len(selected) >= limit:
                    break

                items = feed_groups[feed_url]
                if round_num < len(items):
                    selected.append(items[round_num])
                    added_this_round = True

            # å…¨ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰é¸æŠã—çµ‚ãˆãŸ
            if not added_this_round:
                break

            round_num += 1

        return selected[:limit]

    def send(self):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¦Slackã«æŠ•ç¨¿"""
        print("ğŸ“¤ Slackãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")

        # æ—¥ä»˜Ã—ã‚¹ã‚³ã‚¢ã®è¤‡åˆã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„è¨˜äº‹ã‚’å„ªå…ˆã€åŒã˜æ—¥ä»˜ãªã‚‰ã‚¹ã‚³ã‚¢é †ï¼‰
        sorted_items = sorted(
            self.items,
            key=lambda x: (x.published_at, x.score),
            reverse=True
        )

        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†ã‘
        top_items = sorted_items[:self.config["slack"]["limits"]["top"]]
        provider_items = self._select_diverse_provider_items(sorted_items, self.config["slack"]["limits"]["provider_official"])
        github_items = [i for i in sorted_items if i.source == "github"][:self.config["slack"]["limits"]["github_updates"]]

        # Slack Blocksæ§‹ç¯‰
        blocks = []

        # ãƒ˜ãƒƒãƒ€ãƒ¼
        blocks.append({
            "type": "header",
            "text": {"type": "plain_text", "text": f"ğŸ¦ XæŠ•ç¨¿ç´ æ¡ˆ - {datetime.now().strftime('%Y-%m-%d')}"}
        })

        # åˆ†æå¯¾è±¡ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        source_counts = self._count_sources()
        source_summary = self._format_source_summary(source_counts)
        blocks.append({
            "type": "section",
            "text": {"type": "mrkdwn", "text": source_summary}
        })

        # XæŠ•ç¨¿ç´ æ¡ˆã‚’ç”Ÿæˆï¼ˆå€‹åˆ¥ã®ãƒ–ãƒ­ãƒƒã‚¯ã¨ã—ã¦è¿½åŠ ï¼‰
        draft_blocks = self._generate_x_post_draft_blocks(top_items, provider_items, github_items, sorted_items)
        blocks.extend(draft_blocks)

        # é€ä¿¡
        payload = {"blocks": blocks}
        response = requests.post(self.webhook_url, json=payload)

        if response.status_code == 200:
            print("âœ… Slackã«æŠ•ç¨¿ã—ã¾ã—ãŸ")
        else:
            print(f"âŒ SlackæŠ•ç¨¿å¤±æ•—: {response.status_code} {response.text}")
            raise Exception("SlackæŠ•ç¨¿ã«å¤±æ•—ã—ã¾ã—ãŸ")

    def _count_sources(self) -> Dict[str, int]:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã”ã¨ã®ã‚¢ã‚¤ãƒ†ãƒ æ•°ã‚’é›†è¨ˆ"""
        counts = {
            "x_posts": 0,      # XæŠ•ç¨¿ï¼ˆx_account + x_searchï¼‰
            "rss": 0,          # RSS
            "must_include": 0  # å¿…è¦‹ã®æ›´æ–°
        }

        for item in self.items:
            if item.source in ["x_account", "x_search"]:
                counts["x_posts"] += 1
            elif item.source == "rss":
                counts["rss"] += 1
            elif item.source == "must_include":
                counts["must_include"] += 1

        return counts

    def _format_source_summary(self, counts: Dict[str, int]) -> str:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é›†è¨ˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆå…¨ã‚½ãƒ¼ã‚¹ã‚’å¸¸ã«è¡¨ç¤ºï¼‰"""
        parts = []

        # XæŠ•ç¨¿ï¼ˆå¸¸ã«è¡¨ç¤ºï¼‰
        parts.append(f"XæŠ•ç¨¿ {counts['x_posts']}ä»¶")

        # RSSï¼ˆå¸¸ã«è¡¨ç¤ºï¼‰
        parts.append(f"RSS {counts['rss']}ä»¶")

        # å¿…è¦‹ã®æ›´æ–°ï¼ˆå¸¸ã«è¡¨ç¤ºï¼‰
        parts.append(f"å¿…è¦‹ã®æ›´æ–° {counts['must_include']}ä»¶")

        return "ğŸ“Š åˆ†æå¯¾è±¡: " + "ã€".join(parts)

    def _generate_x_post_draft(self, top_items: List[Item], provider_items: List[Item], github_items: List[Item]) -> str:
        """XæŠ•ç¨¿ç´ æ¡ˆã‚’ç”Ÿæˆï¼ˆè¨˜äº‹ã”ã¨ã«å€‹åˆ¥æŠ•ç¨¿ã‚’ä½œæˆï¼‰"""
        drafts = []
        seen_urls = set()  # URLé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨
        today = datetime.now().strftime('%Y/%m/%d')

        # RSSï¼ˆå…¬å¼ç™ºè¡¨ï¼‰ã‚’å„ªå…ˆçš„ã«æŠ•ç¨¿ç´ æ¡ˆä½œæˆï¼ˆAnthropicãªã©ã®é‡è¦ãªå…¬å¼ç™ºè¡¨ã‚’ç¢ºå®Ÿã«å«ã‚ã‚‹ï¼‰
        for item in provider_items[:7]:  # 5â†’7ã«å¢—åŠ 
            if item.url in seen_urls:
                continue
            seen_urls.add(item.url)

            feed_name = item.metadata.get("feed_name", "")
            post = self._create_single_post(
                title=item.title,
                url=item.url,
                source_type="å…¬å¼ç™ºè¡¨",
                source_name=feed_name,
                date=today,
                item=item
            )
            drafts.append(f"ã€æŠ•ç¨¿æ¡ˆ {len(drafts) + 1}ã€‘\n{post}")

        # GitHub Releaseã¯å‰Šé™¤ï¼ˆRSSã§å–å¾—ã™ã‚‹ãŸã‚ä¸è¦ï¼‰

        # ãƒˆãƒƒãƒ—ãƒã‚¤ãƒ©ã‚¤ãƒˆã‹ã‚‰è¿½åŠ ï¼ˆ2â†’1ã«å‰Šæ¸›ï¼‰
        for item in top_items[:1]:
            if item.url in seen_urls:
                continue
            if item.source in ["rss", "github"]:
                continue  # æ—¢ã«è¿½åŠ æ¸ˆã¿

            seen_urls.add(item.url)

            source_name = item.metadata.get("username", "") or item.metadata.get("keyword", "")
            post = self._create_single_post(
                title=item.title,
                url=item.url,
                source_type="Xæ³¨ç›®æŠ•ç¨¿",
                source_name=source_name,
                date=today,
                item=item
            )
            drafts.append(f"ã€æŠ•ç¨¿æ¡ˆ {len(drafts) + 1}ã€‘\n{post}")

        return "\n\n" + ("-" * 50) + "\n\n".join(drafts) if drafts else ""

    def _generate_x_post_draft_blocks(self, top_items: List[Item], provider_items: List[Item], github_items: List[Item], all_items: List[Item]) -> List[Dict]:
        """XæŠ•ç¨¿ç´ æ¡ˆã‚’Slack Blocksã¨ã—ã¦ç”Ÿæˆï¼ˆå„æŠ•ç¨¿ã‚’å€‹åˆ¥ãƒ–ãƒ­ãƒƒã‚¯ã«ï¼‰"""
        blocks = []
        seen_urls = set()
        today = datetime.now().strftime('%Y/%m/%d')
        draft_count = 0

        # ã€å¿…è¦‹ã®æ›´æ–°ã€‘ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        must_include_items = [i for i in all_items if i.metadata.get("must_include")]
        must_include_config = self.config.get("rss", {}).get("must_include_feeds", [])

        if must_include_config:
            blocks.append({"type": "divider"})
            blocks.append({
                "type": "header",
                "text": {"type": "plain_text", "text": "â­ å¿…è¦‹ã®æ›´æ–°"}
            })

            if must_include_items:
                # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ‰ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
                from collections import defaultdict
                grouped = defaultdict(list)
                for item in must_include_items:
                    feed_name = item.metadata.get("feed_name", "Unknown")
                    grouped[feed_name].append(item)

                # å„ãƒ•ã‚£ãƒ¼ãƒ‰ã®æ›´æ–°ã‚’è¡¨ç¤º
                for feed_name, items in grouped.items():
                    for item in items:
                        seen_urls.add(item.url)
                        draft_count += 1

                        post = self._create_single_post(
                            title=item.title,
                            url=item.url,
                            source_type="å¿…è¦‹ã®æ›´æ–°",
                            source_name=feed_name,
                            date=today,
                            item=item
                        )

                        # æ¤œè¨¼å¤±æ•—æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—
                        if post is None:
                            print(f"â­ï¸  æŠ•ç¨¿æ¡ˆã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ¤œè¨¼å¤±æ•—ï¼‰: {item.title[:50]}...")
                            draft_count -= 1
                            continue

                        blocks.append({
                            "type": "section",
                            "text": {"type": "mrkdwn", "text": f"*ã€æŠ•ç¨¿æ¡ˆ {draft_count}ã€‘{feed_name}*"}
                        })
                        blocks.append({
                            "type": "section",
                            "text": {"type": "mrkdwn", "text": f"```{post}```"}
                        })
                        blocks.append({"type": "divider"})
            else:
                # æ›´æ–°ãŒãªã„å ´åˆ
                feed_names = [f["name"] for f in must_include_config]
                blocks.append({
                    "type": "section",
                    "text": {"type": "mrkdwn", "text": f"ğŸ“­ æœ¬æ—¥ã®æ›´æ–°ãªã—\nå¯¾è±¡: {', '.join(feed_names)}"}
                })
                blocks.append({"type": "divider"})

        # RSSï¼ˆå…¬å¼ç™ºè¡¨ï¼‰ã‚’5ä»¶ã«å‰Šæ¸›ï¼ˆ7â†’5ï¼‰
        for item in provider_items[:5]:
            if item.url in seen_urls:
                continue
            seen_urls.add(item.url)
            draft_count += 1

            feed_name = item.metadata.get("feed_name", "")
            post = self._create_single_post(
                title=item.title,
                url=item.url,
                source_type="å…¬å¼ç™ºè¡¨",
                source_name=feed_name,
                date=today,
                item=item
            )

            # æ¤œè¨¼å¤±æ•—æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—
            if post is None:
                print(f"â­ï¸  æŠ•ç¨¿æ¡ˆã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ¤œè¨¼å¤±æ•—ï¼‰: {item.title[:50]}...")
                draft_count -= 1
                continue

            # å„æŠ•ç¨¿ã‚’å€‹åˆ¥ã®sectionãƒ–ãƒ­ãƒƒã‚¯ã«ï¼ˆ3000æ–‡å­—åˆ¶é™å›é¿ï¼‰
            blocks.append({
                "type": "section",
                "text": {"type": "mrkdwn", "text": f"```ã€æŠ•ç¨¿æ¡ˆ {draft_count}ã€‘\n{post}```"}
            })

            # åŒºåˆ‡ã‚Šç·šã‚’è¿½åŠ ï¼ˆæœ€å¾Œä»¥å¤–ï¼‰
            if draft_count < 7:
                blocks.append({"type": "divider"})

        # Xç”±æ¥ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ç¢ºå®Ÿã«å«ã‚ã‚‹ï¼ˆæ–°è¦è¿½åŠ ï¼‰
        # é‡è¦: top_items ã§ã¯ãªã all_items ã‹ã‚‰æŠ½å‡ºï¼ˆtop_items ã¯3ä»¶ã—ã‹ãªã„ãŸã‚ï¼‰
        x_items = [i for i in all_items if i.source in ["x_account", "x_search"]]
        for item in x_items[:2]:  # Xç”±æ¥ã‚’æœ€å¤§2ä»¶è¿½åŠ 
            if item.url in seen_urls:
                continue
            seen_urls.add(item.url)
            draft_count += 1

            source_name = item.metadata.get("username", "") or item.metadata.get("keyword", "")
            post = self._create_single_post(
                title=item.title,
                url=item.url,
                source_type="Xæ³¨ç›®æŠ•ç¨¿",
                source_name=source_name,
                date=today,
                item=item
            )

            # æ¤œè¨¼å¤±æ•—æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—
            if post is None:
                print(f"â­ï¸  æŠ•ç¨¿æ¡ˆã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ¤œè¨¼å¤±æ•—ï¼‰: {item.title[:50]}...")
                draft_count -= 1
                continue

            blocks.append({
                "type": "section",
                "text": {"type": "mrkdwn", "text": f"```ã€æŠ•ç¨¿æ¡ˆ {draft_count}ã€‘\n{post}```"}
            })

            # åŒºåˆ‡ã‚Šç·šã‚’è¿½åŠ ï¼ˆæœ€å¾Œä»¥å¤–ï¼‰
            if draft_count < 7:
                blocks.append({"type": "divider"})

        return blocks

    def _create_single_post(self, title: str, url: str, source_type: str, source_name: str, date: str, item: Item) -> str:
        """å€‹åˆ¥ã®XæŠ•ç¨¿ã‚’ç”Ÿæˆ"""
        # Twitterã®URLã‚’x.comã«å¤‰æ›
        if "twitter.com" in url:
            url = url.replace("twitter.com", "x.com")

        # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’å–å¾—
        category = item.metadata.get("category", "UNKNOWN")

        # XæŠ•ç¨¿ã®å ´åˆã¯å…¨æ–‡ã‚‚å–å¾—
        tweet_text = None
        if item.source in ["x_account", "x_search"]:
            tweet_text = item.metadata.get("tweet", {}).get("text", "")

        # Claude API ã§ã‚µãƒãƒ©ã‚¤ã‚ºç”Ÿæˆ
        summary = self._generate_summary_with_claude(
            title, url, source_type, category, tweet_text=tweet_text
        )

        return summary

    def _generate_summary_with_claude(self, title: str, url: str, source_type: str, category: str = "UNKNOWN", tweet_text: Optional[str] = None) -> str:
        """Claude API ã§é«˜å“è³ªãªXæŠ•ç¨¿ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ç”Ÿæˆ"""
        try:
            import anthropic

            api_key = os.environ.get("ANTHROPIC_API_KEY")
            if not api_key:
                print("âš ï¸  ANTHROPIC_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚æŠ•ç¨¿æ¡ˆç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return None

            client = anthropic.Anthropic(api_key=api_key)

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæƒ…å ±ã‚’å–å¾—
            target = self.config.get("target_audience", {})
            target_name = target.get("name", "AIã«é–¢å¿ƒã®ã‚ã‚‹ãƒ“ã‚¸ãƒã‚¹ãƒ‘ãƒ¼ã‚½ãƒ³ã‚„ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢")

            # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®ãƒ•ã‚©ãƒ¼ã‚«ã‚¹
            category_focus = {
                "PRACTICAL": "å®Ÿè£…æ–¹æ³•ã€å…·ä½“çš„ãªæ©Ÿèƒ½ã€ä½¿ã„æ–¹ã€çµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³ã€å®Ÿè·µçš„ãªTipsã‚’é‡è¦–ã—ã¦ãã ã•ã„ã€‚",
                "TECHNICAL": "æŠ€è¡“çš„ãªä»•çµ„ã¿ã€æ¯”è¼ƒåˆ†æã€è©³ç´°ãªè§£èª¬ã‚’é‡è¦–ã—ã¦ãã ã•ã„ã€‚",
                "GENERAL": "æ–°æ©Ÿèƒ½ã®æ¦‚è¦ã€åˆ©ç”¨é–‹å§‹æ™‚æœŸã€å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’é‡è¦–ã—ã¦ãã ã•ã„ã€‚"
            }.get(category, "")

            # å…±é€šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
            system_prompt = get_system_prompt()

            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            if tweet_text:
                # XæŠ•ç¨¿ã®å ´åˆï¼šãƒ„ã‚¤ãƒ¼ãƒˆå†…ã®URLã‹ã‚‰è¨˜äº‹æœ¬æ–‡ã‚’å–å¾—ï¼ˆãƒ•ã‚§ãƒ¼ã‚º3ï¼‰
                import re
                urls_in_tweet = re.findall(r'https?://[^\s]+', tweet_text)
                article_content = None

                if urls_in_tweet:
                    for url_in_tweet in urls_in_tweet:
                        _, content = fetch_article_content_safe(url_in_tweet)
                        if content:
                            article_content = content
                            break

                user_prompt = create_user_prompt_from_tweet(url, tweet_text, article_content)
            else:
                # RSSè¨˜äº‹ã®å ´åˆï¼šè¨˜äº‹æœ¬æ–‡ã‚’å–å¾—ï¼ˆãƒ•ã‚§ãƒ¼ã‚º2ï¼‰
                article_title, article_content = fetch_article_content_safe(url)

                if not article_content:
                    print(f"âš ï¸ è¨˜äº‹æœ¬æ–‡å–å¾—å¤±æ•—ã€ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ã§å‡¦ç†: {url}")
                    article_content = title

                user_prompt = create_user_prompt_from_article(
                    url,
                    article_title or title,
                    article_content
                )

            # AI-lintè‡ªå‹•ä¿®æ­£ï¼ˆæœ€å¤§2å›è©¦è¡Œã€è‡ªå‹•ãƒ•ãƒ­ãƒ¼ãªã®ã§é…å»¶æœ€å°åŒ–ï¼‰
            max_retries = 1
            score_threshold = 15
            generated_text = None
            detected_issues = None

            for attempt in range(max_retries + 1):
                message = client.messages.create(
                    model="claude-sonnet-4-5-20250929",
                    max_tokens=1500,  # 800æ–‡å­—è¦æ±‚ãªã®ã§ä½™è£•ã‚’æŒãŸã›ã‚‹
                    system=system_prompt,  # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¿½åŠ 
                    messages=[{
                        "role": "user",
                        "content": user_prompt if attempt == 0 else user_prompt + f"\n\nã€é‡è¦ï¼šä»¥ä¸‹ã®è¡¨ç¾ãŒæ¤œå‡ºã•ã‚ŒãŸã®ã§å¿…ãšä¿®æ­£ã—ã¦ãã ã•ã„ã€‘\n" + "\n".join([f"âŒ ã€Œ{issue.matched_text}ã€â†’ {issue.suggestion}" for issue in detected_issues[:5]])
                    }]
                )

                generated_text = message.content[0].text

                # AI-lintãƒã‚§ãƒƒã‚¯
                lint_result = self.ai_lint_checker.check(generated_text)

                if lint_result.score == 0:
                    break  # AIçš„è¡¨ç¾ãªã—
                elif lint_result.score < score_threshold:
                    break  # è¨±å®¹ç¯„å›²å†…
                else:
                    if attempt < max_retries:
                        detected_issues = lint_result.detections
                        # å†ç”Ÿæˆï¼ˆæ¬¡ã®ãƒ«ãƒ¼ãƒ—ã§ä¿®æ­£æŒ‡ç¤ºã‚’è¿½åŠ ï¼‰
                    else:
                        # æœ€å¤§è©¦è¡Œå›æ•°åˆ°é”ã€è­¦å‘Šã‚’å‡ºã™ãŒç¶šè¡Œ
                        print(f"âš ï¸  AI-lint: ã‚¹ã‚³ã‚¢ {lint_result.score} (é–¾å€¤è¶…éã€ç¶šè¡Œ)")

            # æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º1: æ­£è¦è¡¨ç¾ãƒ™ãƒ¼ã‚¹
            validation_result = self.validator.validate_post(generated_text, title)

            if not validation_result.is_valid:
                print(f"âš ï¸  æŠ•ç¨¿æ¡ˆãŒæ¤œè¨¼å¤±æ•—: {validation_result.rejection_reason}")
                print(f"    ã‚¿ã‚¤ãƒˆãƒ«: {title[:50]}...")
                print(f"    æ¤œå‡ºå•é¡Œ: {validation_result.detected_issues}")
                return None

            # æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º2: Claude APIãƒ¬ãƒ“ãƒ¥ãƒ¼
            review_result = self.validator.review_post_with_claude(generated_text, title, url)

            if not review_result.is_valid:
                print(f"âš ï¸  æŠ•ç¨¿æ¡ˆãŒãƒ¬ãƒ“ãƒ¥ãƒ¼å¤±æ•—: {review_result.rejection_reason}")
                print(f"    ã‚¿ã‚¤ãƒˆãƒ«: {title[:50]}...")
                print(f"    æ¤œå‡ºå•é¡Œ: {review_result.detected_issues}")
                return None

            return generated_text

        except Exception as e:
            print(f"âš ï¸ Claude API ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯è¡Œã‚ãšã€None ã‚’è¿”ã™ï¼ˆæ¤œè¨¼å¤±æ•—ã¨åŒã˜æ‰±ã„ï¼‰
            return None



def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("AIãƒ‡ã‚¤ãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆ")
    print("=" * 60)

    # è¨­å®šèª­ã¿è¾¼ã¿
    with open("config.yaml", 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
    x_bearer_token = os.environ.get("X_BEARER_TOKEN")
    slack_webhook_url = os.environ.get("SLACK_WEBHOOK_URL")

    if not x_bearer_token:
        raise ValueError("ç’°å¢ƒå¤‰æ•° X_BEARER_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    if not slack_webhook_url:
        raise ValueError("ç’°å¢ƒå¤‰æ•° SLACK_WEBHOOK_URL ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

    # åˆæœŸåŒ–
    state = StateManager()
    x_client = XAPIClient(x_bearer_token)
    collector = DataCollector(config, state, x_client)

    try:
        # ãƒ‡ãƒ¼ã‚¿åé›†
        collector.collect_all()

        # Slackãƒ¬ãƒãƒ¼ãƒˆé€ä¿¡
        reporter = SlackReporter(slack_webhook_url, config, collector.items, collector.stats)
        reporter.send()

        # çŠ¶æ…‹ä¿å­˜ï¼ˆæœ€å¾Œã«å®Ÿè¡Œï¼‰
        state.save()
        print("ğŸ’¾ çŠ¶æ…‹ã‚’ä¿å­˜ã—ã¾ã—ãŸ")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        raise

    print("=" * 60)
    print("âœ… å®Œäº†")
    print("=" * 60)


if __name__ == "__main__":
    main()
