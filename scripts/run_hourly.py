#!/usr/bin/env python3
"""
AI Semi-Daily Report - 8æ™‚ã¨15æ™‚ã®change logèª¿æŸ»ã¨XæŠ•ç¨¿ä¸‹æ›¸ãç”Ÿæˆ

ä½¿ã„æ–¹:
  python scripts/run_hourly.py
"""

import os
import sys
import hashlib
import requests
from pathlib import Path
from datetime import datetime, timezone
from dataclasses import asdict, dataclass
from typing import List, Dict, Optional
import yaml
from bs4 import BeautifulSoup
import anthropic
import feedparser

# æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from run_daily import StateManager
from draft_manager import DraftManager
from article_fetcher import fetch_article_content_safe
from post_prompt import get_system_prompt, create_user_prompt_from_article


@dataclass
class PageSnapshot:
    """ãƒšãƒ¼ã‚¸ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ"""
    url: str
    name: str
    content_hash: str
    content: str
    timestamp: str


def extract_text_from_html(html: str) -> str:
    """HTMLã‹ã‚‰æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    soup = BeautifulSoup(html, 'html.parser')

    # ä¸è¦ã‚¿ã‚°ã‚’å‰Šé™¤
    for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
        tag.decompose()

    # æœ¬æ–‡å–å¾—
    return soup.get_text(separator='\n', strip=True)


class SnapshotManager:
    """ãƒšãƒ¼ã‚¸ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç®¡ç†"""

    def __init__(self, snapshots_dir: str = "data/snapshots"):
        self.snapshots_dir = Path(snapshots_dir)
        self.snapshots_dir.mkdir(parents=True, exist_ok=True)

    def _get_snapshot_path(self, url: str) -> Path:
        """URLã‹ã‚‰ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç”Ÿæˆ"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        return self.snapshots_dir / f"{url_hash}.txt"

    def fetch_page_content(self, url: str) -> str:
        """ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—"""
        headers = {
            "User-Agent": "Mozilla/5.0 (compatible; AIResearchBot/1.0)"
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.text

    def save_snapshot(self, snapshot: PageSnapshot):
        """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ä¿å­˜"""
        snapshot_path = self._get_snapshot_path(snapshot.url)
        with open(snapshot_path, 'w', encoding='utf-8') as f:
            f.write(f"# {snapshot.name}\n")
            f.write(f"# URL: {snapshot.url}\n")
            f.write(f"# Timestamp: {snapshot.timestamp}\n")
            f.write(f"# Hash: {snapshot.content_hash}\n")
            f.write("\n")
            f.write(snapshot.content)

    def load_snapshot(self, url: str) -> Optional[PageSnapshot]:
        """ä¿å­˜æ¸ˆã¿ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿"""
        snapshot_path = self._get_snapshot_path(url)
        if not snapshot_path.exists():
            return None

        with open(snapshot_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            if len(lines) < 5:
                return None

            name = lines[0].replace("# ", "").strip()
            url_line = lines[1].replace("# URL: ", "").strip()
            timestamp = lines[2].replace("# Timestamp: ", "").strip()
            content_hash = lines[3].replace("# Hash: ", "").strip()
            content = "".join(lines[5:])

            return PageSnapshot(
                url=url_line,
                name=name,
                content_hash=content_hash,
                content=content,
                timestamp=timestamp
            )

    def check_for_changes(self, url: str, name: str) -> Optional[tuple]:
        """ãƒšãƒ¼ã‚¸ã®å¤‰æ›´ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆå‰å›ã¨ä»Šå›ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’è¿”ã™ï¼‰

        Returns:
            (old_snapshot, new_snapshot) ã®ã‚¿ãƒ—ãƒ«ï¼ˆå¤‰æ›´ã‚ã‚Šæ™‚ï¼‰
            Noneï¼ˆå¤‰æ›´ãªã—æ™‚ã¾ãŸã¯åˆå›æ™‚ï¼‰
        """
        try:
            # æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
            new_content = self.fetch_page_content(url)

            # â˜… HTMLã§ã¯ãªãã€ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¾Œã®å†…å®¹ã‚’ãƒãƒƒã‚·ãƒ¥åŒ–
            new_text = extract_text_from_html(new_content)
            new_text_hash = hashlib.sha256(new_text.encode()).hexdigest()

            # å‰å›ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿
            old_snapshot = self.load_snapshot(url)

            # å‰å›ã®ãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
            if old_snapshot:
                old_text = extract_text_from_html(old_snapshot.content)
                old_text_hash = hashlib.sha256(old_text.encode()).hexdigest()
            else:
                old_text_hash = None

            # åˆå›ã¾ãŸã¯å¤‰æ›´ã‚ã‚Šï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒã‚·ãƒ¥ã§æ¯”è¼ƒï¼‰
            if old_snapshot is None or old_text_hash != new_text_hash:
                new_snapshot = PageSnapshot(
                    url=url,
                    name=name,
                    content_hash=new_text_hash,  # â˜… ãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
                    content=new_content,  # HTMLã¯å‚ç…§ç”¨ã«ä¿å­˜
                    timestamp=datetime.now(timezone.utc).isoformat()
                )
                # å‰å›ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ä¸Šæ›¸ãï¼ˆç´¯ç©ä¿å­˜ã—ãªã„ï¼‰
                self.save_snapshot(new_snapshot)

                if old_snapshot is None:
                    print(f"ğŸ“¸ åˆå›ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ: {name}")
                    return None  # åˆå›ã¯å¤‰æ›´ã¨ã—ã¦æ‰±ã‚ãªã„
                else:
                    print(f"ğŸ”„ ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã®å¤‰æ›´æ¤œå‡º: {name}")
                    return (old_snapshot, new_snapshot)  # å‰å›ã¨ä»Šå›ã‚’è¿”ã™
            else:
                # å¤‰æ›´ãŒãªã„å ´åˆã‚‚æœ€æ–°ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ä¸Šæ›¸ã
                # ï¼ˆå‰å›ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’æœ€æ–°ã«ä¿ã¤ï¼‰
                new_snapshot = PageSnapshot(
                    url=url,
                    name=name,
                    content_hash=new_text_hash,  # â˜… ãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
                    content=new_content,  # HTMLã¯å‚ç…§ç”¨ã«ä¿å­˜
                    timestamp=datetime.now(timezone.utc).isoformat()
                )
                self.save_snapshot(new_snapshot)
                print(f"   â„¹ï¸ {name}: ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã«å¤‰æ›´ãªã—ï¼ˆHTMLå¤‰æ›´ã®ã¿ï¼‰")
                return None

        except Exception as e:
            print(f"âŒ ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå–å¾—å¤±æ•—: {name} - {e}")
            return None


def generate_post_from_snapshot(old_snapshot: Optional[PageSnapshot], new_snapshot: PageSnapshot, config: Dict) -> Optional[str]:
    """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‹ã‚‰æŠ•ç¨¿æ¡ˆã‚’ç”Ÿæˆ

    Args:
        old_snapshot: å‰å›ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼ˆåˆå›ã¯Noneï¼‰
        new_snapshot: ä»Šå›ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
        config: config.yaml ã®è¨­å®š
    """
    try:
        # 1. HTMLã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡ºï¼ˆå‰å›ã¨ä»Šå›ï¼‰
        new_text = extract_text_from_html(new_snapshot.content)
        old_text = extract_text_from_html(old_snapshot.content) if old_snapshot else ""

        # 2. Claude APIåˆæœŸåŒ–
        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            print("âš ï¸ ANTHROPIC_API_KEY æœªè¨­å®š")
            return None

        client = anthropic.Anthropic(api_key=api_key)

        # 3. å…±é€šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
        system_prompt = get_system_prompt()

        # 4. å…±é€šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ï¼ˆchangelogç”¨ï¼‰
        from post_prompt import create_user_prompt_from_changelog
        user_prompt = create_user_prompt_from_changelog(
            new_snapshot.url,
            new_snapshot.name,
            old_text if old_text else "",
            new_text
        )

        # 5. APIå‘¼ã³å‡ºã—
        message = client.messages.create(
            model="claude-sonnet-4-5-20250929",
            max_tokens=1500,
            system=system_prompt,
            messages=[{"role": "user", "content": user_prompt}]
        )

        response_text = message.content[0].text.strip()

        # â˜… NOCHANGEãƒã‚§ãƒƒã‚¯
        if response_text == "NOCHANGE" or response_text.startswith("NOCHANGE"):
            print(f"   â„¹ï¸ Claude APIãŒå¤‰æ›´ãªã—ã¨åˆ¤æ–­: {new_snapshot.name}")
            return None  # æŠ•ç¨¿æ¡ˆãªã—

        return response_text

    except Exception as e:
        print(f"âŒ æŠ•ç¨¿æ¡ˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None


def collect_rss_articles(config: Dict) -> List[Dict]:
    """å½“æ—¥å…¬é–‹ã®RSSè¨˜äº‹ã‚’åé›†

    Returns:
        [{
            "title": "è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«",
            "url": "è¨˜äº‹URL",
            "feed_name": "ãƒ•ã‚£ãƒ¼ãƒ‰å",
            "published_at": "ISO8601å½¢å¼",
            "description": "è¨˜äº‹ã®èª¬æ˜"
        }, ...]
    """
    articles = []
    feeds = config.get("rss", {}).get("feeds", [])

    # ä»Šæ—¥ã®æ—¥ä»˜ã‚’å–å¾—ï¼ˆUTCï¼‰
    today = datetime.now(timezone.utc).date()

    print(f"\nğŸ“¡ RSSè¨˜äº‹åé›†é–‹å§‹: {len(feeds)}ãƒ•ã‚£ãƒ¼ãƒ‰")

    for feed_config in feeds:
        feed_url = feed_config["url"]
        feed_name = feed_config["name"]

        try:
            # ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—
            feed = feedparser.parse(feed_url)

            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            if hasattr(feed, 'status') and feed.status >= 400:
                print(f"   âš ï¸ {feed_name}: HTTP {feed.status}")
                continue

            if not feed.entries:
                print(f"   â„¹ï¸ {feed_name}: è¨˜äº‹0ä»¶")
                continue

            # å½“æ—¥å…¬é–‹ã®è¨˜äº‹ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            today_articles = []
            for entry in feed.entries:
                published = entry.get("published_parsed") or entry.get("updated_parsed")
                if not published:
                    continue

                published_date = datetime(*published[:6], tzinfo=timezone.utc).date()

                # å½“æ—¥å…¬é–‹ã®è¨˜äº‹ã®ã¿
                if published_date == today:
                    article = {
                        "title": entry.get("title", "Untitled"),
                        "url": entry.get("link", ""),
                        "feed_name": feed_name,
                        "published_at": datetime(*published[:6], tzinfo=timezone.utc).isoformat(),
                        "description": entry.get("summary", "") or entry.get("description", "")
                    }
                    today_articles.append(article)

            if today_articles:
                print(f"   âœ… {feed_name}: {len(today_articles)}ä»¶ï¼ˆå½“æ—¥å…¬é–‹ï¼‰")
                articles.extend(today_articles)
            else:
                print(f"   â„¹ï¸ {feed_name}: å½“æ—¥å…¬é–‹ã®è¨˜äº‹ãªã—")

        except Exception as e:
            print(f"   âŒ {feed_name}: {e}")
            continue

    print(f"\nğŸ“Š RSSè¨˜äº‹åé›†å®Œäº†: {len(articles)}ä»¶")
    return articles


def generate_post_from_article(article: Dict, config: Dict) -> Optional[str]:
    """RSSè¨˜äº‹ã‹ã‚‰æŠ•ç¨¿æ¡ˆã‚’ç”Ÿæˆ

    Args:
        article: RSSè¨˜äº‹æƒ…å ±ï¼ˆtitle, url, feed_name, descriptionï¼‰
        config: config.yaml ã®è¨­å®š
    """
    try:
        # 1. è¨˜äº‹æœ¬æ–‡ã‚’å–å¾—ï¼ˆHTMLã‹ã‚‰æŠ½å‡ºï¼‰
        headers = {"User-Agent": "Mozilla/5.0 (compatible; AIResearchBot/1.0)"}
        response = requests.get(article["url"], headers=headers, timeout=30)
        response.raise_for_status()

        # HTMLã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡º
        content_text = extract_text_from_html(response.text)

        # 2. Claude APIåˆæœŸåŒ–
        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            print("âš ï¸ ANTHROPIC_API_KEY æœªè¨­å®š")
            return None

        client = anthropic.Anthropic(api_key=api_key)

        # 3. ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆãƒ–ãƒ­ã‚°è¨˜äº‹ç”¨ã€changelogã¨åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰
        system_prompt = """ã‚ãªãŸã¯AIé–‹ç™ºãƒ„ãƒ¼ãƒ«ã®ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’åˆ†æã—ã€XæŠ•ç¨¿æ¡ˆã‚’ä½œæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
èª­è€…ã¯ç”ŸæˆAIæ´»ç”¨ã«ç©æ¥µçš„ãªWebã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚

ã€é‡è¦ãªåŸå‰‡ã€‘
- ãƒ–ãƒ­ã‚°è¨˜äº‹ã®é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡ºã™ã‚‹
- å…·ä½“çš„ã§å®Ÿç”¨çš„ãªæƒ…å ±ã‚’æä¾›ã™ã‚‹ï¼ˆæŠ½è±¡çš„ãªè¡¨ç¾ã¯é¿ã‘ã‚‹ï¼‰
- ã‚«ãƒ†ã‚´ãƒªï¼ˆæ–°æ©Ÿèƒ½ã€æ”¹å–„ç‚¹ã€ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ãªã©ï¼‰ã‚’æ˜ç¢ºã«ã™ã‚‹
- æŠ€è¡“çš„ãªè©³ç´°ã‚’çœç•¥ã›ãšã€ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒç†è§£ã§ãã‚‹ãƒ¬ãƒ™ãƒ«ã§è¨˜è¼‰

ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
## æ¦‚è¦
ãƒ»[ãƒã‚¤ãƒ³ãƒˆ1: ç°¡æ½”ã«1è¡Œã§]
ãƒ»[ãƒã‚¤ãƒ³ãƒˆ2: ç°¡æ½”ã«1è¡Œã§]
ãƒ»[ãƒã‚¤ãƒ³ãƒˆ3: ç°¡æ½”ã«1è¡Œã§]
ãƒ»[ãƒã‚¤ãƒ³ãƒˆ4: ç°¡æ½”ã«1è¡Œã§]
ï¼ˆ3-5é …ç›®ï¼‰

## è©³ç´°
ãƒ»æ–°æ©Ÿèƒ½: [æ©Ÿèƒ½åã¨è©³ç´°ãªèª¬æ˜]
ãƒ»æ–°æ©Ÿèƒ½: [æ©Ÿèƒ½åã¨è©³ç´°ãªèª¬æ˜]
ãƒ»ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹: [å…·ä½“çš„ãªæ´»ç”¨æ–¹æ³•]
ãƒ»æ”¹å–„ç‚¹: [æ”¹å–„å†…å®¹ã¨è©³ç´°ãªèª¬æ˜]
ãƒ»æŠ€è¡“è©³ç´°: [æŠ€è¡“çš„ãªãƒã‚¤ãƒ³ãƒˆ]
ãƒ»å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼: [ã©ã®ã‚ˆã†ãªé–‹ç™ºè€…ã«æœ‰ç”¨ã‹]
ãƒ»æä¾›é–‹å§‹: [ãƒªãƒªãƒ¼ã‚¹æ™‚æœŸã‚„åˆ©ç”¨æ–¹æ³•]

{url}

ã€ã‚«ãƒ†ã‚´ãƒªã®ä½¿ã„åˆ†ã‘ã€‘
- æ–°æ©Ÿèƒ½: æ–°ãŸã«ç™ºè¡¨ã•ã‚ŒãŸæ©Ÿèƒ½ã‚„ã‚µãƒ¼ãƒ“ã‚¹
- ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹: å…·ä½“çš„ãªæ´»ç”¨æ–¹æ³•ã‚„äº‹ä¾‹
- æ”¹å–„ç‚¹: æ—¢å­˜æ©Ÿèƒ½ã®å¼·åŒ–ãƒ»æœ€é©åŒ–
- æŠ€è¡“è©³ç´°: ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚„å®Ÿè£…ã®è©³ç´°
- ãƒã‚°ä¿®æ­£: ä¸å…·åˆã®ä¿®æ­£ï¼ˆè©²å½“ã™ã‚‹å ´åˆã®ã¿ï¼‰

ã€åˆ¶ç´„ã€‘
- ç®‡æ¡æ›¸ãã«ã¯ã€Œãƒ»ã€ï¼ˆä¸­é»’ï¼‰ã®ã¿ä½¿ç”¨
- å…¨ä½“ã§600-800æ–‡å­—ç¨‹åº¦
- è¨˜äº‹ã«ãªã„æƒ…å ±ã¯æ¨æ¸¬ã—ãªã„
- ã‚«ãƒ†ã‚´ãƒªã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼ˆã€Œæ–°æ©Ÿèƒ½:ã€ãªã©ï¼‰ã‚’å¿…ãšå«ã‚ã‚‹"""

        # 4. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        user_prompt = f"""ä»¥ä¸‹ã®ãƒ–ãƒ­ã‚°è¨˜äº‹ã«ã¤ã„ã¦ã€XæŠ•ç¨¿æ¡ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã€‘
{article["title"]}

ã€URLã€‘
{article["url"]}

ã€ãƒ•ã‚£ãƒ¼ãƒ‰åã€‘
{article["feed_name"]}

ã€è¨˜äº‹å†…å®¹ï¼ˆæŠœç²‹ï¼‰ã€‘
{content_text[:4000]}

ä¸Šè¨˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¾“ã£ã¦æŠ•ç¨¿æ¡ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"""

        # 5. APIå‘¼ã³å‡ºã—
        message = client.messages.create(
            model="claude-sonnet-4-5-20250929",
            max_tokens=1500,
            system=system_prompt,
            messages=[{"role": "user", "content": user_prompt}]
        )

        return message.content[0].text

    except Exception as e:
        print(f"âŒ æŠ•ç¨¿æ¡ˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None


def generate_post_from_rss_article(url: str, title: str, content: str, config: Dict) -> Optional[str]:
    """RSSè¨˜äº‹ã‹ã‚‰æŠ•ç¨¿æ¡ˆã‚’ç”Ÿæˆ

    Args:
        url: è¨˜äº‹URL
        title: è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
        content: è¨˜äº‹æœ¬æ–‡
        config: è¨­å®š

    Returns:
        æŠ•ç¨¿æ¡ˆãƒ†ã‚­ã‚¹ãƒˆã€ç”Ÿæˆå¤±æ•—æ™‚ã¯None
    """
    try:
        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

        client = anthropic.Anthropic(api_key=api_key)

        # å…±é€šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
        system_prompt = get_system_prompt()
        user_prompt = create_user_prompt_from_article(url, title, content)

        message = client.messages.create(
            model="claude-sonnet-4-5-20250929",
            max_tokens=1500,
            system=system_prompt,
            messages=[{
                "role": "user",
                "content": user_prompt
            }]
        )

        return message.content[0].text

    except Exception as e:
        print(f"âŒ RSSæŠ•ç¨¿æ¡ˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None


def process_rss_feeds(state: StateManager, config: Dict) -> List[Dict]:
    """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å‡¦ç†ã—ã¦æ–°è¦è¨˜äº‹ã®æŠ•ç¨¿æ¡ˆã‚’ç”Ÿæˆ

    Args:
        state: çŠ¶æ…‹ç®¡ç†
        config: è¨­å®š

    Returns:
        æ–°è¦è¨˜äº‹ã®æŠ•ç¨¿æ¡ˆãƒªã‚¹ãƒˆ [{"url": str, "post_text": str, "title": str}, ...]
    """
    feeds = config.get("rss", {}).get("feeds", [])
    if not feeds:
        print("ğŸ“° RSSç›£è¦–: ãƒ•ã‚£ãƒ¼ãƒ‰è¨­å®šãªã—")
        return []

    print(f"\nğŸ“° RSSç›£è¦–: {len(feeds)} ãƒ•ã‚£ãƒ¼ãƒ‰")

    new_posts = []

    for feed_config in feeds:
        feed_url = feed_config["url"]
        feed_name = feed_config["name"]

        print(f"\nğŸ“¡ {feed_name}")
        print(f"   URL: {feed_url}")

        try:
            # ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—
            feed = feedparser.parse(feed_url)

            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            if hasattr(feed, 'status') and feed.status >= 400:
                print(f"   âš ï¸  HTTP {feed.status}: å–å¾—å¤±æ•—")
                continue

            if not feed.entries:
                print(f"   â„¹ï¸  è¨˜äº‹0ä»¶")
                continue

            print(f"   âœ… è¨˜äº‹å–å¾—: {len(feed.entries)}ä»¶")

            # å‰å›å–å¾—ã—ãŸè¨˜äº‹URLãƒªã‚¹ãƒˆã‚’å–å¾—
            previous_urls = state.get_rss_article_urls(feed_url)
            if previous_urls is None:
                previous_urls = []
                print(f"   â„¹ï¸  åˆå›å–å¾—ï¼ˆå…¨è¨˜äº‹ã‚’å¯¾è±¡ï¼‰")

            # ä»Šå›å–å¾—ã—ãŸè¨˜äº‹URLãƒªã‚¹ãƒˆ
            current_urls = [entry.link for entry in feed.entries]

            # å·®åˆ†ï¼ˆæ–°è¦è¨˜äº‹ï¼‰ã‚’æŠ½å‡º
            new_urls = set(current_urls) - set(previous_urls)

            if new_urls:
                print(f"   ğŸ†• æ–°è¦è¨˜äº‹: {len(new_urls)}ä»¶")
            else:
                print(f"   â„¹ï¸  æ–°è¦è¨˜äº‹ãªã—")
                # è¨˜äº‹URLãƒªã‚¹ãƒˆã‚’æ›´æ–°
                state.set_rss_article_urls(feed_url, current_urls)
                continue

            # æ–°è¦è¨˜äº‹ã‚’å‡¦ç†
            for entry in feed.entries:
                if entry.link not in new_urls:
                    continue

                print(f"\n   ğŸ“„ æ–°è¦è¨˜äº‹: {entry.title[:60]}...")

                # è¨˜äº‹æœ¬æ–‡ã‚’å–å¾—
                article_title, article_content = fetch_article_content_safe(entry.link)

                if not article_content:
                    print(f"      âš ï¸  è¨˜äº‹æœ¬æ–‡å–å¾—å¤±æ•—: {entry.link}")
                    continue

                print(f"      âœ… è¨˜äº‹æœ¬æ–‡å–å¾—æˆåŠŸ: {len(article_content)}æ–‡å­—")

                # æŠ•ç¨¿æ¡ˆã‚’ç”Ÿæˆ
                post_text = generate_post_from_rss_article(
                    entry.link,
                    article_title or entry.title,
                    article_content,
                    config
                )

                if post_text:
                    print(f"      âœ… æŠ•ç¨¿æ¡ˆç”ŸæˆæˆåŠŸ")
                    new_posts.append({
                        "url": entry.link,
                        "post_text": post_text,
                        "title": article_title or entry.title,
                        "feed_name": feed_name
                    })
                else:
                    print(f"      âš ï¸  æŠ•ç¨¿æ¡ˆç”Ÿæˆå¤±æ•—")

            # è¨˜äº‹URLãƒªã‚¹ãƒˆã‚’æ›´æ–°
            state.set_rss_article_urls(feed_url, current_urls)
            print(f"   ğŸ’¾ è¨˜äº‹URLãƒªã‚¹ãƒˆä¿å­˜: {len(current_urls[:20])}ä»¶")

        except Exception as e:
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    return new_posts


def is_meta_message(post_text: str) -> bool:
    """æŠ•ç¨¿æ¡ˆãŒãƒ¡ã‚¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã©ã†ã‹ã‚’åˆ¤å®š

    Args:
        post_text: ç”Ÿæˆã•ã‚ŒãŸæŠ•ç¨¿æ¡ˆ

    Returns:
        True: ãƒ¡ã‚¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆä¸æ­£ãªå†…å®¹ï¼‰
        False: æ­£å¸¸ãªæŠ•ç¨¿æ¡ˆ
    """
    # ãƒ¡ã‚¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    meta_keywords = [
        "å®Œå…¨ã«åŒä¸€",
        "å®Œå…¨ã«ä¸€è‡´",
        "å¤‰æ›´ç‚¹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ",
        "å¤‰æ›´ãŒè¦‹ã‚‰ã‚Œã¾ã›ã‚“",
        "å¤‰æ›´ã¯ã‚ã‚Šã¾ã›ã‚“",
        "æ–°ãŸã«è¿½åŠ ã•ã‚ŒãŸå¤‰æ›´ç‚¹ã¯ã‚ã‚Šã¾ã›ã‚“",
        "ä¸¡è€…ã®ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ãŒå®Œå…¨ã«ä¸€è‡´",
        "æ–°è¦ã®å¤‰æ›´ç‚¹ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ",
        "å‰å›ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã¨ä»Šå›ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ",  # ãƒ¡ã‚¿çš„ãªè¡¨ç¾
        "## çŠ¶æ³ã®èª¬æ˜",  # ãƒ¡ã‚¿çš„ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³
        "çµè«–ï¼šä»Šå›ã®æ¯”è¼ƒã§ã¯"  # ãƒ¡ã‚¿çš„ãªçµè«–
    ]

    # ã„ãšã‚Œã‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹
    for keyword in meta_keywords:
        if keyword in post_text:
            return True

    # æŠ•ç¨¿æ¡ˆãŒçŸ­ã™ãã‚‹ï¼ˆ100æ–‡å­—æœªæº€ï¼‰
    if len(post_text) < 100:
        return True

    # å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒãªã„ï¼ˆ## æ¦‚è¦ ã¾ãŸã¯ ## è©³ç´°ï¼‰
    if "## " not in post_text:
        return True

    return False


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("AI Semi-Dailyãƒ¬ãƒãƒ¼ãƒˆ - 8æ™‚ãƒ»15æ™‚ã®change logèª¿æŸ»")
    print("=" * 60)

    # è¨­å®šèª­ã¿è¾¼ã¿
    with open("config.yaml", 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # ç’°å¢ƒå¤‰æ•°å–å¾—
    slack_webhook_url = os.environ.get("SLACK_WEBHOOK_URL")

    if not slack_webhook_url:
        raise ValueError("ç’°å¢ƒå¤‰æ•° SLACK_WEBHOOK_URL ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

    # çŠ¶æ…‹ç®¡ç†åˆæœŸåŒ–ï¼ˆrun_daily.pyã¨å…±æœ‰ï¼‰
    state = StateManager("data/state.json")

    try:
        # ãƒšãƒ¼ã‚¸ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç›£è¦–
        print("\nğŸ“¸ ãƒšãƒ¼ã‚¸ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç›£è¦–é–‹å§‹")
        snapshot_manager = SnapshotManager()

        # ç›£è¦–å¯¾è±¡ãƒšãƒ¼ã‚¸ã‚’config.yamlã‹ã‚‰èª­ã¿è¾¼ã¿
        page_config = config.get("page_monitoring", {})
        if not page_config.get("enabled", True):
            print("ğŸ“¸ ãƒšãƒ¼ã‚¸ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç›£è¦–ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
            snapshot_changes = []
        else:
            pages_to_monitor = page_config.get("pages", [])
            print(f"ğŸ“¸ ãƒšãƒ¼ã‚¸ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç›£è¦–é–‹å§‹: {len(pages_to_monitor)}ãƒšãƒ¼ã‚¸")

            snapshot_changes = []  # [(old_snapshot, new_snapshot), ...]
            for page in pages_to_monitor:
                snapshot_pair = snapshot_manager.check_for_changes(
                    page["url"],
                    page["name"]
                )
                if snapshot_pair:  # (old_snapshot, new_snapshot)
                    snapshot_changes.append(snapshot_pair)

        # ä¸‹æ›¸ããƒãƒƒãƒ—ã‚’åˆæœŸåŒ–
        draft_map = {}  # {url: {"id": draft_id, "post_text": post_text}}

        # ä¸‹æ›¸ãç®¡ç†
        draft_manager = DraftManager()

        # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå¤‰æ›´ãŒã‚ã‚‹å ´åˆã®ã¿å‡¦ç†
        if snapshot_changes:
            print(f"\nğŸ“Š å¤‰æ›´æ¤œå‡º: {len(snapshot_changes)} ä»¶")

        # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå¤‰æ›´ã‚’ä¸‹æ›¸ãã¨ã—ã¦ä¿å­˜ï¼ˆæŠ•ç¨¿æ¡ˆç”Ÿæˆï¼‰
        for old_snapshot, new_snapshot in snapshot_changes:
            # æŠ•ç¨¿æ¡ˆç”Ÿæˆï¼ˆå‰å›ã¨ä»Šå›ã‚’æ¸¡ã™ï¼‰
            post_text = generate_post_from_snapshot(old_snapshot, new_snapshot, config)

            if not post_text:
                print(f"âš ï¸ æŠ•ç¨¿æ¡ˆç”Ÿæˆå¤±æ•—: {new_snapshot.name} - ã‚¹ã‚­ãƒƒãƒ—")
                # å¤±æ•—ç†ç”±ã‚’draft_mapã«ä¿å­˜ï¼ˆNOCHANGEã¾ãŸã¯APIå¤±æ•—ï¼‰
                draft_map[new_snapshot.url] = {
                    "id": None,
                    "post_text": None,
                    "failure_reason": "NOCHANGE"
                }
                continue  # â˜… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ã¯ãªãã‚¹ã‚­ãƒƒãƒ—

            # â˜… ãƒ¡ã‚¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¤œè¨¼
            if is_meta_message(post_text):
                print(f"âš ï¸ ãƒ¡ã‚¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¤œå‡ºã€ã‚¹ã‚­ãƒƒãƒ—: {new_snapshot.name}")
                # å¤±æ•—ç†ç”±ã‚’draft_mapã«ä¿å­˜
                draft_map[new_snapshot.url] = {
                    "id": None,
                    "post_text": None,
                    "failure_reason": "META_MESSAGE"
                }
                continue

            # ä¸‹æ›¸ãä¿å­˜ï¼ˆæ­£å¸¸ãªæŠ•ç¨¿æ¡ˆã®ã¿ï¼‰
            draft_id = draft_manager.save_draft(
                {
                    "title": new_snapshot.name,
                    "url": new_snapshot.url,
                    "source": "snapshot",
                    "metadata": {
                        "snapshot_timestamp": new_snapshot.timestamp,
                        "content_hash": new_snapshot.content_hash,
                        "old_hash": old_snapshot.content_hash if old_snapshot else None
                    }
                },
                post_text
            )
            print(f"ğŸ“ ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå¤‰æ›´ã‚’ä¸‹æ›¸ãä¿å­˜: {draft_id}")
            draft_map[new_snapshot.url] = {
                "id": draft_id,
                "post_text": post_text,
                "failure_reason": None
            }

        # RSSè¨˜äº‹åé›†ã¨æŠ•ç¨¿æ¡ˆç”Ÿæˆï¼ˆæ–°è¦è¨˜äº‹ã®ã¿ï¼‰
        print("\nğŸ“° RSSç›£è¦–é–‹å§‹")
        new_rss_posts = process_rss_feeds(state, config)

        rss_articles = []  # Slacké€šçŸ¥ç”¨ã®ãƒªã‚¹ãƒˆ
        for post_data in new_rss_posts:
            # ä¸‹æ›¸ãä¿å­˜
            draft_id = draft_manager.save_draft(
                {
                    "title": post_data["title"],
                    "url": post_data["url"],
                    "source": "rss",
                    "published_at": datetime.now(timezone.utc).isoformat(),
                    "metadata": {
                        "feed_name": post_data["feed_name"],
                        "semi_daily": True  # semi-dailyç”±æ¥
                    }
                },
                post_data["post_text"]
            )
            print(f"ğŸ“ RSSè¨˜äº‹ã‚’ä¸‹æ›¸ãä¿å­˜: {draft_id} - {post_data['title'][:50]}...")
            draft_map[post_data["url"]] = {
                "id": draft_id,
                "post_text": post_data["post_text"],
                "failure_reason": None
            }

            # Slacké€šçŸ¥ç”¨ã®ãƒªã‚¹ãƒˆã«è¿½åŠ 
            rss_articles.append({
                "title": post_data["title"],
                "url": post_data["url"],
                "feed_name": post_data["feed_name"],
                "published_at": datetime.now(timezone.utc).isoformat()
            })

        # å¿…è¦‹ã®æ›´æ–°ã‚’Slackã«é€šçŸ¥ï¼ˆchangelogã¨ãƒ–ãƒ­ã‚°è¨˜äº‹ã®ä¸¡æ–¹ï¼‰
        must_include_snapshots = [
            new_snapshot for old_snapshot, new_snapshot in snapshot_changes
            if any(p.get("must_include", False) and p["url"] == new_snapshot.url
                   for p in pages_to_monitor)
        ]
        send_snapshot_updates_to_slack(must_include_snapshots, rss_articles, slack_webhook_url, draft_map)
        if must_include_snapshots or rss_articles:
            print(f"\nğŸ”” {len(must_include_snapshots)}ä»¶ã®Changelogå¤‰æ›´ + {len(rss_articles)}ä»¶ã®ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’æ¤œå‡º")

        # å¤ã„å±¥æ­´ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        state.cleanup_old_posted_urls()

        # çŠ¶æ…‹ä¿å­˜
        state.save()
        print("ğŸ’¾ çŠ¶æ…‹ã‚’ä¿å­˜ã—ã¾ã—ãŸ")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        raise

    print("=" * 60)
    print("âœ… å‡¦ç†å®Œäº†")
    print("=" * 60)


def send_snapshot_updates_to_slack(snapshots: List, rss_articles: List, webhook_url: str, draft_map: Dict):
    """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå¤‰æ›´ã¨RSSè¨˜äº‹ã‚’Slackã«é€ä¿¡ï¼ˆå¿…è¦‹ã®æ›´æ–°ï¼‰"""
    import requests

    message = {
        "text": f"â­ å¿…è¦‹ã®æ›´æ–°: {len(snapshots) + len(rss_articles)}ä»¶",
        "blocks": [
            {
                "type": "header",
                "text": {
                    "type": "plain_text",
                    "text": "â­ å¿…è¦‹ã®æ›´æ–°"
                }
            },
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"ğŸ“Š åˆ†æå¯¾è±¡:\nãƒ»Changelogã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ {len(snapshots)}ä»¶\nãƒ»ãƒ–ãƒ­ã‚°è¨˜äº‹ {len(rss_articles)}ä»¶"
                }
            }
        ]
    }

    # å„ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®è©³ç´°ã‚’è¿½åŠ 
    for snapshot in snapshots:
        draft_info = draft_map.get(snapshot.url)

        # å¤±æ•—ç†ç”±ã‚’åˆ¤å®š
        if not draft_info:
            post_text = "âŒ æŠ•ç¨¿æ¡ˆç”Ÿæˆå¤±æ•—ï¼ˆä¸æ˜ãªã‚¨ãƒ©ãƒ¼ï¼‰"
        elif draft_info.get("failure_reason") == "NOCHANGE":
            post_text = "â„¹ï¸ å®Ÿè³ªçš„ãªå¤‰æ›´ãªã—ï¼ˆClaude APIåˆ¤æ–­ï¼‰"
        elif draft_info.get("failure_reason") == "META_MESSAGE":
            post_text = "â„¹ï¸ ãƒ¡ã‚¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¤œå‡ºï¼ˆæŠ•ç¨¿æ¡ˆã¨ã—ã¦ä¸é©åˆ‡ï¼‰"
        elif draft_info.get("failure_reason") == "API_FAILURE":
            post_text = "âŒ APIå‘¼ã³å‡ºã—å¤±æ•—"
        else:
            post_text = draft_info["post_text"]

        # ãƒšãƒ¼ã‚¸å + ã‚½ãƒ¼ã‚¹ãƒªãƒ³ã‚¯
        message["blocks"].append({
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": f"ğŸ“ *{snapshot.name}*\n<{snapshot.url}|ã‚½ãƒ¼ã‚¹ã‚’ç¢ºèª>"
            }
        })

        # æŠ•ç¨¿æ¡ˆï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ï¼‰
        message["blocks"].append({
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": f"```\n{post_text}\n```"
            }
        })

        # åŒºåˆ‡ã‚Šç·š
        message["blocks"].append({"type": "divider"})

    # RSSè¨˜äº‹ã‚’è¿½åŠ 
    for article in rss_articles:
        draft_info = draft_map.get(article["url"])

        # å¤±æ•—ç†ç”±ã‚’åˆ¤å®š
        if not draft_info:
            post_text = "âŒ æŠ•ç¨¿æ¡ˆç”Ÿæˆå¤±æ•—ï¼ˆä¸æ˜ãªã‚¨ãƒ©ãƒ¼ï¼‰"
        elif draft_info.get("failure_reason") == "NOCHANGE":
            post_text = "â„¹ï¸ å®Ÿè³ªçš„ãªå¤‰æ›´ãªã—ï¼ˆClaude APIåˆ¤æ–­ï¼‰"
        elif draft_info.get("failure_reason") == "META_MESSAGE":
            post_text = "â„¹ï¸ ãƒ¡ã‚¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¤œå‡ºï¼ˆæŠ•ç¨¿æ¡ˆã¨ã—ã¦ä¸é©åˆ‡ï¼‰"
        elif draft_info.get("failure_reason") == "API_FAILURE":
            post_text = "âŒ APIå‘¼ã³å‡ºã—å¤±æ•—"
        else:
            post_text = draft_info["post_text"]

        # ãƒ•ã‚£ãƒ¼ãƒ‰å + ã‚½ãƒ¼ã‚¹ãƒªãƒ³ã‚¯ + è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
        message["blocks"].append({
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": f"ğŸ“ *{article['feed_name']}*\n<{article['url']}|ã‚½ãƒ¼ã‚¹ã‚’ç¢ºèª>\n_{article['title']}_"
            }
        })

        # æŠ•ç¨¿æ¡ˆï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ï¼‰
        message["blocks"].append({
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": f"```\n{post_text}\n```"
            }
        })

        # åŒºåˆ‡ã‚Šç·šï¼ˆæœ€å¾Œã®è¨˜äº‹ä»¥å¤–ï¼‰
        if article != rss_articles[-1]:
            message["blocks"].append({"type": "divider"})

    # æ›´æ–°ãªã—ã®å ´åˆ
    if not snapshots and not rss_articles:
        message = {
            "text": "ğŸ“­ æœ¬æ—¥ã®æ›´æ–°ãªã—",
            "blocks": [
                {
                    "type": "header",
                    "text": {
                        "type": "plain_text",
                        "text": "â­ å¿…è¦‹ã®æ›´æ–°"
                    }
                },
                {
                    "type": "section",
                    "text": {
                        "type": "mrkdwn",
                        "text": "ğŸ“­ æœ¬æ—¥ã®æ›´æ–°ãªã—\nãƒ»Changelogã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ: 0ä»¶\nãƒ»ãƒ–ãƒ­ã‚°è¨˜äº‹ï¼ˆ15ãƒ•ã‚£ãƒ¼ãƒ‰ï¼‰: 0ä»¶\n\nå¯¾è±¡: Claude Code, GitHub Copilot, Cursorï¼ˆChangelogï¼‰ + OpenAI Blog, Anthropic Newsç­‰ï¼ˆRSSï¼‰"
                    }
                }
            ]
        }

    # Slacké€ä¿¡
    try:
        response = requests.post(webhook_url, json=message)
        if response.status_code == 200:
            print(f"âœ… å¿…è¦‹ã®æ›´æ–°ã‚’Slackã«é€ä¿¡ã—ã¾ã—ãŸï¼ˆChangelog {len(snapshots)}ä»¶ + ãƒ–ãƒ­ã‚°è¨˜äº‹ {len(rss_articles)}ä»¶ï¼‰")
        else:
            print(f"âš ï¸  Slacké€ä¿¡å¤±æ•—: {response.status_code}")
    except Exception as e:
        print(f"âš ï¸  Slacké€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    main()
