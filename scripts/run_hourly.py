#!/usr/bin/env python3
"""
AI Semi-Daily Report - 8æ™‚ã¨15æ™‚ã®change logèª¿æŸ»ã¨XæŠ•ç¨¿ä¸‹æ›¸ãç”Ÿæˆ

ä½¿ã„æ–¹:
  python scripts/run_hourly.py
"""

import os
import sys
import hashlib
import requests
from pathlib import Path
from datetime import datetime, timezone
from dataclasses import asdict, dataclass
from typing import List, Dict, Optional
import yaml
from bs4 import BeautifulSoup
import anthropic
import feedparser

# æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from state_manager import StateManager
from draft_manager import DraftManager
from article_fetcher import fetch_article_content_safe, fetch_rss_feed_safe
from post_prompt import get_system_prompt, create_user_prompt_from_article
from ai_lint_checker import AILintChecker


@dataclass
class PageSnapshot:
    """ãƒšãƒ¼ã‚¸ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ"""
    url: str
    name: str
    content_hash: str
    content: str
    timestamp: str


def extract_text_from_html(html: str) -> str:
    """HTMLã‹ã‚‰æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    soup = BeautifulSoup(html, 'html.parser')

    # ä¸è¦ã‚¿ã‚°ã‚’å‰Šé™¤
    for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
        tag.decompose()

    # æœ¬æ–‡å–å¾—
    return soup.get_text(separator='\n', strip=True)


class SnapshotManager:
    """ãƒšãƒ¼ã‚¸ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç®¡ç†"""

    def __init__(self, snapshots_dir: str = "data/snapshots"):
        self.snapshots_dir = Path(snapshots_dir)
        self.snapshots_dir.mkdir(parents=True, exist_ok=True)

    def _get_snapshot_path(self, url: str) -> Path:
        """URLã‹ã‚‰ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç”Ÿæˆ"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        return self.snapshots_dir / f"{url_hash}.txt"

    def fetch_page_content(self, url: str) -> str:
        """ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—"""
        headers = {
            "User-Agent": "Mozilla/5.0 (compatible; AIResearchBot/1.0)"
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.text

    def save_snapshot(self, snapshot: PageSnapshot):
        """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ä¿å­˜"""
        snapshot_path = self._get_snapshot_path(snapshot.url)
        with open(snapshot_path, 'w', encoding='utf-8') as f:
            f.write(f"# {snapshot.name}\n")
            f.write(f"# URL: {snapshot.url}\n")
            f.write(f"# Timestamp: {snapshot.timestamp}\n")
            f.write(f"# Hash: {snapshot.content_hash}\n")
            f.write("\n")
            f.write(snapshot.content)

    def load_snapshot(self, url: str) -> Optional[PageSnapshot]:
        """ä¿å­˜æ¸ˆã¿ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿"""
        snapshot_path = self._get_snapshot_path(url)
        if not snapshot_path.exists():
            return None

        with open(snapshot_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            if len(lines) < 5:
                return None

            name = lines[0].replace("# ", "").strip()
            url_line = lines[1].replace("# URL: ", "").strip()
            timestamp = lines[2].replace("# Timestamp: ", "").strip()
            content_hash = lines[3].replace("# Hash: ", "").strip()
            content = "".join(lines[5:])

            return PageSnapshot(
                url=url_line,
                name=name,
                content_hash=content_hash,
                content=content,
                timestamp=timestamp
            )

    def check_for_changes(self, url: str, name: str) -> Optional[tuple]:
        """ãƒšãƒ¼ã‚¸ã®å¤‰æ›´ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆå‰å›ã¨ä»Šå›ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’è¿”ã™ï¼‰

        Returns:
            (old_snapshot, new_snapshot) ã®ã‚¿ãƒ—ãƒ«ï¼ˆå¤‰æ›´ã‚ã‚Šæ™‚ï¼‰
            Noneï¼ˆå¤‰æ›´ãªã—æ™‚ã¾ãŸã¯åˆå›æ™‚ï¼‰
        """
        try:
            # æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
            new_content = self.fetch_page_content(url)

            # â˜… HTMLã§ã¯ãªãã€ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¾Œã®å†…å®¹ã‚’ãƒãƒƒã‚·ãƒ¥åŒ–
            new_text = extract_text_from_html(new_content)
            new_text_hash = hashlib.sha256(new_text.encode()).hexdigest()

            # å‰å›ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿
            old_snapshot = self.load_snapshot(url)

            # å‰å›ã®ãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
            if old_snapshot:
                old_text = extract_text_from_html(old_snapshot.content)
                old_text_hash = hashlib.sha256(old_text.encode()).hexdigest()
            else:
                old_text_hash = None

            # åˆå›ã¾ãŸã¯å¤‰æ›´ã‚ã‚Šï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒã‚·ãƒ¥ã§æ¯”è¼ƒï¼‰
            if old_snapshot is None or old_text_hash != new_text_hash:
                new_snapshot = PageSnapshot(
                    url=url,
                    name=name,
                    content_hash=new_text_hash,  # â˜… ãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
                    content=new_content,  # HTMLã¯å‚ç…§ç”¨ã«ä¿å­˜
                    timestamp=datetime.now(timezone.utc).isoformat()
                )
                # å‰å›ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ä¸Šæ›¸ãï¼ˆç´¯ç©ä¿å­˜ã—ãªã„ï¼‰
                self.save_snapshot(new_snapshot)

                if old_snapshot is None:
                    print(f"ğŸ“¸ åˆå›ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ: {name}")
                    return None  # åˆå›ã¯å¤‰æ›´ã¨ã—ã¦æ‰±ã‚ãªã„
                else:
                    print(f"ğŸ”„ ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã®å¤‰æ›´æ¤œå‡º: {name}")
                    return (old_snapshot, new_snapshot)  # å‰å›ã¨ä»Šå›ã‚’è¿”ã™
            else:
                # å¤‰æ›´ãŒãªã„å ´åˆã‚‚æœ€æ–°ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ä¸Šæ›¸ã
                # ï¼ˆå‰å›ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’æœ€æ–°ã«ä¿ã¤ï¼‰
                new_snapshot = PageSnapshot(
                    url=url,
                    name=name,
                    content_hash=new_text_hash,  # â˜… ãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
                    content=new_content,  # HTMLã¯å‚ç…§ç”¨ã«ä¿å­˜
                    timestamp=datetime.now(timezone.utc).isoformat()
                )
                self.save_snapshot(new_snapshot)
                print(f"   â„¹ï¸ {name}: ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã«å¤‰æ›´ãªã—ï¼ˆHTMLå¤‰æ›´ã®ã¿ï¼‰")
                return None

        except Exception as e:
            print(f"âŒ ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå–å¾—å¤±æ•—: {name} - {e}")
            return None


def generate_post_from_snapshot(old_snapshot: Optional[PageSnapshot], new_snapshot: PageSnapshot, config: Dict) -> Optional[str]:
    """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‹ã‚‰æŠ•ç¨¿æ¡ˆã‚’ç”Ÿæˆ

    Args:
        old_snapshot: å‰å›ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼ˆåˆå›ã¯Noneï¼‰
        new_snapshot: ä»Šå›ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
        config: config.yaml ã®è¨­å®š
    """
    try:
        # 1. HTMLã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡ºï¼ˆå‰å›ã¨ä»Šå›ï¼‰
        new_text = extract_text_from_html(new_snapshot.content)
        old_text = extract_text_from_html(old_snapshot.content) if old_snapshot else ""

        # 2. Claude APIåˆæœŸåŒ–
        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            print("âš ï¸ ANTHROPIC_API_KEY æœªè¨­å®š")
            return None

        client = anthropic.Anthropic(api_key=api_key)

        # 3. å…±é€šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
        system_prompt = get_system_prompt()

        # 4. å…±é€šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ï¼ˆchangelogç”¨ï¼‰
        from post_prompt import create_user_prompt_from_changelog
        user_prompt = create_user_prompt_from_changelog(
            new_snapshot.url,
            new_snapshot.name,
            old_text if old_text else "",
            new_text
        )

        # 5. AI-lintãƒã‚§ãƒƒã‚«ãƒ¼åˆæœŸåŒ–
        rules_path = os.path.join(os.path.dirname(__file__), "..", "ai-lint", ".claude", "skills", "ai-lint", "rules", "ai-lint-rules.yml")
        checker = AILintChecker(rules_path) if os.path.exists(rules_path) else AILintChecker()

        # 6. AI-lintè‡ªå‹•ä¿®æ­£ï¼ˆæœ€å¤§2å›è©¦è¡Œï¼‰
        max_retries = 1
        score_threshold = 15
        response_text = None
        detected_issues = None

        for attempt in range(max_retries + 1):
            message = client.messages.create(
                model="claude-sonnet-4-5-20250929",
                max_tokens=1500,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt if attempt == 0 else user_prompt + f"\n\nã€é‡è¦ï¼šä»¥ä¸‹ã®è¡¨ç¾ãŒæ¤œå‡ºã•ã‚ŒãŸã®ã§å¿…ãšä¿®æ­£ã—ã¦ãã ã•ã„ã€‘\n" + "\n".join([f"âŒ ã€Œ{issue.matched_text}ã€â†’ {issue.suggestion}" for issue in detected_issues[:5]])}]
            )

            response_text = message.content[0].text.strip()

            # â˜… NOCHANGEãƒã‚§ãƒƒã‚¯
            if response_text == "NOCHANGE" or response_text.startswith("NOCHANGE"):
                print(f"   â„¹ï¸ Claude APIãŒå¤‰æ›´ãªã—ã¨åˆ¤æ–­: {new_snapshot.name}")
                return None  # æŠ•ç¨¿æ¡ˆãªã—

            # AI-lintãƒã‚§ãƒƒã‚¯
            lint_result = checker.check(response_text)
            if lint_result.score == 0 or lint_result.score < score_threshold:
                break
            elif attempt < max_retries:
                detected_issues = lint_result.detections

        return response_text

    except Exception as e:
        print(f"âŒ æŠ•ç¨¿æ¡ˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None


def collect_rss_articles(config: Dict) -> List[Dict]:
    """å½“æ—¥å…¬é–‹ã®RSSè¨˜äº‹ã‚’åé›†

    Returns:
        [{
            "title": "è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«",
            "url": "è¨˜äº‹URL",
            "feed_name": "ãƒ•ã‚£ãƒ¼ãƒ‰å",
            "published_at": "ISO8601å½¢å¼",
            "description": "è¨˜äº‹ã®èª¬æ˜"
        }, ...]
    """
    articles = []
    feeds = config.get("rss", {}).get("feeds", [])

    # ä»Šæ—¥ã®æ—¥ä»˜ã‚’å–å¾—ï¼ˆUTCï¼‰
    today = datetime.now(timezone.utc).date()

    print(f"\nğŸ“¡ RSSè¨˜äº‹åé›†é–‹å§‹: {len(feeds)}ãƒ•ã‚£ãƒ¼ãƒ‰")

    for feed_config in feeds:
        feed_url = feed_config["url"]
        feed_name = feed_config["name"]

        try:
            # ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—
            feed = feedparser.parse(feed_url)

            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            if hasattr(feed, 'status') and feed.status >= 400:
                print(f"   âš ï¸ {feed_name}: HTTP {feed.status}")
                continue

            if not feed.entries:
                print(f"   â„¹ï¸ {feed_name}: è¨˜äº‹0ä»¶")
                continue

            # å½“æ—¥å…¬é–‹ã®è¨˜äº‹ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            today_articles = []
            for entry in feed.entries:
                published = entry.get("published_parsed") or entry.get("updated_parsed")
                if not published:
                    continue

                published_date = datetime(*published[:6], tzinfo=timezone.utc).date()

                # å½“æ—¥å…¬é–‹ã®è¨˜äº‹ã®ã¿
                if published_date == today:
                    article = {
                        "title": entry.get("title", "Untitled"),
                        "url": entry.get("link", ""),
                        "feed_name": feed_name,
                        "published_at": datetime(*published[:6], tzinfo=timezone.utc).isoformat(),
                        "description": entry.get("summary", "") or entry.get("description", "")
                    }
                    today_articles.append(article)

            if today_articles:
                print(f"   âœ… {feed_name}: {len(today_articles)}ä»¶ï¼ˆå½“æ—¥å…¬é–‹ï¼‰")
                articles.extend(today_articles)
            else:
                print(f"   â„¹ï¸ {feed_name}: å½“æ—¥å…¬é–‹ã®è¨˜äº‹ãªã—")

        except Exception as e:
            print(f"   âŒ {feed_name}: {e}")
            continue

    print(f"\nğŸ“Š RSSè¨˜äº‹åé›†å®Œäº†: {len(articles)}ä»¶")
    return articles


def generate_post_from_article(article: Dict, config: Dict) -> Optional[str]:
    """RSSè¨˜äº‹ã‹ã‚‰æŠ•ç¨¿æ¡ˆã‚’ç”Ÿæˆ

    Args:
        article: RSSè¨˜äº‹æƒ…å ±ï¼ˆtitle, url, feed_name, descriptionï¼‰
        config: config.yaml ã®è¨­å®š
    """
    try:
        # 1. è¨˜äº‹æœ¬æ–‡ã‚’å–å¾—ï¼ˆHTMLã‹ã‚‰æŠ½å‡ºï¼‰
        headers = {"User-Agent": "Mozilla/5.0 (compatible; AIResearchBot/1.0)"}
        response = requests.get(article["url"], headers=headers, timeout=30)
        response.raise_for_status()

        # HTMLã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡º
        content_text = extract_text_from_html(response.text)

        # 2. Claude APIåˆæœŸåŒ–
        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            print("âš ï¸ ANTHROPIC_API_KEY æœªè¨­å®š")
            return None

        client = anthropic.Anthropic(api_key=api_key)

        # AI-lintãƒã‚§ãƒƒã‚«ãƒ¼åˆæœŸåŒ–
        rules_path = os.path.join(os.path.dirname(__file__), "..", "ai-lint", ".claude", "skills", "ai-lint", "rules", "ai-lint-rules.yml")
        if os.path.exists(rules_path):
            checker = AILintChecker(rules_path)
        else:
            checker = AILintChecker()  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ«ãƒ¼ãƒ«ã‚’ä½¿ç”¨

        # 3. ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆãƒ–ãƒ­ã‚°è¨˜äº‹ç”¨ã€changelogã¨åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰
        system_prompt = """ã‚ãªãŸã¯AIé–‹ç™ºãƒ„ãƒ¼ãƒ«ã®ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’åˆ†æã—ã€XæŠ•ç¨¿æ¡ˆã‚’ä½œæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
èª­è€…ã¯ç”ŸæˆAIæ´»ç”¨ã«ç©æ¥µçš„ãªWebã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚

ã€é‡è¦ãªåŸå‰‡ã€‘
- ãƒ–ãƒ­ã‚°è¨˜äº‹ã®é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡ºã™ã‚‹
- å…·ä½“çš„ã§å®Ÿç”¨çš„ãªæƒ…å ±ã‚’æä¾›ã™ã‚‹ï¼ˆæŠ½è±¡çš„ãªè¡¨ç¾ã¯é¿ã‘ã‚‹ï¼‰
- ã‚«ãƒ†ã‚´ãƒªï¼ˆæ–°æ©Ÿèƒ½ã€æ”¹å–„ç‚¹ã€ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ãªã©ï¼‰ã‚’æ˜ç¢ºã«ã™ã‚‹
- æŠ€è¡“çš„ãªè©³ç´°ã‚’çœç•¥ã›ãšã€ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒç†è§£ã§ãã‚‹ãƒ¬ãƒ™ãƒ«ã§è¨˜è¼‰

ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
## æ¦‚è¦
ãƒ»[ãƒã‚¤ãƒ³ãƒˆ1: ç°¡æ½”ã«1è¡Œã§]
ãƒ»[ãƒã‚¤ãƒ³ãƒˆ2: ç°¡æ½”ã«1è¡Œã§]
ãƒ»[ãƒã‚¤ãƒ³ãƒˆ3: ç°¡æ½”ã«1è¡Œã§]
ãƒ»[ãƒã‚¤ãƒ³ãƒˆ4: ç°¡æ½”ã«1è¡Œã§]
ï¼ˆ3-5é …ç›®ï¼‰

## è©³ç´°
ãƒ»æ–°æ©Ÿèƒ½: [æ©Ÿèƒ½åã¨è©³ç´°ãªèª¬æ˜]
ãƒ»æ–°æ©Ÿèƒ½: [æ©Ÿèƒ½åã¨è©³ç´°ãªèª¬æ˜]
ãƒ»ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹: [å…·ä½“çš„ãªæ´»ç”¨æ–¹æ³•]
ãƒ»æ”¹å–„ç‚¹: [æ”¹å–„å†…å®¹ã¨è©³ç´°ãªèª¬æ˜]
ãƒ»æŠ€è¡“è©³ç´°: [æŠ€è¡“çš„ãªãƒã‚¤ãƒ³ãƒˆ]
ãƒ»å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼: [ã©ã®ã‚ˆã†ãªé–‹ç™ºè€…ã«æœ‰ç”¨ã‹]
ãƒ»æä¾›é–‹å§‹: [ãƒªãƒªãƒ¼ã‚¹æ™‚æœŸã‚„åˆ©ç”¨æ–¹æ³•]

{url}

ã€ã‚«ãƒ†ã‚´ãƒªã®ä½¿ã„åˆ†ã‘ã€‘
- æ–°æ©Ÿèƒ½: æ–°ãŸã«ç™ºè¡¨ã•ã‚ŒãŸæ©Ÿèƒ½ã‚„ã‚µãƒ¼ãƒ“ã‚¹
- ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹: å…·ä½“çš„ãªæ´»ç”¨æ–¹æ³•ã‚„äº‹ä¾‹
- æ”¹å–„ç‚¹: æ—¢å­˜æ©Ÿèƒ½ã®å¼·åŒ–ãƒ»æœ€é©åŒ–
- æŠ€è¡“è©³ç´°: ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚„å®Ÿè£…ã®è©³ç´°
- ãƒã‚°ä¿®æ­£: ä¸å…·åˆã®ä¿®æ­£ï¼ˆè©²å½“ã™ã‚‹å ´åˆã®ã¿ï¼‰

ã€åˆ¶ç´„ã€‘
- ç®‡æ¡æ›¸ãã«ã¯ã€Œãƒ»ã€ï¼ˆä¸­é»’ï¼‰ã®ã¿ä½¿ç”¨
- å…¨ä½“ã§600-800æ–‡å­—ç¨‹åº¦
- è¨˜äº‹ã«ãªã„æƒ…å ±ã¯æ¨æ¸¬ã—ãªã„
- ã‚«ãƒ†ã‚´ãƒªã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼ˆã€Œæ–°æ©Ÿèƒ½:ã€ãªã©ï¼‰ã‚’å¿…ãšå«ã‚ã‚‹"""

        # 4. AI-lintè‡ªå‹•ä¿®æ­£ï¼ˆæœ€å¤§2å›è©¦è¡Œã€è‡ªå‹•ãƒ•ãƒ­ãƒ¼ãªã®ã§é…å»¶æœ€å°åŒ–ï¼‰
        max_retries = 1
        score_threshold = 15
        generated_text = None
        lint_result = None

        for attempt in range(max_retries + 1):
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            user_prompt = f"""ä»¥ä¸‹ã®ãƒ–ãƒ­ã‚°è¨˜äº‹ã«ã¤ã„ã¦ã€XæŠ•ç¨¿æ¡ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã€‘
{article["title"]}

ã€URLã€‘
{article["url"]}

ã€ãƒ•ã‚£ãƒ¼ãƒ‰åã€‘
{article["feed_name"]}

ã€è¨˜äº‹å†…å®¹ï¼ˆæŠœç²‹ï¼‰ã€‘
{content_text[:4000]}

ä¸Šè¨˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¾“ã£ã¦æŠ•ç¨¿æ¡ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"""

            # 2å›ç›®ä»¥é™ã¯æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã‚’ä¿®æ­£æŒ‡ç¤ºã¨ã—ã¦è¿½åŠ 
            if attempt > 0 and lint_result and lint_result.detections:
                fix_instructions = "\n\nã€é‡è¦ï¼šä»¥ä¸‹ã®è¡¨ç¾ãŒæ¤œå‡ºã•ã‚ŒãŸã®ã§å¿…ãšä¿®æ­£ã—ã¦ãã ã•ã„ã€‘\n"
                for issue in lint_result.detections[:5]:
                    fix_instructions += f"âŒ ã€Œ{issue.matched_text}ã€â†’ {issue.suggestion}\n"
                user_prompt += fix_instructions

            # 5. APIå‘¼ã³å‡ºã—
            message = client.messages.create(
                model="claude-sonnet-4-5-20250929",
                max_tokens=1500,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}]
            )

            generated_text = message.content[0].text

            # AI-lintãƒã‚§ãƒƒã‚¯
            lint_result = checker.check(generated_text)

            if lint_result.score == 0 or lint_result.score < score_threshold:
                break

        return generated_text

    except Exception as e:
        print(f"âŒ æŠ•ç¨¿æ¡ˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None


def generate_post_from_rss_article(url: str, title: str, content: str, config: Dict) -> Optional[str]:
    """RSSè¨˜äº‹ã‹ã‚‰æŠ•ç¨¿æ¡ˆã‚’ç”Ÿæˆ

    Args:
        url: è¨˜äº‹URL
        title: è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
        content: è¨˜äº‹æœ¬æ–‡
        config: è¨­å®š

    Returns:
        æŠ•ç¨¿æ¡ˆãƒ†ã‚­ã‚¹ãƒˆã€ç”Ÿæˆå¤±æ•—æ™‚ã¯None
    """
    try:
        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

        client = anthropic.Anthropic(api_key=api_key)

        # AI-lintãƒã‚§ãƒƒã‚«ãƒ¼åˆæœŸåŒ–
        rules_path = os.path.join(os.path.dirname(__file__), "..", "ai-lint", ".claude", "skills", "ai-lint", "rules", "ai-lint-rules.yml")
        if os.path.exists(rules_path):
            checker = AILintChecker(rules_path)
        else:
            checker = AILintChecker()  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ«ãƒ¼ãƒ«ã‚’ä½¿ç”¨

        # å…±é€šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
        system_prompt = get_system_prompt()

        # AI-lintè‡ªå‹•ä¿®æ­£ï¼ˆæœ€å¤§2å›è©¦è¡Œã€è‡ªå‹•ãƒ•ãƒ­ãƒ¼ãªã®ã§é…å»¶æœ€å°åŒ–ï¼‰
        max_retries = 1
        score_threshold = 15
        generated_text = None
        lint_result = None

        for attempt in range(max_retries + 1):
            user_prompt = create_user_prompt_from_article(url, title, content)

            # 2å›ç›®ä»¥é™ã¯æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã‚’ä¿®æ­£æŒ‡ç¤ºã¨ã—ã¦è¿½åŠ 
            if attempt > 0 and lint_result and lint_result.detections:
                fix_instructions = "\n\nã€é‡è¦ï¼šä»¥ä¸‹ã®è¡¨ç¾ãŒæ¤œå‡ºã•ã‚ŒãŸã®ã§å¿…ãšä¿®æ­£ã—ã¦ãã ã•ã„ã€‘\n"
                for issue in lint_result.detections[:5]:
                    fix_instructions += f"âŒ ã€Œ{issue.matched_text}ã€â†’ {issue.suggestion}\n"
                user_prompt += fix_instructions

            message = client.messages.create(
                model="claude-sonnet-4-5-20250929",
                max_tokens=1500,
                system=system_prompt,
                messages=[{
                    "role": "user",
                    "content": user_prompt
                }]
            )

            generated_text = message.content[0].text

            # AI-lintãƒã‚§ãƒƒã‚¯
            lint_result = checker.check(generated_text)

            if lint_result.score == 0 or lint_result.score < score_threshold:
                break

        return generated_text

    except Exception as e:
        print(f"âŒ RSSæŠ•ç¨¿æ¡ˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None


def process_rss_feeds(state: StateManager, config: Dict) -> List[Dict]:
    """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å‡¦ç†ã—ã¦æ–°è¦è¨˜äº‹ã®æŠ•ç¨¿æ¡ˆã‚’ç”Ÿæˆ

    Args:
        state: çŠ¶æ…‹ç®¡ç†
        config: è¨­å®š

    Returns:
        æ–°è¦è¨˜äº‹ã®æŠ•ç¨¿æ¡ˆãƒªã‚¹ãƒˆ [{"url": str, "post_text": str, "title": str}, ...]
    """
    feeds = config.get("rss", {}).get("feeds", [])
    if not feeds:
        print("ğŸ“° RSSç›£è¦–: ãƒ•ã‚£ãƒ¼ãƒ‰è¨­å®šãªã—")
        return []

    print(f"\nğŸ“° RSSç›£è¦–: {len(feeds)} ãƒ•ã‚£ãƒ¼ãƒ‰")

    new_posts = []

    for feed_config in feeds:
        feed_url = feed_config["url"]
        feed_name = feed_config["name"]

        print(f"\nğŸ“¡ {feed_name}")
        print(f"   URL: {feed_url}")

        try:
            # ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—ï¼ˆCloudflareå›é¿å¯¾å¿œï¼‰
            feed = fetch_rss_feed_safe(feed_url)

            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            if hasattr(feed, 'status') and feed.status >= 400:
                print(f"   âš ï¸  HTTP {feed.status}: å–å¾—å¤±æ•—")
                continue

            if not feed.entries:
                print(f"   â„¹ï¸  è¨˜äº‹0ä»¶")
                continue

            print(f"   âœ… è¨˜äº‹å–å¾—: {len(feed.entries)}ä»¶")

            # å‰å›å–å¾—ã—ãŸè¨˜äº‹URLãƒªã‚¹ãƒˆã‚’å–å¾—
            previous_urls = state.get_rss_article_urls(feed_url)

            # å…¨è¨˜äº‹ã®URLãƒªã‚¹ãƒˆã‚’å–å¾—
            current_urls_all = [entry.link for entry in feed.entries]

            if previous_urls is None:
                # åˆå›å–å¾—æ™‚ã¯å…¨URLã‚’è¨˜éŒ²ã™ã‚‹ãŒã€æŠ•ç¨¿æ¡ˆã¯ç”Ÿæˆã—ãªã„
                previous_urls = []
                current_urls = current_urls_all
                print(f"   â„¹ï¸  åˆå›å–å¾—ï¼ˆå…¨{len(current_urls_all)}ä»¶ã®URLã‚’è¨˜éŒ²ã€æŠ•ç¨¿æ¡ˆç”Ÿæˆãªã—ï¼‰")
                state.set_rss_article_urls(feed_url, current_urls_all)
                state.save()
                continue
            else:
                # é€šå¸¸æ™‚ã¯å…¨è¨˜äº‹ã‚’å‡¦ç†
                current_urls = current_urls_all

            # å·®åˆ†ï¼ˆæ–°è¦è¨˜äº‹ï¼‰ã‚’æŠ½å‡º
            new_urls = set(current_urls) - set(previous_urls)

            if new_urls:
                print(f"   ğŸ†• æ–°è¦è¨˜äº‹: {len(new_urls)}ä»¶")
            else:
                print(f"   â„¹ï¸  æ–°è¦è¨˜äº‹ãªã—")
                # è¨˜äº‹URLãƒªã‚¹ãƒˆã‚’æ›´æ–°ï¼ˆå…¨URLã‚’è¨˜éŒ²ï¼‰
                state.set_rss_article_urls(feed_url, current_urls_all)
                state.save()  # å³åº§ã«ä¿å­˜
                continue

            # æ–°è¦è¨˜äº‹ã‚’å‡¦ç†
            for entry in feed.entries:
                if entry.link not in new_urls:
                    continue

                print(f"\n   ğŸ“„ æ–°è¦è¨˜äº‹: {entry.title[:60]}...")

                # è¨˜äº‹æœ¬æ–‡ã‚’å–å¾—
                article_title, article_content = fetch_article_content_safe(entry.link)

                if not article_content:
                    print(f"      âš ï¸  è¨˜äº‹æœ¬æ–‡å–å¾—å¤±æ•—: {entry.link}")
                    continue

                print(f"      âœ… è¨˜äº‹æœ¬æ–‡å–å¾—æˆåŠŸ: {len(article_content)}æ–‡å­—")

                # æŠ•ç¨¿æ¡ˆã‚’ç”Ÿæˆ
                post_text = generate_post_from_rss_article(
                    entry.link,
                    article_title or entry.title,
                    article_content,
                    config
                )

                if post_text:
                    print(f"      âœ… æŠ•ç¨¿æ¡ˆç”ŸæˆæˆåŠŸ")
                    new_posts.append({
                        "url": entry.link,
                        "post_text": post_text,
                        "title": article_title or entry.title,
                        "feed_name": feed_name
                    })
                else:
                    print(f"      âš ï¸  æŠ•ç¨¿æ¡ˆç”Ÿæˆå¤±æ•—")

            # è¨˜äº‹URLãƒªã‚¹ãƒˆã‚’æ›´æ–°ï¼ˆåˆå›ã§ã‚‚å…¨URLã‚’è¨˜éŒ²ï¼‰
            state.set_rss_article_urls(feed_url, current_urls_all)
            state.save()  # å³åº§ã«ä¿å­˜
            print(f"   ğŸ’¾ è¨˜äº‹URLãƒªã‚¹ãƒˆä¿å­˜: {len(current_urls_all[:20])}ä»¶")

        except Exception as e:
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    return new_posts


def is_meta_message(post_text: str) -> tuple[bool, str]:
    """æŠ•ç¨¿æ¡ˆãŒãƒ¡ã‚¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã©ã†ã‹ã‚’åˆ¤å®š

    Args:
        post_text: ç”Ÿæˆã•ã‚ŒãŸæŠ•ç¨¿æ¡ˆ

    Returns:
        (True, reason): ãƒ¡ã‚¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆä¸æ­£ãªå†…å®¹ï¼‰
        (False, ""): æ­£å¸¸ãªæŠ•ç¨¿æ¡ˆ
    """
    # ãƒ¡ã‚¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    meta_keywords = [
        "å®Œå…¨ã«åŒä¸€",
        "å®Œå…¨ã«ä¸€è‡´",
        "å¤‰æ›´ç‚¹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ",
        "å¤‰æ›´ãŒè¦‹ã‚‰ã‚Œã¾ã›ã‚“",
        "å¤‰æ›´ã¯ã‚ã‚Šã¾ã›ã‚“",
        "æ–°ãŸã«è¿½åŠ ã•ã‚ŒãŸå¤‰æ›´ç‚¹ã¯ã‚ã‚Šã¾ã›ã‚“",
        "ä¸¡è€…ã®ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ãŒå®Œå…¨ã«ä¸€è‡´",
        "æ–°è¦ã®å¤‰æ›´ç‚¹ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ",
        "å‰å›ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã¨ä»Šå›ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ",  # ãƒ¡ã‚¿çš„ãªè¡¨ç¾
        "## çŠ¶æ³ã®èª¬æ˜",  # ãƒ¡ã‚¿çš„ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³
        "çµè«–ï¼šä»Šå›ã®æ¯”è¼ƒã§ã¯"  # ãƒ¡ã‚¿çš„ãªçµè«–
    ]

    # ã„ãšã‚Œã‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹
    for keyword in meta_keywords:
        if keyword in post_text:
            return True, f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€ã‚’æ¤œå‡º"

    # æŠ•ç¨¿æ¡ˆãŒçŸ­ã™ãã‚‹ï¼ˆ50æ–‡å­—æœªæº€ã«ç·©å’Œï¼‰
    # ç†ç”±: ç°¡æ½”ãªãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆï¼ˆãƒã‚°ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®ã¿ç­‰ï¼‰ã«å¯¾å¿œ
    if len(post_text) < 50:
        return True, f"æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã™ãã‚‹ï¼ˆ{len(post_text)}æ–‡å­— < 50æ–‡å­—ï¼‰"

    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å½¢å¼ã®ãƒã‚§ãƒƒã‚¯ã¯å‰Šé™¤
    # ç†ç”±: ç°¡æ½”ãªãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆã¯ç®‡æ¡æ›¸ãã®ã¿ã®å ´åˆãŒã‚ã‚Šã€
    #       ã‚»ã‚¯ã‚·ãƒ§ãƒ³å½¢å¼ã¯æ¨å¥¨ã ãŒå¿…é ˆã§ã¯ãªã„

    return False, ""


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("AI Semi-Dailyãƒ¬ãƒãƒ¼ãƒˆ - 8æ™‚ãƒ»15æ™‚ã®change logèª¿æŸ»")
    print("=" * 60)

    # è¨­å®šèª­ã¿è¾¼ã¿
    with open("config.yaml", 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # ç’°å¢ƒå¤‰æ•°å–å¾—
    slack_webhook_url = os.environ.get("SLACK_WEBHOOK_URL")

    if not slack_webhook_url:
        raise ValueError("ç’°å¢ƒå¤‰æ•° SLACK_WEBHOOK_URL ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

    # çŠ¶æ…‹ç®¡ç†åˆæœŸåŒ–ï¼ˆrun_hourlyå°‚ç”¨ï¼‰
    state = StateManager("data/state_hourly.json")

    try:
        # ãƒšãƒ¼ã‚¸ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç›£è¦–
        print("\nğŸ“¸ ãƒšãƒ¼ã‚¸ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç›£è¦–é–‹å§‹")
        snapshot_manager = SnapshotManager()

        # ç›£è¦–å¯¾è±¡ãƒšãƒ¼ã‚¸ã‚’config.yamlã‹ã‚‰èª­ã¿è¾¼ã¿
        page_config = config.get("page_monitoring", {})
        if not page_config.get("enabled", True):
            print("ğŸ“¸ ãƒšãƒ¼ã‚¸ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç›£è¦–ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
            snapshot_changes = []
        else:
            pages_to_monitor = page_config.get("pages", [])
            print(f"ğŸ“¸ ãƒšãƒ¼ã‚¸ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç›£è¦–é–‹å§‹: {len(pages_to_monitor)}ãƒšãƒ¼ã‚¸")

            snapshot_changes = []  # [(old_snapshot, new_snapshot), ...]
            for page in pages_to_monitor:
                snapshot_pair = snapshot_manager.check_for_changes(
                    page["url"],
                    page["name"]
                )
                if snapshot_pair:  # (old_snapshot, new_snapshot)
                    snapshot_changes.append(snapshot_pair)

        # ä¸‹æ›¸ããƒãƒƒãƒ—ã‚’åˆæœŸåŒ–
        draft_map = {}  # {url: {"id": draft_id, "post_text": post_text}}

        # ä¸‹æ›¸ãç®¡ç†
        draft_manager = DraftManager()

        # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå¤‰æ›´ãŒã‚ã‚‹å ´åˆã®ã¿å‡¦ç†
        if snapshot_changes:
            print(f"\nğŸ“Š å¤‰æ›´æ¤œå‡º: {len(snapshot_changes)} ä»¶")

        # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå¤‰æ›´ã‚’ä¸‹æ›¸ãã¨ã—ã¦ä¿å­˜ï¼ˆæŠ•ç¨¿æ¡ˆç”Ÿæˆï¼‰
        for old_snapshot, new_snapshot in snapshot_changes:
            # æŠ•ç¨¿æ¡ˆç”Ÿæˆï¼ˆå‰å›ã¨ä»Šå›ã‚’æ¸¡ã™ï¼‰
            post_text = generate_post_from_snapshot(old_snapshot, new_snapshot, config)

            if not post_text:
                print(f"âš ï¸ æŠ•ç¨¿æ¡ˆç”Ÿæˆå¤±æ•—: {new_snapshot.name} - ã‚¹ã‚­ãƒƒãƒ—")
                # å¤±æ•—ç†ç”±ã‚’draft_mapã«ä¿å­˜ï¼ˆNOCHANGEã¾ãŸã¯APIå¤±æ•—ï¼‰
                draft_map[new_snapshot.url] = {
                    "id": None,
                    "post_text": None,
                    "failure_reason": "NOCHANGE"
                }
                continue  # â˜… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ã¯ãªãã‚¹ã‚­ãƒƒãƒ—

            # â˜… ãƒ¡ã‚¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¤œè¨¼
            is_meta, meta_reason = is_meta_message(post_text)
            if is_meta:
                print(f"âš ï¸ ãƒ¡ã‚¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¤œå‡ºã€ã‚¹ã‚­ãƒƒãƒ—: {new_snapshot.name}")
                print(f"   ç†ç”±: {meta_reason}")
                print(f"   ğŸ“ æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæœ€åˆã®200æ–‡å­—ï¼‰:")
                print(f"   {post_text[:200]}")
                # å¤±æ•—ç†ç”±ã‚’draft_mapã«ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ã«æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆã‚‚ä¿å­˜ï¼‰
                draft_map[new_snapshot.url] = {
                    "id": None,
                    "post_text": post_text,
                    "failure_reason": "META_MESSAGE",
                    "meta_reason": meta_reason
                }
                continue

            # ä¸‹æ›¸ãä¿å­˜ï¼ˆæ­£å¸¸ãªæŠ•ç¨¿æ¡ˆã®ã¿ï¼‰
            draft_id = draft_manager.save_draft(
                {
                    "title": new_snapshot.name,
                    "url": new_snapshot.url,
                    "source": "snapshot",
                    "metadata": {
                        "snapshot_timestamp": new_snapshot.timestamp,
                        "content_hash": new_snapshot.content_hash,
                        "old_hash": old_snapshot.content_hash if old_snapshot else None
                    }
                },
                post_text
            )
            print(f"ğŸ“ ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå¤‰æ›´ã‚’ä¸‹æ›¸ãä¿å­˜: {draft_id}")
            draft_map[new_snapshot.url] = {
                "id": draft_id,
                "post_text": post_text,
                "failure_reason": None
            }

        # RSSè¨˜äº‹åé›†ã¨æŠ•ç¨¿æ¡ˆç”Ÿæˆï¼ˆæ–°è¦è¨˜äº‹ã®ã¿ï¼‰
        print("\nğŸ“° RSSç›£è¦–é–‹å§‹")
        new_rss_posts = process_rss_feeds(state, config)

        rss_articles = []  # Slacké€šçŸ¥ç”¨ã®ãƒªã‚¹ãƒˆ
        for post_data in new_rss_posts:
            # ä¸‹æ›¸ãä¿å­˜
            draft_id = draft_manager.save_draft(
                {
                    "title": post_data["title"],
                    "url": post_data["url"],
                    "source": "rss",
                    "published_at": datetime.now(timezone.utc).isoformat(),
                    "metadata": {
                        "feed_name": post_data["feed_name"],
                        "semi_daily": True  # semi-dailyç”±æ¥
                    }
                },
                post_data["post_text"]
            )
            print(f"ğŸ“ RSSè¨˜äº‹ã‚’ä¸‹æ›¸ãä¿å­˜: {draft_id} - {post_data['title'][:50]}...")
            draft_map[post_data["url"]] = {
                "id": draft_id,
                "post_text": post_data["post_text"],
                "failure_reason": None
            }

            # Slacké€šçŸ¥ç”¨ã®ãƒªã‚¹ãƒˆã«è¿½åŠ 
            rss_articles.append({
                "title": post_data["title"],
                "url": post_data["url"],
                "feed_name": post_data["feed_name"],
                "published_at": datetime.now(timezone.utc).isoformat()
            })

        # å¿…è¦‹ã®æ›´æ–°ã‚’Slackã«é€šçŸ¥ï¼ˆchangelogã¨ãƒ–ãƒ­ã‚°è¨˜äº‹ã®ä¸¡æ–¹ï¼‰
        must_include_snapshots = [
            new_snapshot for old_snapshot, new_snapshot in snapshot_changes
            if any(p.get("must_include", False) and p["url"] == new_snapshot.url
                   for p in pages_to_monitor)
        ]
        send_snapshot_updates_to_slack(must_include_snapshots, rss_articles, slack_webhook_url, draft_map)
        if must_include_snapshots or rss_articles:
            print(f"\nğŸ”” {len(must_include_snapshots)}ä»¶ã®Changelogå¤‰æ›´ + {len(rss_articles)}ä»¶ã®ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’æ¤œå‡º")

        # å¤ã„å±¥æ­´ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        state.cleanup_old_posted_urls()

        # çŠ¶æ…‹ä¿å­˜
        state.save()
        print("ğŸ’¾ çŠ¶æ…‹ã‚’ä¿å­˜ã—ã¾ã—ãŸ")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        raise

    print("=" * 60)
    print("âœ… å‡¦ç†å®Œäº†")
    print("=" * 60)


def send_snapshot_updates_to_slack(snapshots: List, rss_articles: List, webhook_url: str, draft_map: Dict):
    """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå¤‰æ›´ã¨RSSè¨˜äº‹ã‚’Slackã«é€ä¿¡ï¼ˆå¿…è¦‹ã®æ›´æ–°ï¼‰- æŠ•ç¨¿æ¡ˆã”ã¨ã«å€‹åˆ¥é€ä¿¡"""
    import requests
    import time

    # æ›´æ–°ãªã—ã®å ´åˆ
    if not snapshots and not rss_articles:
        message = {
            "text": "ğŸ“­ æœ¬æ—¥ã®æ›´æ–°ãªã—",
            "blocks": [
                {
                    "type": "header",
                    "text": {
                        "type": "plain_text",
                        "text": "â­ å¿…è¦‹ã®æ›´æ–°"
                    }
                },
                {
                    "type": "section",
                    "text": {
                        "type": "mrkdwn",
                        "text": "ğŸ“­ æœ¬æ—¥ã®æ›´æ–°ãªã—\nãƒ»Changelogã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ: 0ä»¶\nãƒ»ãƒ–ãƒ­ã‚°è¨˜äº‹ï¼ˆ15ãƒ•ã‚£ãƒ¼ãƒ‰ï¼‰: 0ä»¶\n\nå¯¾è±¡: Claude Code, GitHub Copilot, Cursorï¼ˆChangelogï¼‰ + OpenAI Blog, Anthropic Newsç­‰ï¼ˆRSSï¼‰"
                    }
                }
            ]
        }
        # Slacké€ä¿¡
        try:
            response = requests.post(webhook_url, json=message)
            if response.status_code == 200:
                print(f"âœ… å¿…è¦‹ã®æ›´æ–°ã‚’Slackã«é€ä¿¡ã—ã¾ã—ãŸï¼ˆChangelog 0ä»¶ + ãƒ–ãƒ­ã‚°è¨˜äº‹ 0ä»¶ï¼‰")
            else:
                print(f"âš ï¸  Slacké€ä¿¡å¤±æ•—: {response.status_code}")
                print(f"    ãƒ¬ã‚¹ãƒãƒ³ã‚¹æœ¬æ–‡: {response.text}")
        except Exception as e:
            print(f"âš ï¸  Slacké€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
        return

    # â‘  ãƒ˜ãƒƒãƒ€ãƒ¼ + ã‚µãƒãƒªãƒ¼é€ä¿¡ï¼ˆ1å›ã®ã¿ï¼‰
    header_message = {
        "text": f"â­ å¿…è¦‹ã®æ›´æ–°: {len(snapshots) + len(rss_articles)}ä»¶",
        "blocks": [
            {
                "type": "header",
                "text": {
                    "type": "plain_text",
                    "text": "â­ å¿…è¦‹ã®æ›´æ–°"
                }
            },
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"ğŸ“Š å…¨ä½“: {len(snapshots) + len(rss_articles)}ä»¶ï¼ˆChangelog {len(snapshots)}ä»¶ + ãƒ–ãƒ­ã‚°è¨˜äº‹ {len(rss_articles)}ä»¶ï¼‰"
                }
            }
        ]
    }

    # ãƒ˜ãƒƒãƒ€ãƒ¼é€ä¿¡
    try:
        response = requests.post(webhook_url, json=header_message)
        response.raise_for_status()
        print(f"âœ… ãƒ˜ãƒƒãƒ€ãƒ¼é€ä¿¡: Changelog {len(snapshots)}ä»¶ + ãƒ–ãƒ­ã‚°è¨˜äº‹ {len(rss_articles)}ä»¶")
    except Exception as e:
        print(f"âŒ ãƒ˜ãƒƒãƒ€ãƒ¼é€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # â‘¡ Changelogï¼ˆã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼‰ã‚’å€‹åˆ¥é€ä¿¡
    for idx, snapshot in enumerate(snapshots):
        draft_info = draft_map.get(snapshot.url)

        # æŠ•ç¨¿æ¡ˆã¨ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’åˆ†é›¢
        post_text = None
        error_message = None

        if not draft_info:
            error_message = "âŒ æŠ•ç¨¿æ¡ˆç”Ÿæˆå¤±æ•—ï¼ˆä¸æ˜ãªã‚¨ãƒ©ãƒ¼ï¼‰"
        elif draft_info.get("failure_reason") == "NOCHANGE":
            error_message = "â„¹ï¸ å®Ÿè³ªçš„ãªå¤‰æ›´ãªã—ï¼ˆClaude APIåˆ¤æ–­ï¼‰"
        elif draft_info.get("failure_reason") == "META_MESSAGE":
            error_message = "â„¹ï¸ ãƒ¡ã‚¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¤œå‡ºï¼ˆæŠ•ç¨¿æ¡ˆã¨ã—ã¦ä¸é©åˆ‡ï¼‰"
        elif draft_info.get("failure_reason") == "API_FAILURE":
            error_message = "âŒ APIå‘¼ã³å‡ºã—å¤±æ•—"
        else:
            post_text = draft_info["post_text"]

        # â‘  ã‚¿ã‚¤ãƒˆãƒ«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ï¼ˆå¸¸ã«ï¼‰
        title_message = {
            "text": f"ğŸ“ {snapshot.name}",
            "blocks": [
                {
                    "type": "section",
                    "text": {
                        "type": "mrkdwn",
                        "text": f"ğŸ“ *{snapshot.name}*\n<{snapshot.url}|ã‚½ãƒ¼ã‚¹ã‚’ç¢ºèª>"
                    }
                }
            ]
        }
        try:
            response = requests.post(webhook_url, json=title_message)
            response.raise_for_status()
        except Exception as e:
            print(f"  âŒ ã‚¿ã‚¤ãƒˆãƒ«é€ä¿¡ã‚¨ãƒ©ãƒ¼: {snapshot.name} - {e}")
        time.sleep(1)

        # â‘¡ æŠ•ç¨¿æ¡ˆãƒ†ã‚­ã‚¹ãƒˆé€ä¿¡ï¼ˆæˆåŠŸæ™‚ã®ã¿ï¼‰
        if post_text:
            post_message = {
                "text": "æŠ•ç¨¿æ¡ˆ",
                "blocks": [
                    {
                        "type": "section",
                        "text": {
                            "type": "mrkdwn",
                            "text": f"```\n{post_text}\n```"
                        }
                    }
                ]
            }
            try:
                response = requests.post(webhook_url, json=post_message)
                response.raise_for_status()
                print(f"  âœ… Changelogé€ä¿¡ ({idx + 1}/{len(snapshots)}): {snapshot.name}")
            except Exception as e:
                print(f"  âŒ æŠ•ç¨¿æ¡ˆé€ä¿¡ã‚¨ãƒ©ãƒ¼: {snapshot.name} - {e}")
            time.sleep(1)

        # â‘¢ ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ï¼ˆå¤±æ•—æ™‚ã®ã¿ï¼‰
        if error_message:
            error_msg = {
                "text": "ã‚¨ãƒ©ãƒ¼",
                "blocks": [
                    {
                        "type": "section",
                        "text": {
                            "type": "mrkdwn",
                            "text": f"âš ï¸ {error_message}"
                        }
                    }
                ]
            }
            try:
                response = requests.post(webhook_url, json=error_msg)
                response.raise_for_status()
                print(f"  âš ï¸  Changelogã‚¨ãƒ©ãƒ¼é€šçŸ¥ ({idx + 1}/{len(snapshots)}): {snapshot.name}")
            except Exception as e:
                print(f"  âŒ ã‚¨ãƒ©ãƒ¼é€šçŸ¥é€ä¿¡å¤±æ•—: {snapshot.name} - {e}")
            time.sleep(1)

    # â‘¢ RSSè¨˜äº‹ã‚’å€‹åˆ¥é€ä¿¡
    for idx, article in enumerate(rss_articles):
        draft_info = draft_map.get(article["url"])

        # æŠ•ç¨¿æ¡ˆã¨ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’åˆ†é›¢
        post_text = None
        error_message = None

        if not draft_info:
            error_message = "âŒ æŠ•ç¨¿æ¡ˆç”Ÿæˆå¤±æ•—ï¼ˆä¸æ˜ãªã‚¨ãƒ©ãƒ¼ï¼‰"
        elif draft_info.get("failure_reason") == "NOCHANGE":
            error_message = "â„¹ï¸ å®Ÿè³ªçš„ãªå¤‰æ›´ãªã—ï¼ˆClaude APIåˆ¤æ–­ï¼‰"
        elif draft_info.get("failure_reason") == "META_MESSAGE":
            error_message = "â„¹ï¸ ãƒ¡ã‚¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¤œå‡ºï¼ˆæŠ•ç¨¿æ¡ˆã¨ã—ã¦ä¸é©åˆ‡ï¼‰"
        elif draft_info.get("failure_reason") == "API_FAILURE":
            error_message = "âŒ APIå‘¼ã³å‡ºã—å¤±æ•—"
        else:
            post_text = draft_info["post_text"]

        # â‘  ã‚¿ã‚¤ãƒˆãƒ«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ï¼ˆå¸¸ã«ï¼‰
        title_message = {
            "text": f"ğŸ“ {article['feed_name']}",
            "blocks": [
                {
                    "type": "section",
                    "text": {
                        "type": "mrkdwn",
                        "text": f"ğŸ“ *{article['feed_name']}*\n<{article['url']}|ã‚½ãƒ¼ã‚¹ã‚’ç¢ºèª>\n_{article['title']}_"
                    }
                }
            ]
        }
        try:
            response = requests.post(webhook_url, json=title_message)
            response.raise_for_status()
        except Exception as e:
            print(f"  âŒ ã‚¿ã‚¤ãƒˆãƒ«é€ä¿¡ã‚¨ãƒ©ãƒ¼: {article['feed_name']} - {e}")
        time.sleep(1)

        # â‘¡ æŠ•ç¨¿æ¡ˆãƒ†ã‚­ã‚¹ãƒˆé€ä¿¡ï¼ˆæˆåŠŸæ™‚ã®ã¿ï¼‰
        if post_text:
            post_message = {
                "text": "æŠ•ç¨¿æ¡ˆ",
                "blocks": [
                    {
                        "type": "section",
                        "text": {
                            "type": "mrkdwn",
                            "text": f"```\n{post_text}\n```"
                        }
                    }
                ]
            }
            try:
                response = requests.post(webhook_url, json=post_message)
                response.raise_for_status()
                print(f"  âœ… RSSè¨˜äº‹é€ä¿¡ ({idx + 1}/{len(rss_articles)}): {article['feed_name']} - {article['title'][:30]}...")
            except Exception as e:
                print(f"  âŒ æŠ•ç¨¿æ¡ˆé€ä¿¡ã‚¨ãƒ©ãƒ¼: {article['feed_name']} - {e}")
            time.sleep(1)

        # â‘¢ ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ï¼ˆå¤±æ•—æ™‚ã®ã¿ï¼‰
        if error_message:
            error_msg = {
                "text": "ã‚¨ãƒ©ãƒ¼",
                "blocks": [
                    {
                        "type": "section",
                        "text": {
                            "type": "mrkdwn",
                            "text": f"âš ï¸ {error_message}"
                        }
                    }
                ]
            }
            try:
                response = requests.post(webhook_url, json=error_msg)
                response.raise_for_status()
                print(f"  âš ï¸  RSSè¨˜äº‹ã‚¨ãƒ©ãƒ¼é€šçŸ¥ ({idx + 1}/{len(rss_articles)}): {article['feed_name']}")
            except Exception as e:
                print(f"  âŒ ã‚¨ãƒ©ãƒ¼é€šçŸ¥é€ä¿¡å¤±æ•—: {article['feed_name']} - {e}")
            time.sleep(1)

    print(f"\nâœ… å…¨ã¦ã®æŠ•ç¨¿æ¡ˆã‚’é€ä¿¡å®Œäº†: Changelog {len(snapshots)}ä»¶ + ãƒ–ãƒ­ã‚°è¨˜äº‹ {len(rss_articles)}ä»¶")


if __name__ == "__main__":
    main()
